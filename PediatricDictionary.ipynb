{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eefa5f36-4c70-4887-8198-8df3a1c56412",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 03:38:02.722403: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-01 03:38:02.733685: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-01 03:38:02.747284: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-01 03:38:02.751425: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-01 03:38:02.761594: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from scipy import stats\n",
    "from scipy.signal import find_peaks, detrend\n",
    "import pywt\n",
    "\n",
    "from numpy import arange, array, linspace, loadtxt, log2, logspace, mean, polyfit\n",
    "from numpy import zeros, pi, sin, cos, arctan2, sqrt, real, imag, conj, tile\n",
    "from numpy import round, interp, diff, unique, where\n",
    "from pandas import DataFrame, date_range\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib import pyplot\n",
    "from scipy.stats import pearsonr, mannwhitneyu, kruskal, norm\n",
    "from sklearn.linear_model import LinearRegression, Ridge, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GroupKFold\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score,f1_score, recall_score, precision_score,roc_auc_score, roc_curve, auc,accuracy_score,classification_report,confusion_matrix\n",
    "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier, GradientBoostingClassifier,AdaBoostClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.utils import shuffle\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import GlobalAveragePooling2D,GlobalMaxPooling2D,Concatenate,Input,Dropout, Conv2D, MaxPooling2D, Flatten,Multiply,Attention,Dense,concatenate,Masking,BatchNormalization, Reshape\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.applications import DenseNet121\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, History\n",
    "history = History()\n",
    "\n",
    "import pickle\n",
    "import joblib\n",
    "import dill\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d619773c-0a15-4bd1-8112-b34755cb65dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USUBJID</th>\n",
       "      <th>LBSTRESN</th>\n",
       "      <th>LBDTC</th>\n",
       "      <th>LBTEST</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>115.0</td>\n",
       "      <td>2020-05-11 00:01:17</td>\n",
       "      <td>Glucose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>109.0</td>\n",
       "      <td>2020-05-11 00:06:17</td>\n",
       "      <td>Glucose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>105.0</td>\n",
       "      <td>2020-05-11 00:11:17</td>\n",
       "      <td>Glucose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>106.0</td>\n",
       "      <td>2020-05-11 00:16:18</td>\n",
       "      <td>Glucose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>110.0</td>\n",
       "      <td>2020-05-11 00:21:18</td>\n",
       "      <td>Glucose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3785750</th>\n",
       "      <td>988</td>\n",
       "      <td>94.0</td>\n",
       "      <td>2021-04-16 23:41:40</td>\n",
       "      <td>Glucose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3785751</th>\n",
       "      <td>988</td>\n",
       "      <td>95.0</td>\n",
       "      <td>2021-04-16 23:46:40</td>\n",
       "      <td>Glucose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3785752</th>\n",
       "      <td>988</td>\n",
       "      <td>93.0</td>\n",
       "      <td>2021-04-16 23:51:40</td>\n",
       "      <td>Glucose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3785753</th>\n",
       "      <td>988</td>\n",
       "      <td>90.0</td>\n",
       "      <td>2021-04-16 23:56:44</td>\n",
       "      <td>Glucose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3785754</th>\n",
       "      <td>988</td>\n",
       "      <td>6.3</td>\n",
       "      <td>2021-02-23 00:00:00</td>\n",
       "      <td>Hemoglobin A1c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3785755 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         USUBJID  LBSTRESN               LBDTC          LBTEST\n",
       "0              1     115.0 2020-05-11 00:01:17         Glucose\n",
       "1              1     109.0 2020-05-11 00:06:17         Glucose\n",
       "2              1     105.0 2020-05-11 00:11:17         Glucose\n",
       "3              1     106.0 2020-05-11 00:16:18         Glucose\n",
       "4              1     110.0 2020-05-11 00:21:18         Glucose\n",
       "...          ...       ...                 ...             ...\n",
       "3785750      988      94.0 2021-04-16 23:41:40         Glucose\n",
       "3785751      988      95.0 2021-04-16 23:46:40         Glucose\n",
       "3785752      988      93.0 2021-04-16 23:51:40         Glucose\n",
       "3785753      988      90.0 2021-04-16 23:56:44         Glucose\n",
       "3785754      988       6.3 2021-02-23 00:00:00  Hemoglobin A1c\n",
       "\n",
       "[3785755 rows x 4 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEXI_folder = '/home/ma98/Compressed_DEXI_Data/'\n",
    "\n",
    "T1DEXIP_cgm = pd.read_parquet(os.path.join(DEXI_folder, 'T1DEXI_Dataset.csv'))\n",
    "# T1DEXIP_cgm['LBDTC'] = pd.to_datetime(T1DEXIP_cgm['LBDTC'])\n",
    "\n",
    "T1DEXIP_cgm['USUBJID'] = T1DEXIP_cgm['USUBJID'].astype(str) \n",
    "#T1DEXIP_cgm['USUBJID'] = T1DEXIP_cgm['USUBJID'].str[2:-1] \n",
    "T1DEXIP_cgm['USUBJID'] = T1DEXIP_cgm['USUBJID'].astype(int)\n",
    "T1DEXIP_cgm['LBDTC'] = pd.to_datetime(T1DEXIP_cgm['LBDTC'])\n",
    "# T1DEXIP_cgm['LBDTC'] = T1DEXIP_cgm['LBDTC'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "T1DEXIP_cgm = T1DEXIP_cgm[['USUBJID','LBSTRESN','LBDTC','LBTEST']]\n",
    "df = T1DEXIP_cgm\n",
    "df\n",
    "# # # HDF5\n",
    "# # df.to_hdf('data.h5', key='df', mode='w', complevel=9, complib='blosc')\n",
    "# # hdf5_size = os.path.getsize('data.h5')\n",
    "\n",
    "# # Parquet\n",
    "# df.to_parquet('data.parquet', compression='snappy')\n",
    "# parquet_size = os.path.getsize('data.parquet')\n",
    "\n",
    "# # Feather\n",
    "# df.to_feather('data.feather')\n",
    "# feather_size = os.path.getsize('data.feather')\n",
    "\n",
    "# # CSV with gzip\n",
    "# df.to_csv('data.csv.gz', index=False, compression='gzip')\n",
    "# csv_size = os.path.getsize('data.csv.gz')\n",
    "\n",
    "# # print(f\"HDF5 file size: {hdf5_size/1000000} bytes\")\n",
    "# print(f\"Parquet file size: {parquet_size/1000000} bytes\")\n",
    "# print(f\"Feather file size: {feather_size/1000000} bytes\")\n",
    "# print(f\"CSV (gzip) file size: {csv_size/1000000} bytes\")\n",
    "# T1DEXI_CGM_Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a65507f8-3c8b-46fa-8ff9-208901ade9b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STUDYID</th>\n",
       "      <th>DOMAIN</th>\n",
       "      <th>USUBJID</th>\n",
       "      <th>FASEQ</th>\n",
       "      <th>FAGRPID</th>\n",
       "      <th>FATESTCD</th>\n",
       "      <th>FATEST</th>\n",
       "      <th>FAOBJ</th>\n",
       "      <th>FACAT</th>\n",
       "      <th>FASCAT</th>\n",
       "      <th>FAORRES</th>\n",
       "      <th>FAORRESU</th>\n",
       "      <th>FASTRESC</th>\n",
       "      <th>FASTRESN</th>\n",
       "      <th>FASTRESU</th>\n",
       "      <th>FAMETHOD</th>\n",
       "      <th>FAANMETH</th>\n",
       "      <th>FADTC</th>\n",
       "      <th>FPANLTYP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'T1DEXI-P'</td>\n",
       "      <td>b'FA'</td>\n",
       "      <td>b'100'</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6069.0</td>\n",
       "      <td>b'ALCDRKN'</td>\n",
       "      <td>b'Number of Alcoholic Drinks'</td>\n",
       "      <td>b'Almond milk, unsweetened'</td>\n",
       "      <td>b'TAKEN'</td>\n",
       "      <td>b'ALCOHOLIC DRINKS'</td>\n",
       "      <td>b'0.00'</td>\n",
       "      <td>b'DRINK'</td>\n",
       "      <td>b'0.00'</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>b'DRINK'</td>\n",
       "      <td>b'RFPM'</td>\n",
       "      <td>b'FNDDS 2017-2018'</td>\n",
       "      <td>1.944835e+09</td>\n",
       "      <td>b'Photo'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'T1DEXI-P'</td>\n",
       "      <td>b'FA'</td>\n",
       "      <td>b'100'</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6069.0</td>\n",
       "      <td>b'ALCDRKN'</td>\n",
       "      <td>b'Number of Alcoholic Drinks'</td>\n",
       "      <td>b'Almond milk, unsweetened'</td>\n",
       "      <td>b'RETURNED'</td>\n",
       "      <td>b'ALCOHOLIC DRINKS'</td>\n",
       "      <td>b'0.00'</td>\n",
       "      <td>b'DRINK'</td>\n",
       "      <td>b'0.00'</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>b'DRINK'</td>\n",
       "      <td>b'RFPM'</td>\n",
       "      <td>b'FNDDS 2017-2018'</td>\n",
       "      <td>1.944835e+09</td>\n",
       "      <td>b'Photo'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'T1DEXI-P'</td>\n",
       "      <td>b'FA'</td>\n",
       "      <td>b'100'</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6069.0</td>\n",
       "      <td>b'ALCDRKN'</td>\n",
       "      <td>b'Number of Alcoholic Drinks'</td>\n",
       "      <td>b'Almond milk, unsweetened'</td>\n",
       "      <td>b'CONSUMED'</td>\n",
       "      <td>b'ALCOHOLIC DRINKS'</td>\n",
       "      <td>b'0.00'</td>\n",
       "      <td>b'DRINK'</td>\n",
       "      <td>b'0.00'</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>b'DRINK'</td>\n",
       "      <td>b'RFPM'</td>\n",
       "      <td>b'FNDDS 2017-2018'</td>\n",
       "      <td>1.944835e+09</td>\n",
       "      <td>b'Photo'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'T1DEXI-P'</td>\n",
       "      <td>b'FA'</td>\n",
       "      <td>b'100'</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4857.0</td>\n",
       "      <td>b'ALCDRKN'</td>\n",
       "      <td>b'Number of Alcoholic Drinks'</td>\n",
       "      <td>b'Apple, raw'</td>\n",
       "      <td>b'TAKEN'</td>\n",
       "      <td>b'ALCOHOLIC DRINKS'</td>\n",
       "      <td>b'0.00'</td>\n",
       "      <td>b'DRINK'</td>\n",
       "      <td>b'0.00'</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>b'DRINK'</td>\n",
       "      <td>b'RFPM'</td>\n",
       "      <td>b'FNDDS 2017-2018'</td>\n",
       "      <td>1.944648e+09</td>\n",
       "      <td>b'Photo'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'T1DEXI-P'</td>\n",
       "      <td>b'FA'</td>\n",
       "      <td>b'100'</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4857.0</td>\n",
       "      <td>b'ALCDRKN'</td>\n",
       "      <td>b'Number of Alcoholic Drinks'</td>\n",
       "      <td>b'Apple, raw'</td>\n",
       "      <td>b'RETURNED'</td>\n",
       "      <td>b'ALCOHOLIC DRINKS'</td>\n",
       "      <td>b'0.00'</td>\n",
       "      <td>b'DRINK'</td>\n",
       "      <td>b'0.00'</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>b'DRINK'</td>\n",
       "      <td>b'RFPM'</td>\n",
       "      <td>b'FNDDS 2017-2018'</td>\n",
       "      <td>1.944648e+09</td>\n",
       "      <td>b'Photo'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1727923</th>\n",
       "      <td>b'T1DEXI-P'</td>\n",
       "      <td>b'FA'</td>\n",
       "      <td>b'99'</td>\n",
       "      <td>31511.0</td>\n",
       "      <td>2209.0</td>\n",
       "      <td>b'TKNAMT'</td>\n",
       "      <td>b'Taken Amount'</td>\n",
       "      <td>b'Taffy'</td>\n",
       "      <td>b'TAKEN'</td>\n",
       "      <td>b'TOTAL'</td>\n",
       "      <td>b'16.00'</td>\n",
       "      <td>b'g'</td>\n",
       "      <td>b'16.00'</td>\n",
       "      <td>1.600000e+01</td>\n",
       "      <td>b'g'</td>\n",
       "      <td>b'RFPM'</td>\n",
       "      <td>b'FNDDS 2017-2018'</td>\n",
       "      <td>1.949328e+09</td>\n",
       "      <td>b'Photo'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1727924</th>\n",
       "      <td>b'T1DEXI-P'</td>\n",
       "      <td>b'FA'</td>\n",
       "      <td>b'99'</td>\n",
       "      <td>31512.0</td>\n",
       "      <td>2230.0</td>\n",
       "      <td>b'TKNAMT'</td>\n",
       "      <td>b'Taken Amount'</td>\n",
       "      <td>b'Taffy'</td>\n",
       "      <td>b'TAKEN'</td>\n",
       "      <td>b'TOTAL'</td>\n",
       "      <td>b'15.60'</td>\n",
       "      <td>b'g'</td>\n",
       "      <td>b'15.60'</td>\n",
       "      <td>1.560000e+01</td>\n",
       "      <td>b'g'</td>\n",
       "      <td>b'RFPM'</td>\n",
       "      <td>b'FNDDS 2017-2018'</td>\n",
       "      <td>1.949414e+09</td>\n",
       "      <td>b'Photo'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1727925</th>\n",
       "      <td>b'T1DEXI-P'</td>\n",
       "      <td>b'FA'</td>\n",
       "      <td>b'99'</td>\n",
       "      <td>31513.0</td>\n",
       "      <td>2241.0</td>\n",
       "      <td>b'TKNAMT'</td>\n",
       "      <td>b'Taken Amount'</td>\n",
       "      <td>b'Taffy'</td>\n",
       "      <td>b'TAKEN'</td>\n",
       "      <td>b'TOTAL'</td>\n",
       "      <td>b'15.60'</td>\n",
       "      <td>b'g'</td>\n",
       "      <td>b'15.60'</td>\n",
       "      <td>1.560000e+01</td>\n",
       "      <td>b'g'</td>\n",
       "      <td>b'RFPM'</td>\n",
       "      <td>b'FNDDS 2017-2018'</td>\n",
       "      <td>1.949739e+09</td>\n",
       "      <td>b'Photo'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1727926</th>\n",
       "      <td>b'T1DEXI-P'</td>\n",
       "      <td>b'FA'</td>\n",
       "      <td>b'99'</td>\n",
       "      <td>31514.0</td>\n",
       "      <td>2247.0</td>\n",
       "      <td>b'TKNAMT'</td>\n",
       "      <td>b'Taken Amount'</td>\n",
       "      <td>b'Taffy'</td>\n",
       "      <td>b'TAKEN'</td>\n",
       "      <td>b'TOTAL'</td>\n",
       "      <td>b'15.60'</td>\n",
       "      <td>b'g'</td>\n",
       "      <td>b'15.60'</td>\n",
       "      <td>1.560000e+01</td>\n",
       "      <td>b'g'</td>\n",
       "      <td>b'RFPM'</td>\n",
       "      <td>b'FNDDS 2017-2018'</td>\n",
       "      <td>1.949749e+09</td>\n",
       "      <td>b'Photo'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1727927</th>\n",
       "      <td>b'T1DEXI-P'</td>\n",
       "      <td>b'FA'</td>\n",
       "      <td>b'99'</td>\n",
       "      <td>31515.0</td>\n",
       "      <td>2215.0</td>\n",
       "      <td>b'TKNAMT'</td>\n",
       "      <td>b'Taken Amount'</td>\n",
       "      <td>b'Waffle, plain'</td>\n",
       "      <td>b'TAKEN'</td>\n",
       "      <td>b'TOTAL'</td>\n",
       "      <td>b'39.00'</td>\n",
       "      <td>b'g'</td>\n",
       "      <td>b'39.00'</td>\n",
       "      <td>3.900000e+01</td>\n",
       "      <td>b'g'</td>\n",
       "      <td>b'RFPM'</td>\n",
       "      <td>b'FNDDS 2017-2018'</td>\n",
       "      <td>1.949382e+09</td>\n",
       "      <td>b'Photo'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1727928 rows Ã— 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             STUDYID DOMAIN USUBJID    FASEQ  FAGRPID    FATESTCD  \\\n",
       "0        b'T1DEXI-P'  b'FA'  b'100'      2.0   6069.0  b'ALCDRKN'   \n",
       "1        b'T1DEXI-P'  b'FA'  b'100'      3.0   6069.0  b'ALCDRKN'   \n",
       "2        b'T1DEXI-P'  b'FA'  b'100'      4.0   6069.0  b'ALCDRKN'   \n",
       "3        b'T1DEXI-P'  b'FA'  b'100'      5.0   4857.0  b'ALCDRKN'   \n",
       "4        b'T1DEXI-P'  b'FA'  b'100'      6.0   4857.0  b'ALCDRKN'   \n",
       "...              ...    ...     ...      ...      ...         ...   \n",
       "1727923  b'T1DEXI-P'  b'FA'   b'99'  31511.0   2209.0   b'TKNAMT'   \n",
       "1727924  b'T1DEXI-P'  b'FA'   b'99'  31512.0   2230.0   b'TKNAMT'   \n",
       "1727925  b'T1DEXI-P'  b'FA'   b'99'  31513.0   2241.0   b'TKNAMT'   \n",
       "1727926  b'T1DEXI-P'  b'FA'   b'99'  31514.0   2247.0   b'TKNAMT'   \n",
       "1727927  b'T1DEXI-P'  b'FA'   b'99'  31515.0   2215.0   b'TKNAMT'   \n",
       "\n",
       "                                FATEST                        FAOBJ  \\\n",
       "0        b'Number of Alcoholic Drinks'  b'Almond milk, unsweetened'   \n",
       "1        b'Number of Alcoholic Drinks'  b'Almond milk, unsweetened'   \n",
       "2        b'Number of Alcoholic Drinks'  b'Almond milk, unsweetened'   \n",
       "3        b'Number of Alcoholic Drinks'                b'Apple, raw'   \n",
       "4        b'Number of Alcoholic Drinks'                b'Apple, raw'   \n",
       "...                                ...                          ...   \n",
       "1727923                b'Taken Amount'                     b'Taffy'   \n",
       "1727924                b'Taken Amount'                     b'Taffy'   \n",
       "1727925                b'Taken Amount'                     b'Taffy'   \n",
       "1727926                b'Taken Amount'                     b'Taffy'   \n",
       "1727927                b'Taken Amount'             b'Waffle, plain'   \n",
       "\n",
       "               FACAT               FASCAT   FAORRES  FAORRESU  FASTRESC  \\\n",
       "0           b'TAKEN'  b'ALCOHOLIC DRINKS'   b'0.00'  b'DRINK'   b'0.00'   \n",
       "1        b'RETURNED'  b'ALCOHOLIC DRINKS'   b'0.00'  b'DRINK'   b'0.00'   \n",
       "2        b'CONSUMED'  b'ALCOHOLIC DRINKS'   b'0.00'  b'DRINK'   b'0.00'   \n",
       "3           b'TAKEN'  b'ALCOHOLIC DRINKS'   b'0.00'  b'DRINK'   b'0.00'   \n",
       "4        b'RETURNED'  b'ALCOHOLIC DRINKS'   b'0.00'  b'DRINK'   b'0.00'   \n",
       "...              ...                  ...       ...       ...       ...   \n",
       "1727923     b'TAKEN'             b'TOTAL'  b'16.00'      b'g'  b'16.00'   \n",
       "1727924     b'TAKEN'             b'TOTAL'  b'15.60'      b'g'  b'15.60'   \n",
       "1727925     b'TAKEN'             b'TOTAL'  b'15.60'      b'g'  b'15.60'   \n",
       "1727926     b'TAKEN'             b'TOTAL'  b'15.60'      b'g'  b'15.60'   \n",
       "1727927     b'TAKEN'             b'TOTAL'  b'39.00'      b'g'  b'39.00'   \n",
       "\n",
       "             FASTRESN  FASTRESU FAMETHOD            FAANMETH         FADTC  \\\n",
       "0        5.397605e-79  b'DRINK'  b'RFPM'  b'FNDDS 2017-2018'  1.944835e+09   \n",
       "1        5.397605e-79  b'DRINK'  b'RFPM'  b'FNDDS 2017-2018'  1.944835e+09   \n",
       "2        5.397605e-79  b'DRINK'  b'RFPM'  b'FNDDS 2017-2018'  1.944835e+09   \n",
       "3        5.397605e-79  b'DRINK'  b'RFPM'  b'FNDDS 2017-2018'  1.944648e+09   \n",
       "4        5.397605e-79  b'DRINK'  b'RFPM'  b'FNDDS 2017-2018'  1.944648e+09   \n",
       "...               ...       ...      ...                 ...           ...   \n",
       "1727923  1.600000e+01      b'g'  b'RFPM'  b'FNDDS 2017-2018'  1.949328e+09   \n",
       "1727924  1.560000e+01      b'g'  b'RFPM'  b'FNDDS 2017-2018'  1.949414e+09   \n",
       "1727925  1.560000e+01      b'g'  b'RFPM'  b'FNDDS 2017-2018'  1.949739e+09   \n",
       "1727926  1.560000e+01      b'g'  b'RFPM'  b'FNDDS 2017-2018'  1.949749e+09   \n",
       "1727927  3.900000e+01      b'g'  b'RFPM'  b'FNDDS 2017-2018'  1.949382e+09   \n",
       "\n",
       "         FPANLTYP  \n",
       "0        b'Photo'  \n",
       "1        b'Photo'  \n",
       "2        b'Photo'  \n",
       "3        b'Photo'  \n",
       "4        b'Photo'  \n",
       "...           ...  \n",
       "1727923  b'Photo'  \n",
       "1727924  b'Photo'  \n",
       "1727925  b'Photo'  \n",
       "1727926  b'Photo'  \n",
       "1727927  b'Photo'  \n",
       "\n",
       "[1727928 rows x 19 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEXI_folder = '/home/ma98/T1DEXIP/'\n",
    "\n",
    "T1DEXIP_cgm = pd.read_sas(os.path.join(DEXI_folder, 'FAMLPI.xpt'), format=\"xport\")\n",
    "# T1DEXIP_cgm['LBDTC'] = pd.to_datetime(T1DEXIP_cgm['LBDTC'])\n",
    "\n",
    "# T1DEXIP_cgm['LBTEST'] = T1DEXIP_cgm['LBTEST'].astype(str) \n",
    "# # T1DEXIP_cgm['LBTEST'] = T1DEXIP_cgm['LBTEST'].str[2:-1] \n",
    "# T1DEXIP_cgm['USUBJID'] = T1DEXIP_cgm['USUBJID'].astype(int)\n",
    "# T1DEXIP_cgm['LBDTC'] = pd.to_datetime(T1DEXIP_cgm['LBDTC'], unit='s')\n",
    "# T1DEXIP_cgm['LBDTC'] = T1DEXIP_cgm['LBDTC'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# T1DEXIP_cgm = T1DEXIP_cgm[['USUBJID','LBSTRESN','LBDTC','LBTEST']]\n",
    "\n",
    "print(np.unique(T1DEXIP_cgm[\"FAMETHOD\"]))\n",
    "print(np.unique(T1DEXIP_cgm[\"FACAT\"]))\n",
    "print(np.unique(T1DEXIP_cgm[\"FASCAT\"]))\n",
    "\n",
    "T1DEXIP_cgm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1663bdfe-e32f-4341-9d3d-548926048f06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file size: 0.108292 bytes\n",
      "Feather file size: 0.225522 bytes\n",
      "CSV (gzip) file size: 0.048944 bytes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USUBJID</th>\n",
       "      <th>FAMETHOD</th>\n",
       "      <th>FACAT</th>\n",
       "      <th>FASCAT</th>\n",
       "      <th>FATEST</th>\n",
       "      <th>FADTC</th>\n",
       "      <th>FASTRESN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>100</td>\n",
       "      <td>SELF-REPORT</td>\n",
       "      <td>Consumed</td>\n",
       "      <td>MACRONUTRIENTS</td>\n",
       "      <td>Dietary Total Carbohydrate</td>\n",
       "      <td>2031-08-14 11:21:47</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>100</td>\n",
       "      <td>SELF-REPORT</td>\n",
       "      <td>Consumed</td>\n",
       "      <td>MACRONUTRIENTS</td>\n",
       "      <td>Dietary Total Carbohydrate</td>\n",
       "      <td>2031-08-15 12:19:35</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>100</td>\n",
       "      <td>SELF-REPORT</td>\n",
       "      <td>Consumed</td>\n",
       "      <td>MACRONUTRIENTS</td>\n",
       "      <td>Dietary Total Carbohydrate</td>\n",
       "      <td>2031-08-16 11:58:08</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>100</td>\n",
       "      <td>SELF-REPORT</td>\n",
       "      <td>Consumed</td>\n",
       "      <td>MACRONUTRIENTS</td>\n",
       "      <td>Dietary Total Carbohydrate</td>\n",
       "      <td>2031-08-17 11:52:00</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>100</td>\n",
       "      <td>SELF-REPORT</td>\n",
       "      <td>Consumed</td>\n",
       "      <td>MACRONUTRIENTS</td>\n",
       "      <td>Dietary Total Carbohydrate</td>\n",
       "      <td>2031-08-18 12:00:30</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>100</td>\n",
       "      <td>SELF-REPORT</td>\n",
       "      <td>Consumed</td>\n",
       "      <td>MACRONUTRIENTS</td>\n",
       "      <td>Dietary Total Carbohydrate</td>\n",
       "      <td>2031-08-16 20:02:17</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>100</td>\n",
       "      <td>SELF-REPORT</td>\n",
       "      <td>Consumed</td>\n",
       "      <td>MACRONUTRIENTS</td>\n",
       "      <td>Dietary Total Carbohydrate</td>\n",
       "      <td>2031-08-17 19:43:39</td>\n",
       "      <td>80.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>100</td>\n",
       "      <td>SELF-REPORT</td>\n",
       "      <td>Consumed</td>\n",
       "      <td>MACRONUTRIENTS</td>\n",
       "      <td>Dietary Total Carbohydrate</td>\n",
       "      <td>2031-08-18 19:53:07</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>100</td>\n",
       "      <td>SELF-REPORT</td>\n",
       "      <td>Consumed</td>\n",
       "      <td>MACRONUTRIENTS</td>\n",
       "      <td>Dietary Total Carbohydrate</td>\n",
       "      <td>2031-08-14 14:47:43</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>100</td>\n",
       "      <td>SELF-REPORT</td>\n",
       "      <td>Consumed</td>\n",
       "      <td>MACRONUTRIENTS</td>\n",
       "      <td>Dietary Total Carbohydrate</td>\n",
       "      <td>2031-08-16 15:08:00</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>100</td>\n",
       "      <td>SELF-REPORT</td>\n",
       "      <td>Consumed</td>\n",
       "      <td>MACRONUTRIENTS</td>\n",
       "      <td>Dietary Total Carbohydrate</td>\n",
       "      <td>2031-08-17 15:43:00</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>100</td>\n",
       "      <td>SELF-REPORT</td>\n",
       "      <td>Consumed</td>\n",
       "      <td>MACRONUTRIENTS</td>\n",
       "      <td>Dietary Total Carbohydrate</td>\n",
       "      <td>2031-08-18 15:55:00</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>100</td>\n",
       "      <td>RFPM</td>\n",
       "      <td>CONSUMED</td>\n",
       "      <td>MACRONUTRIENTS</td>\n",
       "      <td>Dietary Total Carbohydrate</td>\n",
       "      <td>2031-08-15 12:19:35</td>\n",
       "      <td>49.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>100</td>\n",
       "      <td>RFPM</td>\n",
       "      <td>CONSUMED</td>\n",
       "      <td>MACRONUTRIENTS</td>\n",
       "      <td>Dietary Total Carbohydrate</td>\n",
       "      <td>2031-08-16 11:58:08</td>\n",
       "      <td>29.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>100</td>\n",
       "      <td>RFPM</td>\n",
       "      <td>CONSUMED</td>\n",
       "      <td>MACRONUTRIENTS</td>\n",
       "      <td>Dietary Total Carbohydrate</td>\n",
       "      <td>2031-08-18 12:00:30</td>\n",
       "      <td>23.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>100</td>\n",
       "      <td>RFPM</td>\n",
       "      <td>CONSUMED</td>\n",
       "      <td>MACRONUTRIENTS</td>\n",
       "      <td>Dietary Total Carbohydrate</td>\n",
       "      <td>2031-08-16 20:02:17</td>\n",
       "      <td>58.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>100</td>\n",
       "      <td>RFPM</td>\n",
       "      <td>CONSUMED</td>\n",
       "      <td>MACRONUTRIENTS</td>\n",
       "      <td>Dietary Total Carbohydrate</td>\n",
       "      <td>2031-08-18 19:53:07</td>\n",
       "      <td>79.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>100</td>\n",
       "      <td>RFPM</td>\n",
       "      <td>CONSUMED</td>\n",
       "      <td>MACRONUTRIENTS</td>\n",
       "      <td>Dietary Total Carbohydrate</td>\n",
       "      <td>2031-08-16 15:08:00</td>\n",
       "      <td>38.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>100</td>\n",
       "      <td>RFPM</td>\n",
       "      <td>CONSUMED</td>\n",
       "      <td>MACRONUTRIENTS</td>\n",
       "      <td>Dietary Total Carbohydrate</td>\n",
       "      <td>2031-08-18 15:55:00</td>\n",
       "      <td>36.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2945</th>\n",
       "      <td>101</td>\n",
       "      <td>SELF-REPORT</td>\n",
       "      <td>Consumed</td>\n",
       "      <td>MACRONUTRIENTS</td>\n",
       "      <td>Dietary Total Carbohydrate</td>\n",
       "      <td>2031-12-26 11:04:00</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2946</th>\n",
       "      <td>101</td>\n",
       "      <td>SELF-REPORT</td>\n",
       "      <td>Consumed</td>\n",
       "      <td>MACRONUTRIENTS</td>\n",
       "      <td>Dietary Total Carbohydrate</td>\n",
       "      <td>2031-12-27 07:43:07</td>\n",
       "      <td>66.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2947</th>\n",
       "      <td>101</td>\n",
       "      <td>SELF-REPORT</td>\n",
       "      <td>Consumed</td>\n",
       "      <td>MACRONUTRIENTS</td>\n",
       "      <td>Dietary Total Carbohydrate</td>\n",
       "      <td>2031-12-29 08:15:40</td>\n",
       "      <td>90.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2948</th>\n",
       "      <td>101</td>\n",
       "      <td>SELF-REPORT</td>\n",
       "      <td>Consumed</td>\n",
       "      <td>MACRONUTRIENTS</td>\n",
       "      <td>Dietary Total Carbohydrate</td>\n",
       "      <td>2031-12-31 07:31:10</td>\n",
       "      <td>70.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2949</th>\n",
       "      <td>101</td>\n",
       "      <td>SELF-REPORT</td>\n",
       "      <td>Consumed</td>\n",
       "      <td>MACRONUTRIENTS</td>\n",
       "      <td>Dietary Total Carbohydrate</td>\n",
       "      <td>2032-01-01 09:40:05</td>\n",
       "      <td>62.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2950</th>\n",
       "      <td>101</td>\n",
       "      <td>SELF-REPORT</td>\n",
       "      <td>Consumed</td>\n",
       "      <td>MACRONUTRIENTS</td>\n",
       "      <td>Dietary Total Carbohydrate</td>\n",
       "      <td>2031-12-26 20:37:43</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2951</th>\n",
       "      <td>101</td>\n",
       "      <td>SELF-REPORT</td>\n",
       "      <td>Consumed</td>\n",
       "      <td>MACRONUTRIENTS</td>\n",
       "      <td>Dietary Total Carbohydrate</td>\n",
       "      <td>2031-12-27 15:48:00</td>\n",
       "      <td>97.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2952</th>\n",
       "      <td>101</td>\n",
       "      <td>SELF-REPORT</td>\n",
       "      <td>Consumed</td>\n",
       "      <td>MACRONUTRIENTS</td>\n",
       "      <td>Dietary Total Carbohydrate</td>\n",
       "      <td>2031-12-27 19:40:18</td>\n",
       "      <td>70.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2953</th>\n",
       "      <td>101</td>\n",
       "      <td>SELF-REPORT</td>\n",
       "      <td>Consumed</td>\n",
       "      <td>MACRONUTRIENTS</td>\n",
       "      <td>Dietary Total Carbohydrate</td>\n",
       "      <td>2031-12-28 20:30:44</td>\n",
       "      <td>60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2954</th>\n",
       "      <td>101</td>\n",
       "      <td>SELF-REPORT</td>\n",
       "      <td>Consumed</td>\n",
       "      <td>MACRONUTRIENTS</td>\n",
       "      <td>Dietary Total Carbohydrate</td>\n",
       "      <td>2031-12-26 14:26:40</td>\n",
       "      <td>62.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2955</th>\n",
       "      <td>101</td>\n",
       "      <td>SELF-REPORT</td>\n",
       "      <td>Consumed</td>\n",
       "      <td>MACRONUTRIENTS</td>\n",
       "      <td>Dietary Total Carbohydrate</td>\n",
       "      <td>2031-12-27 11:08:16</td>\n",
       "      <td>72.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2956</th>\n",
       "      <td>101</td>\n",
       "      <td>SELF-REPORT</td>\n",
       "      <td>Consumed</td>\n",
       "      <td>MACRONUTRIENTS</td>\n",
       "      <td>Dietary Total Carbohydrate</td>\n",
       "      <td>2031-12-28 11:27:14</td>\n",
       "      <td>70.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2957</th>\n",
       "      <td>101</td>\n",
       "      <td>SELF-REPORT</td>\n",
       "      <td>Consumed</td>\n",
       "      <td>MACRONUTRIENTS</td>\n",
       "      <td>Dietary Total Carbohydrate</td>\n",
       "      <td>2031-12-29 12:55:00</td>\n",
       "      <td>80.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2958</th>\n",
       "      <td>101</td>\n",
       "      <td>SELF-REPORT</td>\n",
       "      <td>Consumed</td>\n",
       "      <td>MACRONUTRIENTS</td>\n",
       "      <td>Dietary Total Carbohydrate</td>\n",
       "      <td>2031-12-31 11:05:13</td>\n",
       "      <td>62.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2959</th>\n",
       "      <td>101</td>\n",
       "      <td>SELF-REPORT</td>\n",
       "      <td>Consumed</td>\n",
       "      <td>MACRONUTRIENTS</td>\n",
       "      <td>Dietary Total Carbohydrate</td>\n",
       "      <td>2032-01-01 14:58:30</td>\n",
       "      <td>120.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2960</th>\n",
       "      <td>101</td>\n",
       "      <td>SELF-REPORT</td>\n",
       "      <td>Consumed</td>\n",
       "      <td>MACRONUTRIENTS</td>\n",
       "      <td>Dietary Total Carbohydrate</td>\n",
       "      <td>2031-12-26 14:49:54</td>\n",
       "      <td>18.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2961</th>\n",
       "      <td>101</td>\n",
       "      <td>SELF-REPORT</td>\n",
       "      <td>Consumed</td>\n",
       "      <td>MACRONUTRIENTS</td>\n",
       "      <td>Dietary Total Carbohydrate</td>\n",
       "      <td>2031-12-26 18:05:07</td>\n",
       "      <td>25.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2962</th>\n",
       "      <td>101</td>\n",
       "      <td>SELF-REPORT</td>\n",
       "      <td>Consumed</td>\n",
       "      <td>MACRONUTRIENTS</td>\n",
       "      <td>Dietary Total Carbohydrate</td>\n",
       "      <td>2031-12-26 21:00:10</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2963</th>\n",
       "      <td>101</td>\n",
       "      <td>SELF-REPORT</td>\n",
       "      <td>Consumed</td>\n",
       "      <td>MACRONUTRIENTS</td>\n",
       "      <td>Dietary Total Carbohydrate</td>\n",
       "      <td>2031-12-27 13:08:40</td>\n",
       "      <td>32.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2964</th>\n",
       "      <td>101</td>\n",
       "      <td>SELF-REPORT</td>\n",
       "      <td>Consumed</td>\n",
       "      <td>MACRONUTRIENTS</td>\n",
       "      <td>Dietary Total Carbohydrate</td>\n",
       "      <td>2031-12-28 13:31:58</td>\n",
       "      <td>28.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2967</th>\n",
       "      <td>101</td>\n",
       "      <td>RFPM</td>\n",
       "      <td>CONSUMED</td>\n",
       "      <td>MACRONUTRIENTS</td>\n",
       "      <td>Dietary Total Carbohydrate</td>\n",
       "      <td>2031-12-26 11:04:00</td>\n",
       "      <td>68.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      USUBJID     FAMETHOD     FACAT          FASCAT  \\\n",
       "385       100  SELF-REPORT  Consumed  MACRONUTRIENTS   \n",
       "386       100  SELF-REPORT  Consumed  MACRONUTRIENTS   \n",
       "387       100  SELF-REPORT  Consumed  MACRONUTRIENTS   \n",
       "388       100  SELF-REPORT  Consumed  MACRONUTRIENTS   \n",
       "389       100  SELF-REPORT  Consumed  MACRONUTRIENTS   \n",
       "390       100  SELF-REPORT  Consumed  MACRONUTRIENTS   \n",
       "391       100  SELF-REPORT  Consumed  MACRONUTRIENTS   \n",
       "392       100  SELF-REPORT  Consumed  MACRONUTRIENTS   \n",
       "393       100  SELF-REPORT  Consumed  MACRONUTRIENTS   \n",
       "394       100  SELF-REPORT  Consumed  MACRONUTRIENTS   \n",
       "395       100  SELF-REPORT  Consumed  MACRONUTRIENTS   \n",
       "396       100  SELF-REPORT  Consumed  MACRONUTRIENTS   \n",
       "399       100         RFPM  CONSUMED  MACRONUTRIENTS   \n",
       "402       100         RFPM  CONSUMED  MACRONUTRIENTS   \n",
       "405       100         RFPM  CONSUMED  MACRONUTRIENTS   \n",
       "408       100         RFPM  CONSUMED  MACRONUTRIENTS   \n",
       "411       100         RFPM  CONSUMED  MACRONUTRIENTS   \n",
       "414       100         RFPM  CONSUMED  MACRONUTRIENTS   \n",
       "417       100         RFPM  CONSUMED  MACRONUTRIENTS   \n",
       "2945      101  SELF-REPORT  Consumed  MACRONUTRIENTS   \n",
       "2946      101  SELF-REPORT  Consumed  MACRONUTRIENTS   \n",
       "2947      101  SELF-REPORT  Consumed  MACRONUTRIENTS   \n",
       "2948      101  SELF-REPORT  Consumed  MACRONUTRIENTS   \n",
       "2949      101  SELF-REPORT  Consumed  MACRONUTRIENTS   \n",
       "2950      101  SELF-REPORT  Consumed  MACRONUTRIENTS   \n",
       "2951      101  SELF-REPORT  Consumed  MACRONUTRIENTS   \n",
       "2952      101  SELF-REPORT  Consumed  MACRONUTRIENTS   \n",
       "2953      101  SELF-REPORT  Consumed  MACRONUTRIENTS   \n",
       "2954      101  SELF-REPORT  Consumed  MACRONUTRIENTS   \n",
       "2955      101  SELF-REPORT  Consumed  MACRONUTRIENTS   \n",
       "2956      101  SELF-REPORT  Consumed  MACRONUTRIENTS   \n",
       "2957      101  SELF-REPORT  Consumed  MACRONUTRIENTS   \n",
       "2958      101  SELF-REPORT  Consumed  MACRONUTRIENTS   \n",
       "2959      101  SELF-REPORT  Consumed  MACRONUTRIENTS   \n",
       "2960      101  SELF-REPORT  Consumed  MACRONUTRIENTS   \n",
       "2961      101  SELF-REPORT  Consumed  MACRONUTRIENTS   \n",
       "2962      101  SELF-REPORT  Consumed  MACRONUTRIENTS   \n",
       "2963      101  SELF-REPORT  Consumed  MACRONUTRIENTS   \n",
       "2964      101  SELF-REPORT  Consumed  MACRONUTRIENTS   \n",
       "2967      101         RFPM  CONSUMED  MACRONUTRIENTS   \n",
       "\n",
       "                          FATEST                FADTC  FASTRESN  \n",
       "385   Dietary Total Carbohydrate  2031-08-14 11:21:47     50.00  \n",
       "386   Dietary Total Carbohydrate  2031-08-15 12:19:35     50.00  \n",
       "387   Dietary Total Carbohydrate  2031-08-16 11:58:08     50.00  \n",
       "388   Dietary Total Carbohydrate  2031-08-17 11:52:00     50.00  \n",
       "389   Dietary Total Carbohydrate  2031-08-18 12:00:30     50.00  \n",
       "390   Dietary Total Carbohydrate  2031-08-16 20:02:17    100.00  \n",
       "391   Dietary Total Carbohydrate  2031-08-17 19:43:39     80.00  \n",
       "392   Dietary Total Carbohydrate  2031-08-18 19:53:07     50.00  \n",
       "393   Dietary Total Carbohydrate  2031-08-14 14:47:43     50.00  \n",
       "394   Dietary Total Carbohydrate  2031-08-16 15:08:00     50.00  \n",
       "395   Dietary Total Carbohydrate  2031-08-17 15:43:00     50.00  \n",
       "396   Dietary Total Carbohydrate  2031-08-18 15:55:00     50.00  \n",
       "399   Dietary Total Carbohydrate  2031-08-15 12:19:35     49.68  \n",
       "402   Dietary Total Carbohydrate  2031-08-16 11:58:08     29.01  \n",
       "405   Dietary Total Carbohydrate  2031-08-18 12:00:30     23.85  \n",
       "408   Dietary Total Carbohydrate  2031-08-16 20:02:17     58.41  \n",
       "411   Dietary Total Carbohydrate  2031-08-18 19:53:07     79.61  \n",
       "414   Dietary Total Carbohydrate  2031-08-16 15:08:00     38.52  \n",
       "417   Dietary Total Carbohydrate  2031-08-18 15:55:00     36.63  \n",
       "2945  Dietary Total Carbohydrate  2031-12-26 11:04:00     50.00  \n",
       "2946  Dietary Total Carbohydrate  2031-12-27 07:43:07     66.00  \n",
       "2947  Dietary Total Carbohydrate  2031-12-29 08:15:40     90.00  \n",
       "2948  Dietary Total Carbohydrate  2031-12-31 07:31:10     70.00  \n",
       "2949  Dietary Total Carbohydrate  2032-01-01 09:40:05     62.00  \n",
       "2950  Dietary Total Carbohydrate  2031-12-26 20:37:43     50.00  \n",
       "2951  Dietary Total Carbohydrate  2031-12-27 15:48:00     97.00  \n",
       "2952  Dietary Total Carbohydrate  2031-12-27 19:40:18     70.00  \n",
       "2953  Dietary Total Carbohydrate  2031-12-28 20:30:44     60.00  \n",
       "2954  Dietary Total Carbohydrate  2031-12-26 14:26:40     62.00  \n",
       "2955  Dietary Total Carbohydrate  2031-12-27 11:08:16     72.00  \n",
       "2956  Dietary Total Carbohydrate  2031-12-28 11:27:14     70.00  \n",
       "2957  Dietary Total Carbohydrate  2031-12-29 12:55:00     80.00  \n",
       "2958  Dietary Total Carbohydrate  2031-12-31 11:05:13     62.00  \n",
       "2959  Dietary Total Carbohydrate  2032-01-01 14:58:30    120.00  \n",
       "2960  Dietary Total Carbohydrate  2031-12-26 14:49:54     18.00  \n",
       "2961  Dietary Total Carbohydrate  2031-12-26 18:05:07     25.00  \n",
       "2962  Dietary Total Carbohydrate  2031-12-26 21:00:10     30.00  \n",
       "2963  Dietary Total Carbohydrate  2031-12-27 13:08:40     32.00  \n",
       "2964  Dietary Total Carbohydrate  2031-12-28 13:31:58     28.00  \n",
       "2967  Dietary Total Carbohydrate  2031-12-26 11:04:00     68.78  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEXI_folder = '/home/ma98/T1DEXIP/'\n",
    "\n",
    "T1DEXIP_cgm = pd.read_sas(os.path.join(DEXI_folder, 'FAMLPM.xpt'), format=\"xport\")\n",
    "\n",
    "T1DEXIP_cgm['FAMETHOD'] = T1DEXIP_cgm['FAMETHOD'].astype(str) \n",
    "T1DEXIP_cgm['FACAT'] = T1DEXIP_cgm['FACAT'].astype(str) \n",
    "T1DEXIP_cgm['FASCAT'] = T1DEXIP_cgm['FASCAT'].astype(str) \n",
    "T1DEXIP_cgm['FATEST'] = T1DEXIP_cgm['FATEST'].astype(str) \n",
    "\n",
    "T1DEXIP_cgm['USUBJID'] = T1DEXIP_cgm['USUBJID'].astype(int)\n",
    "T1DEXIP_cgm = T1DEXIP_cgm[T1DEXIP_cgm['FACAT'].isin(['CONSUMED', 'Consumed'])]\n",
    "\n",
    "T1DEXIP_cgm = T1DEXIP_cgm[T1DEXIP_cgm['FATEST']=='Dietary Total Carbohydrate']\n",
    "T1DEXIP_cgm['FADTC'] = pd.to_datetime(T1DEXIP_cgm['FADTC'], unit='s')\n",
    "T1DEXIP_cgm['FADTC'] = T1DEXIP_cgm['FADTC'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "T1DEXIP_cgm = T1DEXIP_cgm[['USUBJID','FAMETHOD','FACAT','FASCAT','FATEST','FADTC','FASTRESN']]\n",
    "df = T1DEXIP_cgm\n",
    "\n",
    "# # # Parquet\n",
    "df.to_parquet('pediatricCarbs.parquet', compression='snappy')\n",
    "parquet_size = os.path.getsize('pediatricCarbs.parquet')\n",
    "\n",
    "# Feather\n",
    "df.to_feather('pediatricCarbs.feather')\n",
    "feather_size = os.path.getsize('pediatricCarbs.feather')\n",
    "\n",
    "# CSV with gzip\n",
    "df.to_csv('pediatricCarbs.csv.gz', index=False, compression='gzip')\n",
    "csv_size = os.path.getsize('pediatricCarbs.csv.gz')\n",
    "\n",
    "print(f\"Parquet file size: {parquet_size/1000000} bytes\")\n",
    "print(f\"Feather file size: {feather_size/1000000} bytes\")\n",
    "print(f\"CSV (gzip) file size: {csv_size/1000000} bytes\")\n",
    "\n",
    "T1DEXIP_cgm[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c77be62c-0859-4ddc-9327-548d02055465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    }
   ],
   "source": [
    "DEXI_folder = '/home/ma98/T1DEXIP/Compressed/'\n",
    "\n",
    "T1DEXI_CGM_Dataset = pd.read_parquet(os.path.join(DEXI_folder, 'pediatricCGM.parquet')) \n",
    "T1DEXI_CGM_Dataset['LBDTC'] = pd.to_datetime(T1DEXI_CGM_Dataset['LBDTC'])\n",
    "\n",
    "T1DEXI_basal = pd.read_parquet(os.path.join(DEXI_folder, 'pediatricBasal.parquet'))  \n",
    "T1DEXI_basal['FADTC'] = pd.to_datetime(T1DEXI_basal['FADTC'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "FAMLPM_Dataset = pd.read_parquet(os.path.join(DEXI_folder, 'pediatricCarbs.parquet')) \n",
    "FAMLPM_Dataset['FADTC'] = pd.to_datetime(FAMLPM_Dataset['FADTC'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "T1DEXI_age_Dataset = pd.read_parquet(os.path.join(DEXI_folder, 'pediatricAge.parquet')) \n",
    "\n",
    "T1DEXI_VS_Dataset = pd.read_csv(os.path.join(DEXI_folder, 'pediatricVS.csv.gz'))  \n",
    "T1DEXI_VS_Dataset['VSDTC'] = pd.to_datetime(T1DEXI_VS_Dataset['VSDTC'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "shared_participants = sorted(set(T1DEXI_CGM_Dataset['USUBJID']) & set(T1DEXI_basal['USUBJID'])&\n",
    "                             set(FAMLPM_Dataset['USUBJID']) & set(T1DEXI_VS_Dataset['USUBJID'])) \n",
    "\n",
    "print(len(shared_participants))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e40ae87b-e918-4c13-b983-3c09974a8d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261\n"
     ]
    }
   ],
   "source": [
    "DEXI_folder = '/home/ma98/T1DEXIP/Compressed/'\n",
    "\n",
    "T1DEXI_CGM_Dataset = pd.read_parquet(os.path.join(DEXI_folder, 'pediatricCGM.parquet')) \n",
    "T1DEXI_CGM_Dataset['LBDTC'] = pd.to_datetime(T1DEXI_CGM_Dataset['LBDTC'])\n",
    "\n",
    "shared_participants = sorted(set(T1DEXI_CGM_Dataset['USUBJID'])) \n",
    "\n",
    "print(len(shared_participants))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cef89b0-0e38-484c-87d8-9850747798ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cgm_prediction_stats(current_day_cgm):\n",
    "    general_stats = calculate_stats(current_day_cgm.values[:,0])\n",
    "    general_stats = extract_prediction_stats(general_stats, 'cgm')\n",
    "    masks = {\n",
    "        'night': (current_day_cgm.index.hour >= 0) & (current_day_cgm.index.hour < 6),\n",
    "        'early_night': (current_day_cgm.index.hour >= 0) & (current_day_cgm.index.hour < 3),\n",
    "        'late_night': (current_day_cgm.index.hour >= 3) & (current_day_cgm.index.hour < 6),\n",
    "        'long_night': (current_day_cgm.index.hour >= 0) & (current_day_cgm.index.hour < 8),\n",
    "        'night_morning': (current_day_cgm.index.hour >= 0) & (current_day_cgm.index.hour < 12),\n",
    "        'morning': (current_day_cgm.index.hour >= 6) & (current_day_cgm.index.hour < 12),\n",
    "        'afternoon': (current_day_cgm.index.hour >= 12) & (current_day_cgm.index.hour < 18),\n",
    "        'evening': (current_day_cgm.index.hour >= 18) & (current_day_cgm.index.hour < 24)\n",
    "    }\n",
    "    time_range_stats = {}\n",
    "    for period, mask in masks.items():\n",
    "        period_stats = calculate_stats(current_day_cgm, mask)\n",
    "        period_stats = extract_prediction_stats(period_stats, f'cgm_{period}')\n",
    "        time_range_stats.update(period_stats)\n",
    "    \n",
    "    time_in_range_stats = {\n",
    "        'time_in_range_70_140_day': calculate_time_in_range(current_day_cgm, 70, 140),\n",
    "        'time_in_range_70_180_day': calculate_time_in_range(current_day_cgm, 70, 180),\n",
    "        'time_below_range_70_day': calculate_time_in_range(current_day_cgm, 0, 70),\n",
    "        'time_above_range_140_day': calculate_time_in_range(current_day_cgm, 140, np.inf),\n",
    "        'time_above_range_180_day': calculate_time_in_range(current_day_cgm, 180, np.inf)\n",
    "    }\n",
    "    for period, mask in masks.items():\n",
    "        time_in_range_stats[f'time_in_range_70_140_{period}'] = calculate_time_in_range(current_day_cgm, 70, 140, mask)\n",
    "        time_in_range_stats[f'time_in_range_70_180_{period}'] = calculate_time_in_range(current_day_cgm, 70, 180, mask)\n",
    "        time_in_range_stats[f'time_below_range_70_{period}'] = calculate_time_in_range(current_day_cgm, 0, 70, mask)\n",
    "        time_in_range_stats[f'time_above_range_140_{period}'] = calculate_time_in_range(current_day_cgm, 140, np.inf, mask)\n",
    "        time_in_range_stats[f'time_above_range_180_{period}'] = calculate_time_in_range(current_day_cgm, 180, np.inf, mask)\n",
    "    \n",
    "    cgm_prediction_stats = pd.DataFrame({**general_stats, **time_range_stats, **time_in_range_stats})\n",
    "    return cgm_prediction_stats.astype(np.float16)\n",
    "def get_daily_cgm_stats(current_day_cgm):\n",
    "    general_stats = calculate_stats(current_day_cgm.values[:,0])\n",
    "    general_stats = extract_stats(general_stats, 'daily_cgm')\n",
    "    mean_crossings = calculate_crossings(current_day_cgm.values[:,0])\n",
    "    general_stats['daily_cgm_mean_crossings'] = mean_crossings\n",
    "\n",
    "    # Add new features\n",
    "    last_cgm_value = current_day_cgm.values[-1, 0]\n",
    "    general_stats['last_cgm_value'] = last_cgm_value\n",
    "    general_stats['last_cgm_sq'] = np.sqrt(last_cgm_value)\n",
    "    \n",
    "    diff_10_min = last_cgm_value - current_day_cgm.values[-3, 0]\n",
    "    general_stats['diff_10_min'] = diff_10_min\n",
    "\n",
    "    diff_20_min = last_cgm_value - current_day_cgm.values[-5, 0]\n",
    "    general_stats['diff_20_min'] = diff_20_min\n",
    "\n",
    "    diff_30_min = last_cgm_value - current_day_cgm.values[-7, 0]\n",
    "    general_stats['diff_30_min'] = diff_30_min\n",
    "\n",
    "    diff_50_min = last_cgm_value - current_day_cgm.values[-11, 0]\n",
    "    general_stats['diff_50_min'] = diff_50_min\n",
    "    \n",
    "    # Rate of change (slope) in CGM in the last hours\n",
    "    slope_last_half_hour = (last_cgm_value - current_day_cgm.values[-7, 0]) / last_cgm_value\n",
    "    general_stats['slope_last_half_hour'] = slope_last_half_hour\n",
    "    slope_last_hour = (last_cgm_value - current_day_cgm.values[-13, 0]) / last_cgm_value\n",
    "    general_stats['slope_last_hour'] = slope_last_hour\n",
    "    slope_last_2_hour = (last_cgm_value - current_day_cgm.values[-25, 0]) / last_cgm_value\n",
    "    general_stats['slope_last_2_hour'] = slope_last_2_hour\n",
    "    \n",
    "    # SD of CGM in the last hours\n",
    "    std_last_2_hours = np.std(current_day_cgm.values[-25:, 0])\n",
    "    general_stats['std_last_2_hours'] = std_last_2_hours\n",
    "    std_last_hour = np.std(current_day_cgm.values[-13:, 0])\n",
    "    general_stats['std_last_hour'] = std_last_hour\n",
    "    \n",
    "    # Sum of all increments in adjacent CGM observations in the last two hours\n",
    "    increments_sum_last_2_hours = np.sum(np.diff(current_day_cgm.values[-25:, 0])[np.diff(current_day_cgm.values[-25:, 0]) > 0])\n",
    "    general_stats['increments_sum_last_2_hours'] = increments_sum_last_2_hours\n",
    "\n",
    "    # Sum of all decrements in adjacent CGM observations in the last two hours\n",
    "    decrements_sum_last_2_hours = np.sum(np.diff(current_day_cgm.values[-25:, 0])[np.diff(current_day_cgm.values[-25:, 0]) < 0])\n",
    "    general_stats['decrements_sum_last_2_hours'] = decrements_sum_last_2_hours\n",
    "\n",
    "    # Maximum increase in adjacent CGM observations in the last two hours\n",
    "    max_increase_last_2_hours = np.max(np.diff(current_day_cgm.values[-25:, 0]))\n",
    "    general_stats['max_increase_last_2_hours'] = max_increase_last_2_hours\n",
    "\n",
    "    # Maximum decrease in adjacent CGM observations in the last two hours\n",
    "    max_decrease_last_2_hours = np.min(np.diff(current_day_cgm.values[-25:, 0]))\n",
    "    general_stats['max_decrease_last_2_hours'] = max_decrease_last_2_hours\n",
    "\n",
    "    masks = {\n",
    "        'night': (current_day_cgm.index.hour >= 0) & (current_day_cgm.index.hour < 6),\n",
    "        'morning': (current_day_cgm.index.hour >= 6) & (current_day_cgm.index.hour < 12),\n",
    "        'afternoon': (current_day_cgm.index.hour >= 12) & (current_day_cgm.index.hour < 18),\n",
    "        'evening': (current_day_cgm.index.hour >= 18) & (current_day_cgm.index.hour < 24),\n",
    "        'late_evening': (current_day_cgm.index.hour >= 21) & (current_day_cgm.index.hour < 24)\n",
    "    }\n",
    "    time_range_stats = {}\n",
    "    for period, mask in masks.items():\n",
    "        period_stats = calculate_stats(current_day_cgm, mask)\n",
    "        period_stats = extract_stats(period_stats, f'daily_cgm_{period}')\n",
    "        time_range_stats.update(period_stats)\n",
    "    \n",
    "    time_in_range_stats = {\n",
    "        'daily_time_in_range_70_140_day': calculate_time_in_range(current_day_cgm, 70, 140),\n",
    "        'daily_time_in_range_70_180_day': calculate_time_in_range(current_day_cgm, 70, 180),\n",
    "        'daily_time_below_range_70_day': calculate_time_in_range(current_day_cgm, 0, 70),\n",
    "        'daily_time_above_range_140_day': calculate_time_in_range(current_day_cgm, 140, np.inf),\n",
    "        'daily_time_above_range_180_day': calculate_time_in_range(current_day_cgm, 180, np.inf)\n",
    "    }\n",
    "    for period, mask in masks.items():\n",
    "        time_in_range_stats[f'daily_time_in_range_70_140_{period}'] = calculate_time_in_range(current_day_cgm, 70, 140, mask)\n",
    "        time_in_range_stats[f'daily_time_in_range_70_180_{period}'] = calculate_time_in_range(current_day_cgm, 70, 180, mask)\n",
    "        time_in_range_stats[f'daily_time_below_range_70_{period}'] = calculate_time_in_range(current_day_cgm, 0, 70, mask)\n",
    "        time_in_range_stats[f'daily_time_above_range_140_{period}'] = calculate_time_in_range(current_day_cgm, 140, np.inf, mask)\n",
    "        time_in_range_stats[f'daily_time_above_range_180_{period}'] = calculate_time_in_range(current_day_cgm, 180, np.inf, mask)\n",
    "    \n",
    "    daily_cgm_stats = pd.DataFrame({**general_stats, **time_range_stats, **time_in_range_stats})\n",
    "    return daily_cgm_stats\n",
    "\n",
    "def get_daily_cgm_stats(current_day_cgm):\n",
    "    general_stats = calculate_stats(current_day_cgm.values[:,0])\n",
    "    general_stats = extract_stats(general_stats, 'daily_cgm')\n",
    "    mean_crossings = calculate_crossings(current_day_cgm.values[:,0])\n",
    "    general_stats['daily_cgm_mean_crossings'] = mean_crossings\n",
    "\n",
    "    # Add new features\n",
    "    last_cgm_value = current_day_cgm.values[-1, 0]\n",
    "    general_stats['last_cgm_value'] = last_cgm_value #11:55\n",
    "    general_stats['last_cgm_sq'] = np.sqrt(last_cgm_value)\n",
    "    \n",
    "    general_stats['diff_10_min'] = last_cgm_value - current_day_cgm.values[-3, 0] #11:45\n",
    "    general_stats['diff_20_min'] = last_cgm_value - current_day_cgm.values[-5, 0] #11:35\n",
    "    general_stats['diff_30_min'] = last_cgm_value - current_day_cgm.values[-7, 0] #11:25\n",
    "    general_stats['diff_40_min'] = last_cgm_value - current_day_cgm.values[-9, 0] #11:15\n",
    "    general_stats['diff_50_min'] = last_cgm_value - current_day_cgm.values[-11, 0] #11:05\n",
    "    \n",
    "    # Rate of change (slope) in CGM in the last hours\n",
    "    general_stats['slope_last_half_hour'] = (last_cgm_value - current_day_cgm.values[-6, 0]) / last_cgm_value #11:30\n",
    "    general_stats['slope_last_hour'] = (last_cgm_value - current_day_cgm.values[-13, 0]) / last_cgm_value #10:55\n",
    "    general_stats['slope_last_hour_half'] = (last_cgm_value - current_day_cgm.values[-19, 0]) / last_cgm_value #10:25\n",
    "    general_stats['slope_last_2_hour'] = (last_cgm_value - current_day_cgm.values[-25, 0]) / last_cgm_value #9:55\n",
    "    \n",
    "    # SD of CGM in the last hours\n",
    "    general_stats['std_last_2_hours'] = np.std(current_day_cgm.values[-25:, 0]) #9:55\n",
    "    general_stats['std_last_hour'] = np.std(current_day_cgm.values[-13:, 0]) #10:55\n",
    "    \n",
    "    # Sum of all increments in adjacent CGM observations in the last two hours\n",
    "    increments_sum_last_2_hours = np.sum(np.diff(current_day_cgm.values[-25:, 0])[np.diff(current_day_cgm.values[-25:, 0]) > 0])\n",
    "    general_stats['increments_sum_last_2_hours'] = increments_sum_last_2_hours\n",
    "\n",
    "    # Sum of all decrements in adjacent CGM observations in the last two hours\n",
    "    decrements_sum_last_2_hours = np.sum(np.diff(current_day_cgm.values[-25:, 0])[np.diff(current_day_cgm.values[-25:, 0]) < 0])\n",
    "    general_stats['decrements_sum_last_2_hours'] = decrements_sum_last_2_hours\n",
    "\n",
    "    # Maximum increase in adjacent CGM observations in the last two hours\n",
    "    max_increase_last_2_hours = np.max(np.diff(current_day_cgm.values[-25:, 0]))\n",
    "    general_stats['max_increase_last_2_hours'] = max_increase_last_2_hours\n",
    "\n",
    "    # Maximum decrease in adjacent CGM observations in the last two hours\n",
    "    max_decrease_last_2_hours = np.min(np.diff(current_day_cgm.values[-25:, 0]))\n",
    "    general_stats['max_decrease_last_2_hours'] = max_decrease_last_2_hours\n",
    "    masks = {\n",
    "        'night': (current_day_cgm.index.hour >= 0) & (current_day_cgm.index.hour < 6),\n",
    "        'morning': (current_day_cgm.index.hour >= 6) & (current_day_cgm.index.hour < 12),\n",
    "        'afternoon': (current_day_cgm.index.hour >= 12) & (current_day_cgm.index.hour < 18),\n",
    "        'evening': (current_day_cgm.index.hour >= 18) & (current_day_cgm.index.hour < 24),\n",
    "        'late_evening': (current_day_cgm.index.hour >= 21) & (current_day_cgm.index.hour < 24)\n",
    "    }\n",
    "    time_range_stats = {}\n",
    "    for period, mask in masks.items():\n",
    "        period_stats = calculate_stats(current_day_cgm, mask)\n",
    "        period_stats = extract_stats(period_stats, f'daily_cgm_{period}')\n",
    "        time_range_stats.update(period_stats)\n",
    "\n",
    "    time_in_range_stats = {\n",
    "        'daily_time_in_range_70_140_day': calculate_time_in_range(current_day_cgm, 70, 140),\n",
    "        'daily_time_in_range_70_180_day': calculate_time_in_range(current_day_cgm, 70, 180),\n",
    "        'daily_time_below_range_70_day': calculate_time_in_range(current_day_cgm, 0, 70),\n",
    "        'daily_time_above_range_140_day': calculate_time_in_range(current_day_cgm, 140, np.inf),\n",
    "        'daily_time_above_range_180_day': calculate_time_in_range(current_day_cgm, 180, np.inf)\n",
    "    }\n",
    "    for period, mask in masks.items():\n",
    "        time_in_range_stats[f'daily_time_in_range_70_140_{period}'] = calculate_time_in_range(current_day_cgm, 70, 140, mask)\n",
    "        time_in_range_stats[f'daily_time_in_range_70_180_{period}'] = calculate_time_in_range(current_day_cgm, 70, 180, mask)\n",
    "        time_in_range_stats[f'daily_time_below_range_70_{period}'] = calculate_time_in_range(current_day_cgm, 0, 70, mask)\n",
    "        time_in_range_stats[f'daily_time_above_range_140_{period}'] = calculate_time_in_range(current_day_cgm, 140, np.inf, mask)\n",
    "        time_in_range_stats[f'daily_time_above_range_180_{period}'] = calculate_time_in_range(current_day_cgm, 180, np.inf, mask)\n",
    "    \n",
    "    daily_cgm_stats = pd.DataFrame({**general_stats, **time_range_stats, **time_in_range_stats})\n",
    "    return daily_cgm_stats.astype(np.float16)\n",
    "\n",
    "def daily_cgm_labels(current_day_cgm):\n",
    "    masks = {\n",
    "        'night': (current_day_cgm.index.hour >= 0) & (current_day_cgm.index.hour < 6),\n",
    "        'early_night': (current_day_cgm.index.hour >= 0) & (current_day_cgm.index.hour < 3),\n",
    "        'late_night': (current_day_cgm.index.hour >= 3) & (current_day_cgm.index.hour < 6),\n",
    "        'long_night': (current_day_cgm.index.hour >= 0) & (current_day_cgm.index.hour < 8),\n",
    "        'night_morning': (current_day_cgm.index.hour >= 0) & (current_day_cgm.index.hour < 12),\n",
    "        'morning': (current_day_cgm.index.hour >= 6) & (current_day_cgm.index.hour < 12),\n",
    "        'afternoon': (current_day_cgm.index.hour >= 12) & (current_day_cgm.index.hour < 18),\n",
    "        'evening': (current_day_cgm.index.hour >= 18) & (current_day_cgm.index.hour < 24)\n",
    "    }\n",
    "    labels = {\n",
    "        'hyper_day': return_label(current_day_cgm.values[:,0], 180),\n",
    "        'hypo_day': return_label(current_day_cgm.values[:,0], 70)\n",
    "    }\n",
    "    for period, mask in masks.items():\n",
    "        labels[f'hyper_{period}'] = return_label(current_day_cgm.values[:,0], 180, mask)\n",
    "        labels[f'hypo_{period}'] = return_label(current_day_cgm.values[:,0], 70, mask)\n",
    "    \n",
    "    daily_cgm_labels = pd.DataFrame({**labels}, index=[0])\n",
    "    return daily_cgm_labels\n",
    "# --- Calculate Time in Range for Different Ranges and Periods ---\n",
    "def calculate_time_in_range(glucose_data, range_lower, range_upper, period_mask=None):\n",
    "\n",
    "    if period_mask is not None:\n",
    "        glucose_data = glucose_data[period_mask]\n",
    "    in_range = (glucose_data <= range_upper) & (glucose_data >= range_lower)\n",
    "    return 100 * in_range.sum() / len(glucose_data) if len(glucose_data) > 0 else 0\n",
    "def return_label(glucose_data, range, period_mask=None):\n",
    "    if period_mask is not None:\n",
    "        glucose_data = glucose_data[period_mask]\n",
    "    if range==180:\n",
    "        return np.sum(glucose_data>range)>2\n",
    "    if range==70:\n",
    "        return np.sum(glucose_data<range)>2\n",
    "def calculate_sleep_statistics(daily_sleep_data):\n",
    "    participant_total_sleep = participant_sleep_data[participant_sleep_data['NVTEST'] == \"Total Sleep Time\"]\n",
    "    participant_deep_sleep = participant_sleep_data[participant_sleep_data['NVTEST'] == \"Deep NREM Duration\"]\n",
    "    participant_light_sleep = participant_sleep_data[participant_sleep_data['NVTEST'] == \"Light NREM Duration\"]\n",
    "    participant_NREM_sleep = participant_sleep_data[participant_sleep_data['NVTEST'] == \"NREM Duration\"]\n",
    "    participant_REM_sleep = participant_sleep_data[participant_sleep_data['NVTEST'] == \"REM Duration\"]\n",
    "    participant_efficiency = participant_sleep_data[participant_sleep_data['NVTEST'] == \"Sleep Efficiency\"]\n",
    "    participant_awakenings = participant_sleep_data[participant_sleep_data['NVTEST'] == \"Number of Awakenings\"]\n",
    "    participant_latency = participant_sleep_data[participant_sleep_data['NVTEST'] == \"Sleep Onset Latency\"]\n",
    "\n",
    "    avg_total_sleep = (participant_total_sleep['NVORRES']).mean()\n",
    "    std_total_sleep = (participant_total_sleep['NVORRES']).std()\n",
    "    avg_deep_sleep = (participant_deep_sleep['NVORRES']).mean()\n",
    "    std_deep_sleep = (participant_deep_sleep['NVORRES']).std()\n",
    "    avg_light_sleep = (participant_light_sleep['NVORRES']).mean()\n",
    "    std_light_sleep = (participant_light_sleep['NVORRES']).std()\n",
    "    avg_NREM_sleep = (participant_NREM_sleep['NVORRES']).mean()\n",
    "    std_NREM_sleep = (participant_NREM_sleep['NVORRES']).std()\n",
    "    avg_REM_sleep = (participant_REM_sleep['NVORRES']).mean()\n",
    "    std_REM_sleep = (participant_REM_sleep['NVORRES']).std()\n",
    "    avg_awakenings = (participant_awakenings['NVORRES']*100000.).mean()\n",
    "    std_awakenings = (participant_awakenings['NVORRES']*100000. ).std()\n",
    "    avg_latency = (participant_latency['NVORRES']).mean()\n",
    "    std_latency = (participant_latency['NVORRES']).std()\n",
    "    avg_efficiency = (participant_efficiency['NVORRES']*100000.).mean()\n",
    "    std_efficiency = (participant_efficiency['NVORRES']*100000.).std()\n",
    "    # Calculate avg deviation from midnight\n",
    "    bedtime_deviations_midnight = []\n",
    "    wakeup_deviations_midnight = []\n",
    "    for dt in participant_total_sleep['NVDTC']:\n",
    "        sleep_time = 60*dt.hour + (dt.minute)\n",
    "        \n",
    "        if sleep_time >720:\n",
    "            sleep_time -= 1440\n",
    "        bedtime_deviations_midnight.append(sleep_time)\n",
    "    \n",
    "    for dt in participant_total_sleep['NVENDTC']:\n",
    "        wakeup_time = 60*dt.hour + (dt.minute)\n",
    "        wakeup_deviations_midnight.append(wakeup_time)\n",
    "  \n",
    "    average_bedtime_midnight = np.mean(bedtime_deviations_midnight)\n",
    "    average_wakeup_midnight = np.mean(wakeup_deviations_midnight)\n",
    "    std_bedtime_midnight = np.std(bedtime_deviations_midnight)\n",
    "    std_wakeup_midnight =  np.std(wakeup_deviations_midnight)\n",
    "   #     # Convert average deviation back to time format\n",
    "    average_bedtime = pd.to_datetime('00:00:00') + pd.to_timedelta(average_bedtime_midnight, unit='m')\n",
    "    average_wakeup = pd.to_datetime('00:00:00') + pd.to_timedelta(average_wakeup_midnight, unit='m')\n",
    "       \n",
    "    # Calculate bedtime deviations from average & variance & Consistency Score (inverse of variance)\n",
    "    participant_sleep_data['bedtime_from_avg'] = ((60*participant_sleep_data['NVDTC'].dt.hour + participant_sleep_data['NVDTC'].dt.minute) - average_bedtime_midnight)\n",
    "    participant_sleep_data.loc[participant_sleep_data['bedtime_from_avg'] > 720, 'bedtime_from_avg'] -= 1440\n",
    "\n",
    "    participant_sleep_data['wakeup_from_avg'] = ((60*participant_sleep_data['NVENDTC'].dt.hour + participant_sleep_data['NVENDTC'].dt.minute) - average_wakeup_midnight)\n",
    "    bedtime_std = np.round(participant_sleep_data['bedtime_from_avg'].std(),3)\n",
    "    wakeup_std = np.round(participant_sleep_data['wakeup_from_avg'].std(),3)\n",
    "    bedtime_var = np.round(participant_sleep_data['bedtime_from_avg'].var(),3)\n",
    "    wakeup_var = np.round(participant_sleep_data['wakeup_from_avg'].var(),3)\n",
    "    sleep_stats = pd.DataFrame({\n",
    "        'avg_bedtime_midnight': [average_bedtime_midnight.round(3)],\n",
    "        'avg_bedtime': [average_bedtime],\n",
    "        'bedtime_consistency_score': [(100. / bedtime_var).round(3)],\n",
    "        'bedtime_std': [bedtime_std],\n",
    "        'bedtime_var': [bedtime_var],\n",
    "        'avg_wakeup_midnight': [average_wakeup_midnight.round(3)],\n",
    "        'avg_wakeup': [average_wakeup],\n",
    "        'wakeup_consistency_score': [(100. / wakeup_var).round(3)],\n",
    "        'wakeup_std': [wakeup_std],\n",
    "        'wakeup_var': [wakeup_var],\n",
    "        'avg_deep_sleep': [round(avg_deep_sleep, 3)],\n",
    "        'std_deep_sleep': [round(std_deep_sleep, 3)],\n",
    "        'avg_total_sleep': [round(avg_total_sleep, 3)],\n",
    "        'std_total_sleep': [round(std_total_sleep, 3)],\n",
    "        'avg_light_sleep': [round(avg_light_sleep, 3)],\n",
    "        'std_light_sleep': [round(std_light_sleep, 3)],\n",
    "        'avg_NREM_sleep': [round(avg_NREM_sleep, 3)],\n",
    "        'std_NREM_sleep': [round(std_NREM_sleep, 3)],\n",
    "        'avg_REM_sleep': [round(avg_REM_sleep, 3)],\n",
    "        'std_REM_sleep': [round(std_REM_sleep, 3)],\n",
    "        'avg_awakenings': [round(avg_awakenings, 3)],\n",
    "        'std_awakenings': [round(std_awakenings, 3)],\n",
    "        'avg_latency': [round(avg_latency, 3)],\n",
    "        'std_latency': [round(std_latency, 3)],\n",
    "        'avg_efficiency': [round(avg_efficiency, 3)],\n",
    "        'std_efficiency': [round(std_efficiency, 3)]\n",
    "    })\n",
    "    return participant_sleep_data, sleep_stats.astype(np.float16)\n",
    "def calculate_stats(data, period_mask=None):\n",
    "    if period_mask is not None:\n",
    "        data = data[period_mask]\n",
    "        data = data.values[:,0]\n",
    "    data = data[~np.isnan(data)]\n",
    "    if len(data) == 0:\n",
    "        return pd.DataFrame({\n",
    "            'avg': [np.nan],\n",
    "            'std': [np.nan],\n",
    "            'min': [np.nan],\n",
    "            'n5': [np.nan],\n",
    "            'n25': [np.nan],\n",
    "            'median': [np.nan],\n",
    "            'n75': [np.nan],\n",
    "            'n95': [np.nan],\n",
    "            'max': [np.nan],\n",
    "            'var': [np.nan],\n",
    "            'std_score': [np.nan],\n",
    "            'var_score': [np.nan],\n",
    "            'consistency_score': [np.nan],\n",
    "            'entropy': [np.nan]\n",
    "        })\n",
    "    max_val = round(np.nanmax(data), 3)\n",
    "    min_val = round(np.nanmin(data), 3)\n",
    "    avg_val = round(np.nanmean(data), 3)\n",
    "    median = round(np.nanpercentile(data, 50),3)\n",
    "    \n",
    "    if len(data) > 1:\n",
    "        n5 = round(np.nanpercentile(data, 5), 3)\n",
    "        n25 = round(np.nanpercentile(data, 25), 3)\n",
    "        n75 = round(np.nanpercentile(data, 75), 3)\n",
    "        n95 = round(np.nanpercentile(data, 95), 3)\n",
    "        std_val = round(np.nanstd(data), 3)\n",
    "        var_val = round(np.nanvar(data), 3)\n",
    "        std_score_val = round(100 * np.nanstd(data) / np.nanmean(data), 3)\n",
    "        var_score_val = round(np.nanvar(data) / np.nanmean(data), 3)\n",
    "        consistency_score_val = round(100. / np.nanvar(data), 3) if np.nanvar(data) != 0 else np.nan\n",
    "        entropy = calculate_entropy(data)\n",
    "    else:\n",
    "        n5 = np.nan\n",
    "        n25 = np.nan\n",
    "        n75 = np.nan\n",
    "        n95 = np.nan\n",
    "        std_val = np.nan\n",
    "        var_val = np.nan\n",
    "        std_score_val = np.nan\n",
    "        var_score_val = np.nan\n",
    "        consistency_score_val = np.nan\n",
    "        entropy = np.nan\n",
    "    data_stats = pd.DataFrame({\n",
    "        'avg': [avg_val],\n",
    "        'std': [std_val],\n",
    "        'min': [min_val],\n",
    "        'n5': [n5],\n",
    "        'n25': [n25],\n",
    "        'median': [median],\n",
    "        'n75': [n75],\n",
    "        'n95': [n95],\n",
    "        'max': [max_val],\n",
    "        'var': [var_val],\n",
    "        'std_score': [std_score_val],\n",
    "        'var_score': [var_score_val],\n",
    "        'consistency_score': [consistency_score_val],\n",
    "        'entropy': [entropy]\n",
    "    })\n",
    "    return data_stats.astype(np.float16)\n",
    "# Helper function to extract values from the DataFrame\n",
    "def extract_stats(stats_df, prefix):\n",
    "    return {\n",
    "        f'{prefix}_avg': float(stats_df['avg'].values[0]),\n",
    "        f'{prefix}_std': float(stats_df['std'].values[0]),\n",
    "        f'{prefix}_min': float(stats_df['min'].values[0]),\n",
    "        f'{prefix}_n5': float(stats_df['n5'].values[0]),\n",
    "        f'{prefix}_n25': float(stats_df['n25'].values[0]),\n",
    "        f'{prefix}_median': float(stats_df['median'].values[0]),\n",
    "        f'{prefix}_n75': float(stats_df['n75'].values[0]),\n",
    "        f'{prefix}_n95': float(stats_df['n95'].values[0]),\n",
    "        f'{prefix}_max': float(stats_df['max'].values[0]),\n",
    "        f'{prefix}_var': float(stats_df['var'].values[0]),\n",
    "        f'{prefix}_std_score': float(stats_df['std_score'].values[0]),\n",
    "        f'{prefix}_var_score': float(stats_df['var_score'].values[0]),\n",
    "        f'{prefix}_consistency_score': float(stats_df['consistency_score'].values[0]),\n",
    "        f'{prefix}_entropy': float(stats_df['entropy'].values[0]),\n",
    "    }\n",
    "def extract_prediction_stats(stats_df, prefix):\n",
    "    return {\n",
    "        f'{prefix}_max': float(stats_df['max'].values[0]),\n",
    "        f'{prefix}_median': float(stats_df['median'].values[0]),\n",
    "        f'{prefix}_avg': float(stats_df['avg'].values[0]),\n",
    "        f'{prefix}_std_score': float(stats_df['std'].values[0])\n",
    "    }\n",
    "def extract_reduced_stats(stats_df, prefix):\n",
    "    return {\n",
    "        f'{prefix}_median': float(stats_df['median'].values[0]),\n",
    "        f'{prefix}_avg': float(stats_df['avg'].values[0]),\n",
    "        f'{prefix}_entropy': float(stats_df['entropy'].values[0]),\n",
    "        f'{prefix}_std_score': float(stats_df['std'].values[0]),\n",
    "        f'{prefix}_var_score': float(stats_df['var'].values[0])\n",
    "    }\n",
    "def calculate_sum(carbs_data, period_mask=None):\n",
    "    if period_mask is not None:\n",
    "        carbs_data = carbs_data[period_mask]\n",
    "    return np.sum(carbs_data['corrected_value']) if not carbs_data['corrected_value'].empty else 0\n",
    "def corrected_daily_carbs_stats(daily_carb_data):\n",
    "    overall_stats = calculate_stats(daily_carb_data['corrected_value'].values)\n",
    "    overall_stats = extract_prediction_stats(overall_stats, 'carbs')\n",
    "    \n",
    "    if len(daily_carb_data) > 0:\n",
    "        # Calculate carb on board at midnight considering only intake after 7 PM\n",
    "        start_time = pd.Timestamp(daily_carb_data['FADTC'].dt.date.iloc[0]) + pd.Timedelta(hours=19)  # 7 PM\n",
    "        midnight = pd.Timestamp(daily_carb_data['FADTC'].dt.date.iloc[0]) + pd.Timedelta(days=1)  # 12 AM next day\n",
    "        carb_on_board = 0.0\n",
    "        for _, row in daily_carb_data[daily_carb_data['FADTC'] >= start_time].iterrows():\n",
    "            carb_amount = row['corrected_value']\n",
    "            intake_time = row['FADTC']\n",
    "            absorption_start_time = intake_time + pd.Timedelta(minutes=15)\n",
    "\n",
    "            if midnight > absorption_start_time:\n",
    "                absorbed_duration = min((midnight - absorption_start_time).total_seconds() / 60, carb_amount / 0.5)\n",
    "                absorbed_carbs = absorbed_duration * 0.5\n",
    "                remaining_carbs = max(carb_amount - absorbed_carbs, 0)\n",
    "                carb_on_board += remaining_carbs\n",
    "    else:\n",
    "        carb_on_board = np.nan\n",
    "    overall_stats['carb_on_board_midnight'] = carb_on_board\n",
    "    masks = {\n",
    "        'night': (daily_carb_data['FADTC'].dt.hour >= 0) & (daily_carb_data['FADTC'].dt.hour < 6),\n",
    "        'morning': (daily_carb_data['FADTC'].dt.hour >= 6) & (daily_carb_data['FADTC'].dt.hour < 12),\n",
    "        'afternoon': (daily_carb_data['FADTC'].dt.hour >= 12) & (daily_carb_data['FADTC'].dt.hour < 18),\n",
    "        'evening': (daily_carb_data['FADTC'].dt.hour >= 18) & (daily_carb_data['FADTC'].dt.hour < 24),\n",
    "        'late_evening': (daily_carb_data['FADTC'].dt.hour >= 21) & (daily_carb_data['FADTC'].dt.hour < 24)\n",
    "    } \n",
    "    time_range_stats = {}\n",
    "    if len(daily_carb_data) > 0:\n",
    "        time_range_stats['carbs_day'] = calculate_sum(daily_carb_data)\n",
    "    else:\n",
    "        time_range_stats['carbs_day'] = np.nan\n",
    "        \n",
    "    for period, mask in masks.items():\n",
    "        if len(daily_carb_data) > 0:\n",
    "            time_range_stats[f'carbs_{period}'] = calculate_sum(daily_carb_data, mask)\n",
    "        else:\n",
    "            time_range_stats[f'carbs_{period}'] = np.nan\n",
    "\n",
    "    carbs_stats = pd.DataFrame({**overall_stats, **time_range_stats}, index=[0])\n",
    "    return carbs_stats.astype(np.float16)\n",
    "def total_stats(data, times, label_data='bolus'):\n",
    "    daily_data = pd.DataFrame({'datetime': times, 'value': data})\n",
    "    daily_data.set_index('datetime', inplace=True)\n",
    "    \n",
    "    general_stats = calculate_stats(daily_data.values[:,0])\n",
    "    general_stats = extract_stats(general_stats, label_data)\n",
    "    masks = {\n",
    "        'night': (daily_data.index.hour >= 0) & (daily_data.index.hour < 6),\n",
    "        'morning': (daily_data.index.hour >= 6) & (daily_data.index.hour < 12),\n",
    "        'afternoon': (daily_data.index.hour >= 12) & (daily_data.index.hour < 18),\n",
    "        'evening': (daily_data.index.hour >= 18) & (daily_data.index.hour < 24),\n",
    "        'late_evening': (daily_data.index.hour >= 22) & (daily_data.index.hour < 24)\n",
    "    }\n",
    "    time_range_stats = {}\n",
    "    for period, mask in masks.items():\n",
    "        period_stats = calculate_stats(daily_data, mask)\n",
    "        period_stats = extract_stats(period_stats, f'{label_data}_{period}')\n",
    "        time_range_stats.update(period_stats)\n",
    "    \n",
    "    total_stats = pd.DataFrame({**general_stats, **time_range_stats}, index=[0])\n",
    "    return total_stats.astype(np.float16)\n",
    "def daily_stats_hr(data, times, label_data='hr'):\n",
    "    daily_data = pd.DataFrame({'datetime': times, 'value': data})\n",
    "    daily_data.set_index('datetime', inplace=True)\n",
    "    \n",
    "    general_stats = calculate_stats(daily_data.values[:,0])\n",
    "    general_stats = extract_stats(general_stats, label_data)\n",
    "    mean_crossings = calculate_crossings(daily_data.values[:,0])\n",
    "    general_stats[f'{label_data}_mean_crossings'] = mean_crossings\n",
    "    masks = {\n",
    "        'night': (daily_data.index.hour >= 0) & (daily_data.index.hour < 6),\n",
    "        'morning': (daily_data.index.hour >= 6) & (daily_data.index.hour < 12),\n",
    "        'afternoon': (daily_data.index.hour >= 12) & (daily_data.index.hour < 18),\n",
    "        'evening': (daily_data.index.hour >= 18) & (daily_data.index.hour < 24),\n",
    "        'late_evening': (daily_data.index.hour >= 21) & (daily_data.index.hour < 24),\n",
    "        'before_sleep': (daily_data.index.hour >= 23) & (daily_data.index.hour < 24)\n",
    "    }\n",
    "    time_range_stats = {}\n",
    "    for period, mask in masks.items():\n",
    "        period_stats = calculate_stats(daily_data, mask)\n",
    "        period_stats = extract_stats(period_stats, f'{label_data}_{period}')\n",
    "        time_range_stats.update(period_stats)\n",
    "    \n",
    "    daily_stats = pd.DataFrame({**general_stats, **time_range_stats}, index=[0])\n",
    "    return daily_stats.astype(np.float16)\n",
    "    \n",
    "def daily_stats(data, times, label_data='basal'):\n",
    "    daily_data = pd.DataFrame({'datetime': times, 'value': data})\n",
    "    daily_data.set_index('datetime', inplace=True)\n",
    "    \n",
    "    general_stats = calculate_stats(daily_data.values[:,0])\n",
    "    general_stats = extract_stats(general_stats, label_data)\n",
    "    mean_crossings = calculate_crossings(daily_data.values[:,0])\n",
    "    general_stats[f'{label_data}_mean_crossings'] = mean_crossings\n",
    "    masks = {\n",
    "        'night': (daily_data.index.hour >= 0) & (daily_data.index.hour < 6),\n",
    "        'morning': (daily_data.index.hour >= 6) & (daily_data.index.hour < 12),\n",
    "        'afternoon': (daily_data.index.hour >= 12) & (daily_data.index.hour < 18),\n",
    "        'evening': (daily_data.index.hour >= 18) & (daily_data.index.hour < 24),\n",
    "        'late_evening': (daily_data.index.hour >= 22) & (daily_data.index.hour < 24)\n",
    "    }\n",
    "    time_range_stats = {}\n",
    "    for period, mask in masks.items():\n",
    "        period_stats = calculate_stats(daily_data, mask)\n",
    "        period_stats = extract_stats(period_stats, f'{label_data}_{period}')\n",
    "        time_range_stats.update(period_stats)\n",
    "    \n",
    "    daily_stats = pd.DataFrame({**general_stats, **time_range_stats}, index=[0])\n",
    "    return daily_stats.astype(np.float16)\n",
    "def daily_stats_basal(data, times, label_data='basal'):\n",
    "    daily_data = pd.DataFrame({'datetime': times, 'value': data})\n",
    "    daily_data.set_index('datetime', inplace=True)\n",
    "    \n",
    "    general_stats = calculate_stats(daily_data.values[:,0])\n",
    "    general_stats = extract_stats(general_stats, label_data)\n",
    "    mean_crossings = calculate_crossings(daily_data.values[:,0])\n",
    "    general_stats[f'{label_data}_mean_crossings'] = mean_crossings\n",
    "    masks = {\n",
    "        'night': (daily_data.index.hour >= 0) & (daily_data.index.hour < 6),\n",
    "        'morning': (daily_data.index.hour >= 6) & (daily_data.index.hour < 12),\n",
    "        'afternoon': (daily_data.index.hour >= 12) & (daily_data.index.hour < 18),\n",
    "        'evening': (daily_data.index.hour >= 18) & (daily_data.index.hour < 24),\n",
    "        'late_evening': (daily_data.index.hour >= 22) & (daily_data.index.hour < 24)\n",
    "    }\n",
    "    time_range_stats = {}\n",
    "    for period, mask in masks.items():\n",
    "        period_stats = calculate_stats(daily_data, mask)\n",
    "        period_stats = extract_stats(period_stats, f'{label_data}_{period}')\n",
    "        time_range_stats.update(period_stats)\n",
    "    \n",
    "    daily_stats = pd.DataFrame({**general_stats, **time_range_stats}, index=[0])\n",
    "    return daily_stats.astype(np.float16)\n",
    "def daily_stats_bolus(data, times, label_data='bolus'):\n",
    "    daily_data = pd.DataFrame({'datetime': times, 'value': data})\n",
    "    daily_data.set_index('datetime', inplace=True)\n",
    "    \n",
    "    general_stats = calculate_stats(daily_data.values[:,0])\n",
    "    general_stats = extract_stats(general_stats, label_data)\n",
    "    mean_crossings = calculate_crossings(daily_data.values[:,0])\n",
    "    general_stats[f'{label_data}_mean_crossings'] = mean_crossings\n",
    "    masks = {\n",
    "        'night': (daily_data.index.hour >= 0) & (daily_data.index.hour < 6),\n",
    "        'afternoon': (daily_data.index.hour >= 12) & (daily_data.index.hour < 18),\n",
    "        'evening': (daily_data.index.hour >= 18) & (daily_data.index.hour < 24)\n",
    "    }\n",
    "    time_range_stats = {}\n",
    "    for period, mask in masks.items():\n",
    "        period_stats = calculate_stats(daily_data, mask)\n",
    "        period_stats = extract_stats(period_stats, f'{label_data}_{period}')\n",
    "        time_range_stats.update(period_stats)\n",
    "    \n",
    "    daily_stats = pd.DataFrame({**general_stats, **time_range_stats}, index=[0])\n",
    "    return daily_stats.astype(np.float16)\n",
    "def calculate_entropy(list_values):\n",
    "    counter_values = Counter(list_values).most_common()\n",
    "    probabilities = [elem[1]/len(list_values) for elem in counter_values]\n",
    "    entropy=stats.entropy(probabilities)\n",
    "    return entropy.round(3)\n",
    "\n",
    "def calculate_crossings(list_values):\n",
    "    mean_crossing_indices = np.nonzero(np.diff(list_values > np.nanmean(list_values)))[0]\n",
    "    no_mean_crossings = len(mean_crossing_indices)\n",
    "    return no_mean_crossings\n",
    "\n",
    "def resample_avg(data, times, interval='5T'):\n",
    "    df_data = pd.DataFrame({'datetime': times, 'value': data})\n",
    "    df_data.set_index('datetime', inplace=True)\n",
    "    data_resampled = df_data.copy()\n",
    "    data_resampled.index = data_resampled.index.floor(interval)\n",
    "    data_resampled = data_resampled.groupby(data_resampled.index).mean()\n",
    "    data_resampled = data_resampled.resample(interval).asfreq()\n",
    "    return data_resampled\n",
    "\n",
    "def smooth_signal(df, window_size=17, treshold=10, order=None):\n",
    "    data = df['value']\n",
    "\n",
    "    if order is not None:\n",
    "        # Temporarily fill NaNs with interpolation to avoid issues with savgol_filter\n",
    "        data_filled = data.interpolate(method='linear')\n",
    "        # Apply savgol_filter only to non-NaN values\n",
    "        smoothed_filled = savgol_filter(data_filled.fillna(0), window_length=window_size-8, polyorder=order)\n",
    "        # Reintroduce NaNs where they were originally\n",
    "        smoothed_values = pd.Series(smoothed_filled, index=data.index)\n",
    "        smoothed_values[np.isnan(data)] = np.nan\n",
    "    else:\n",
    "        smoothed_values = data\n",
    "        \n",
    "    half_window = window_size // 2\n",
    "    # Generate a Gaussian (normal) distribution for weights\n",
    "    # The mean is at the center of the window, and the standard deviation determines the spread\n",
    "    weights = norm.pdf(np.arange(-half_window, half_window + 1), 0, 2)\n",
    "    # Normalize weights so that they sum to 1\n",
    "    weights /= weights.sum()\n",
    "    data_smooth = pd.Series(smoothed_values, index=df.index)\n",
    "    rolling_avg = data_smooth.rolling(window=window_size, center=True).apply(lambda x: np.dot(x, weights), raw=True)\n",
    "\n",
    "    #rolling_avg = data.rolling(window=window_size, center=True).mean()\n",
    "    fluctuations = data - rolling_avg\n",
    "    smoothed_values = rolling_avg + fluctuations.where(fluctuations.abs() > treshold, 0)\n",
    "    \n",
    "    weights = norm.pdf(np.arange(-3, 3 + 1), 0, 2)\n",
    "    weights /= weights.sum()\n",
    "    rolling_avg = smoothed_values.rolling(window=7, center=True).apply(lambda x: np.dot(x, weights), raw=True)\n",
    "    fluctuations = smoothed_values - rolling_avg\n",
    "    smoothed_values = rolling_avg + fluctuations.where(fluctuations.abs() > treshold, 0)\n",
    "\n",
    "    data_filled = smoothed_values.interpolate(method='linear')\n",
    "    # Apply savgol_filter only to non-NaN values\n",
    "    smoothed_filled = savgol_filter(data_filled.fillna(0), window_length=window_size-10, polyorder=order+1)\n",
    "    # Reintroduce NaNs where they were originally\n",
    "    smoothed_values = pd.Series(smoothed_filled, index=data.index)\n",
    "    smoothed_values[np.isnan(data)] = np.nan\n",
    "        \n",
    "    smoothed_df = pd.DataFrame({'datetime': df.index, 'value': smoothed_values})\n",
    "    smoothed_df.set_index('datetime', inplace=True)\n",
    "    return smoothed_df\n",
    "    \n",
    "def compute_cwt(data, scales, dt=1./12, normalize = True, waveletname = 'mexh', label_data = 'CGM ' ):\n",
    "    times = data.index\n",
    "    N = len(data)\n",
    "    if waveletname in ['cmor', 'shan']:\n",
    "        waveletname += '1.5-1'\n",
    "        \n",
    "    [coefficients_mex, frequencies_mex] = pywt.cwt(data.values.squeeze(), scales, waveletname, dt)\n",
    "    [coefficients_morl, frequencies_morl] = pywt.cwt(data.values.squeeze(), 4*scales, 'morl', dt)\n",
    "    # period_mex = 1. / frequencies_mex\n",
    "    # period_morl = 1. / frequencies_morl\n",
    "    # fft = np.fft.fft(data)\n",
    "    # fftfreqs = np.fft.fftfreq(N, dt)\n",
    "    scaleMatrix = np.ones([1, N]) * scales[:, None]\n",
    "\n",
    "    power_mex =  (abs(coefficients_mex)) ** 2 / scaleMatrix\n",
    "    power_morl =  (abs(coefficients_morl)) ** 2 / scaleMatrix\n",
    "    # Other metrics to analyze later\n",
    "    # signed_log_power_mex = np.sign(coefficients_mex) * np.log2(power_mex)\n",
    "    # signed_log_power_morl = np.sign(coefficients_morl) * np.log2(power_morl)\n",
    "    # Replace -inf with the minimum finite value\n",
    "    # signed_log_power_mex = np.where(np.isfinite(signed_log_power_mex), signed_log_power_mex, np.min(signed_log_power_mex[np.isfinite(signed_log_power_mex)])/10)\n",
    "    # signed_log_power_morl = np.where(np.isfinite(signed_log_power_morl), signed_log_power_morl, np.min(signed_log_power_morl[np.isfinite(signed_log_power_morl)])/10)\n",
    "    return (\n",
    "        coefficients_mex,\n",
    "        power_mex,\n",
    "        # signed_log_power_mex.astype(np.float32),\n",
    "        coefficients_morl,\n",
    "        power_morl\n",
    "        # signed_log_power_morl.astype(np.float32)\n",
    "        # fft.astype(np.float32),\n",
    "        # fftfreqs.astype(np.float32)\n",
    "    )\n",
    "def remove_nan_rows(X_daily, X_individual, y_label):\n",
    "    # Find rows with NaNs in either feature vector\n",
    "    nan_mask_daily = np.any(np.isnan(X_daily), axis=(1, 2))\n",
    "    nan_mask_individual = np.any(np.isnan(X_individual), axis=(1, 2))\n",
    "    \n",
    "    nan_mask_combined = nan_mask_daily | nan_mask_individual    \n",
    "    # Keep only rows without NaNs\n",
    "    X_daily_clean = X_daily[~nan_mask_combined]\n",
    "    X_individual_clean = X_individual[~nan_mask_combined]\n",
    "    y_label_clean = y_label[~nan_mask_combined]\n",
    "    return X_daily_clean, X_individual_clean, y_label_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b642f193-331e-4011-bf1e-d960cbc038ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "participant id:  6\n",
      "\n",
      "participant id:  7\n",
      "\n",
      "participant id:  8\n",
      "\n",
      "participant id:  9\n",
      "\n",
      "participant id:  11\n",
      "\n",
      "participant id:  13\n",
      "\n",
      "participant id:  16\n",
      "\n",
      "participant id:  18\n",
      "\n",
      "participant id:  19\n",
      "\n",
      "participant id:  23\n",
      "\n",
      "participant id:  24\n",
      "\n",
      "participant id:  25\n",
      "\n",
      "participant id:  26\n",
      "\n",
      "participant id:  29\n",
      "\n",
      "participant id:  33\n",
      "\n",
      "participant id:  34\n",
      "\n",
      "participant id:  35\n",
      "\n",
      "participant id:  36\n",
      "\n",
      "participant id:  37\n",
      "\n",
      "participant id:  40\n",
      "\n",
      "participant id:  44\n",
      "\n",
      "participant id:  45\n",
      "\n",
      "participant id:  46\n",
      "\n",
      "participant id:  47\n",
      "\n",
      "participant id:  55\n",
      "\n",
      "participant id:  57\n",
      "\n",
      "participant id:  58\n",
      "\n",
      "participant id:  65\n",
      "\n",
      "participant id:  66\n",
      "\n",
      "participant id:  67\n",
      "\n",
      "participant id:  68\n",
      "\n",
      "participant id:  69\n",
      "\n",
      "participant id:  70\n",
      "\n",
      "participant id:  71\n",
      "\n",
      "participant id:  73\n",
      "\n",
      "participant id:  75\n",
      "\n",
      "participant id:  76\n",
      "\n",
      "participant id:  77\n",
      "\n",
      "participant id:  83\n",
      "\n",
      "participant id:  84\n",
      "\n",
      "participant id:  86\n",
      "\n",
      "participant id:  88\n",
      "\n",
      "participant id:  91\n",
      "\n",
      "participant id:  92\n",
      "\n",
      "participant id:  99\n",
      "\n",
      "participant id:  100\n",
      "\n",
      "participant id:  101\n",
      "\n",
      "participant id:  102\n",
      "\n",
      "participant id:  103\n",
      "\n",
      "participant id:  105\n",
      "\n",
      "participant id:  107\n",
      "\n",
      "participant id:  111\n",
      "\n",
      "participant id:  113\n",
      "\n",
      "participant id:  114\n",
      "\n",
      "participant id:  116\n",
      "\n",
      "participant id:  118\n",
      "\n",
      "participant id:  119\n",
      "\n",
      "participant id:  122\n",
      "\n",
      "participant id:  125\n",
      "\n",
      "participant id:  128\n",
      "\n",
      "participant id:  131\n",
      "\n",
      "participant id:  133\n",
      "\n",
      "participant id:  136\n",
      "\n",
      "participant id:  138\n",
      "\n",
      "participant id:  140\n",
      "\n",
      "participant id:  141\n",
      "\n",
      "participant id:  147\n",
      "\n",
      "participant id:  150\n",
      "\n",
      "participant id:  151\n",
      "\n",
      "participant id:  153\n",
      "\n",
      "participant id:  154\n",
      "\n",
      "participant id:  158\n",
      "\n",
      "participant id:  165\n",
      "\n",
      "participant id:  167\n",
      "\n",
      "participant id:  171\n",
      "\n",
      "participant id:  174\n",
      "\n",
      "participant id:  175\n",
      "\n",
      "participant id:  177\n",
      "\n",
      "participant id:  181\n",
      "\n",
      "participant id:  193\n",
      "\n",
      "participant id:  194\n",
      "\n",
      "participant id:  195\n",
      "\n",
      "participant id:  197\n",
      "\n",
      "participant id:  199\n",
      "\n",
      "participant id:  202\n",
      "\n",
      "participant id:  203\n",
      "\n",
      "participant id:  205\n",
      "\n",
      "participant id:  206\n",
      "\n",
      "participant id:  207\n",
      "\n",
      "participant id:  208\n",
      "\n",
      "participant id:  210\n",
      "\n",
      "participant id:  211\n",
      "\n",
      "participant id:  213\n",
      "\n",
      "participant id:  214\n",
      "\n",
      "participant id:  215\n",
      "\n",
      "participant id:  220\n",
      "\n",
      "participant id:  221\n",
      "\n",
      "participant id:  225\n",
      "\n",
      "participant id:  228\n",
      "\n",
      "participant id:  230\n",
      "\n",
      "participant id:  231\n",
      "\n",
      "participant id:  234\n",
      "\n",
      "participant id:  237\n",
      "\n",
      "participant id:  239\n",
      "\n",
      "participant id:  240\n",
      "\n",
      "participant id:  241\n",
      "\n",
      "participant id:  242\n",
      "\n",
      "participant id:  244\n",
      "\n",
      "participant id:  245\n",
      "\n",
      "participant id:  246\n",
      "\n",
      "participant id:  249\n",
      "\n",
      "participant id:  251\n",
      "\n",
      "participant id:  254\n",
      "\n",
      "participant id:  256\n",
      "\n",
      "participant id:  257\n",
      "\n",
      "participant id:  258\n",
      "\n",
      "participant id:  264\n",
      "\n",
      "participant id:  265\n",
      "\n",
      "participant id:  270\n",
      "\n",
      "participant id:  274\n",
      "\n",
      "participant id:  276\n",
      "\n",
      "participant id:  278\n",
      "\n",
      "participant id:  279\n",
      "\n",
      "participant id:  281\n",
      "\n",
      "participant id:  284\n",
      "\n",
      "participant id:  286\n",
      "\n",
      "participant id:  288\n",
      "\n",
      "participant id:  290\n",
      "\n",
      "participant id:  292\n",
      "\n",
      "participant id:  293\n",
      "\n",
      "participant id:  295\n",
      "\n",
      "participant id:  298\n",
      "\n",
      "participant id:  301\n",
      "\n",
      "participant id:  302\n",
      "\n",
      "participant id:  303\n",
      "\n",
      "participant id:  304\n",
      "\n",
      "participant id:  307\n",
      "\n",
      "participant id:  312\n",
      "\n",
      "participant id:  317\n",
      "\n",
      "participant id:  318\n",
      "\n",
      "participant id:  320\n",
      "\n",
      "participant id:  324\n",
      "\n",
      "participant id:  328\n",
      "\n",
      "participant id:  329\n",
      "\n",
      "participant id:  332\n",
      "\n",
      "participant id:  339\n",
      "\n",
      "participant id:  340\n",
      "\n",
      "participant id:  341\n",
      "\n",
      "participant id:  350\n",
      "\n",
      "participant id:  354\n",
      "\n",
      "participant id:  357\n",
      "\n",
      "participant id:  360\n",
      "\n",
      "participant id:  363\n",
      "\n",
      "participant id:  364\n",
      "\n",
      "participant id:  365\n",
      "\n",
      "participant id:  367\n",
      "\n",
      "participant id:  368\n",
      "\n",
      "participant id:  370\n",
      "\n",
      "participant id:  372\n",
      "\n",
      "participant id:  376\n",
      "\n",
      "participant id:  378\n",
      "\n",
      "participant id:  381\n",
      "\n",
      "participant id:  382\n",
      "\n",
      "participant id:  383\n",
      "\n",
      "participant id:  384\n",
      "\n",
      "participant id:  387\n",
      "\n",
      "participant id:  388\n",
      "\n",
      "participant id:  389\n",
      "\n",
      "participant id:  390\n",
      "\n",
      "participant id:  392\n",
      "\n",
      "participant id:  393\n",
      "\n",
      "participant id:  394\n",
      "\n",
      "participant id:  399\n",
      "\n",
      "participant id:  401\n",
      "\n",
      "participant id:  404\n",
      "\n",
      "participant id:  405\n",
      "\n",
      "participant id:  408\n",
      "\n",
      "participant id:  409\n",
      "\n",
      "participant id:  411\n",
      "\n",
      "participant id:  414\n",
      "\n",
      "participant id:  416\n",
      "\n",
      "participant id:  423\n",
      "\n",
      "participant id:  429\n",
      "\n",
      "participant id:  431\n",
      "\n",
      "participant id:  435\n",
      "\n",
      "participant id:  436\n",
      "\n",
      "participant id:  439\n",
      "\n",
      "participant id:  441\n",
      "\n",
      "participant id:  442\n",
      "\n",
      "participant id:  443\n",
      "\n",
      "participant id:  444\n",
      "\n",
      "participant id:  446\n",
      "\n",
      "participant id:  455\n",
      "\n",
      "participant id:  456\n",
      "\n",
      "participant id:  458\n",
      "\n",
      "participant id:  460\n",
      "\n",
      "participant id:  463\n",
      "\n",
      "participant id:  464\n",
      "\n",
      "participant id:  466\n",
      "\n",
      "participant id:  467\n",
      "\n",
      "participant id:  469\n",
      "\n",
      "participant id:  471\n",
      "\n",
      "participant id:  473\n",
      "\n",
      "participant id:  475\n",
      "\n",
      "participant id:  478\n",
      "\n",
      "participant id:  479\n",
      "\n",
      "participant id:  480\n",
      "\n",
      "participant id:  482\n",
      "\n",
      "participant id:  483\n",
      "\n",
      "participant id:  484\n",
      "\n",
      "participant id:  492\n",
      "\n",
      "participant id:  494\n",
      "\n",
      "participant id:  496\n",
      "\n",
      "participant id:  497\n",
      "\n",
      "participant id:  499\n",
      "\n",
      "participant id:  501\n",
      "\n",
      "participant id:  502\n",
      "\n",
      "participant id:  510\n",
      "\n",
      "participant id:  515\n",
      "\n",
      "participant id:  516\n",
      "\n",
      "participant id:  517\n",
      "\n",
      "participant id:  518\n",
      "\n",
      "participant id:  520\n",
      "\n",
      "participant id:  521\n",
      "\n",
      "participant id:  522\n",
      "\n",
      "participant id:  525\n",
      "\n",
      "participant id:  526\n",
      "\n",
      "participant id:  530\n",
      "\n",
      "participant id:  531\n",
      "\n",
      "participant id:  533\n",
      "\n",
      "participant id:  534\n",
      "\n",
      "participant id:  535\n",
      "\n",
      "participant id:  540\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "data_dict = defaultdict(lambda: defaultdict(dict))\n",
    "for participant_id in shared_participants: \n",
    "    participant_insulin_data = T1DEXI_basal[T1DEXI_basal['USUBJID'] == participant_id].reset_index()     \n",
    "    participant_cgm_data = T1DEXI_CGM_Dataset[T1DEXI_CGM_Dataset['USUBJID'] == participant_id].reset_index()\n",
    "    participant_hr_data = T1DEXI_VS_Dataset[T1DEXI_VS_Dataset['USUBJID'] == participant_id].reset_index()\n",
    "    # Less than two days\n",
    "    if len(participant_cgm_data)<550:\n",
    "        continue\n",
    "        \n",
    "    participant_carbs_self = FAMLPM_Dataset[FAMLPM_Dataset['USUBJID']==participant_id][FAMLPM_Dataset['FAMETHOD']=='SELF-REPORT'].reset_index()\n",
    "    participant_carbs_RFPM = FAMLPM_Dataset[FAMLPM_Dataset['USUBJID']==participant_id][FAMLPM_Dataset['FAMETHOD']=='RFPM'].reset_index()\n",
    "\n",
    "    merged_carbs = pd.merge(participant_carbs_self[['FASTRESN','FADTC']], participant_carbs_RFPM[['FASTRESN','FADTC']], on='FADTC', how='left', suffixes=('_self', '_RFPM'))\n",
    "    merged_carbs['corrected_value'] = (merged_carbs['FASTRESN_self'] + merged_carbs['FASTRESN_RFPM']) / 2\n",
    "    participant_carbs_self['corrected_value'] = merged_carbs['corrected_value'].fillna(participant_carbs_self['FASTRESN'])        \n",
    "    participant_carbs_self = participant_carbs_self.dropna(subset=['corrected_value']).reset_index()\n",
    "    if participant_hr_data.empty:\n",
    "        continue\n",
    "    a1c_data = participant_cgm_data[participant_cgm_data['LBTEST'] == \"Hemoglobin A1c\"]\n",
    "    if a1c_data.empty:\n",
    "        continue\n",
    "    else:\n",
    "        a1c = a1c_data['LBSTRESN'].iloc[0]\n",
    "    height_data = participant_hr_data[participant_hr_data['VSTESTCD'] == \"HEIGHT\"]\n",
    "    height = height_data['VSSTRESN'].iloc[0]\n",
    "    weight_data = participant_hr_data[participant_hr_data['VSTESTCD'] == \"WEIGHT\"]\n",
    "    weight = weight_data['VSSTRESN'].iloc[0]\n",
    "    print(\"\\nparticipant id: \", participant_id)\n",
    "\n",
    "    # --- Separate data types ---\n",
    "    participant_basal_data = participant_insulin_data[participant_insulin_data['FACAT'] == \"BASAL\"][participant_insulin_data['FATESTCD'] == \"INSBASAL\"].dropna(subset=['FASTRESN']).reset_index() #BASFLRT  #INSBASAL\n",
    "    participant_cgm_data = participant_cgm_data[participant_cgm_data['LBTEST'] == \"Glucose\"].dropna(subset=['LBSTRESN']).reset_index()\n",
    "    participant_hr_data = participant_hr_data[participant_hr_data['VSTESTCD']=='HRM'].dropna(subset=['VSSTRESN']).reset_index()\n",
    "\n",
    "    # --- Align data to 5-minute intervals ---    \n",
    "    aligned_cgm = resample_avg(participant_cgm_data['LBSTRESN'], participant_cgm_data['LBDTC'], interval='5T')\n",
    "    # --- Interpolate max 3 hour gaps ---    \n",
    "    method = 'linear'  \n",
    "    interpolated_cgm = aligned_cgm.interpolate(method=method, order=2,limit_direction='both')\n",
    "    smoothed_cgm = smooth_signal(interpolated_cgm, window_size=17, treshold=10, order=2)\n",
    "    interpolated_cgm = smoothed_cgm.interpolate(method=method, order=2,limit_direction='both')\n",
    "\n",
    "    # Total Participant Stats\n",
    "    total_basal_stats = total_stats(participant_basal_data['FASTRESN'], participant_basal_data['FADTC'], 'basal')\n",
    "    total_hr_stats = total_stats(participant_hr_data['VSSTRESN'], participant_hr_data['VSDTC'], 'hr')\n",
    "    total_carbs_stats = total_stats(participant_carbs_self['corrected_value'], participant_carbs_self['FADTC'], 'carb')\n",
    "\n",
    "# --- Compute & Plot overall CWT ---    \n",
    "    dt_cgm = 1./12 \n",
    "    s0 = 1./2  #starting period is 15 minutes\n",
    "    dj = 1/8  # Resolusion\n",
    "    J = int(12 / dj)\n",
    "    scales = s0 * np.arange(1,576,2)\n",
    "    coefficients_mex,power_mex,coefficients_morl,power_morl = compute_cwt(interpolated_cgm, scales, dt=dt_cgm , normalize = False, waveletname = 'mexh', label_data = \"CGM ID#\" +str(participant_id) )\n",
    " \n",
    "    # --- Find overlapping dates ---\n",
    "    unique_dates = sorted(set(interpolated_cgm.index.date))\n",
    "    unique_dates = unique_dates[:-1] # Skip the last day\n",
    "    for current_date in unique_dates:\n",
    "        # --- Get current day data (for feature extraction) ---\n",
    "        current_day_mask_cgm = interpolated_cgm.index.date == current_date\n",
    "        current_day_indices_cgm = np.where(current_day_mask_cgm)[0]\n",
    "        \n",
    "        current_day_basal = participant_basal_data[participant_basal_data['FADTC'].dt.date == current_date]\n",
    "        current_day_mask_cgm = current_day_mask_cgm.astype(bool)\n",
    "        current_day_cgm = interpolated_cgm[current_day_mask_cgm]\n",
    "        current_day_cgm_original = aligned_cgm[aligned_cgm.index.date == current_date]\n",
    "        current_day_hr = participant_hr_data[participant_hr_data['VSDTC'].dt.date == current_date]\n",
    "        current_day_carbs = participant_carbs_self[participant_carbs_self['FADTC'].dt.date == current_date]\n",
    "        if len(current_day_cgm_original)<288 or current_day_cgm_original.isna().sum()[0]>60 or current_day_cgm_original[-48:].isna().sum()[0]>5:\n",
    "            continue\n",
    "\n",
    "        next_date = current_date + pd.Timedelta(days=1) # Get one hour before \n",
    "        next_day_cgm_original = aligned_cgm[aligned_cgm.index.date == next_date]\n",
    "        next_night = current_day_cgm.index[-1] + pd.Timedelta(hours=12) \n",
    "        next_half_day = next_day_cgm_original[next_day_cgm_original.index <= next_night] #More than 2hour missing in the next 12 hours\n",
    "        if next_half_day.isna().sum()[0]>36:\n",
    "            continue\n",
    "\n",
    "        current_day_carbs_stats = corrected_daily_carbs_stats(current_day_carbs)   \n",
    "        current_day_cgm_stats_original = get_daily_cgm_stats(current_day_cgm_original)  \n",
    "        current_day_basal_stats = daily_stats_basal(current_day_basal['FASTRESN'], current_day_basal['FADTC'], 'basal_daily')\n",
    "        current_day_hr_stats = daily_stats_hr(current_day_hr['VSSTRESN'], current_day_hr['VSDTC'], 'hr_daily')\n",
    "        next_day_cgm_stats_original = get_cgm_prediction_stats(next_day_cgm_original)\n",
    "        next_day_cgm_labels_original = daily_cgm_labels(next_day_cgm_original)\n",
    "        \n",
    "        matrix_shape0 = len(scales)\n",
    "        matrix_shape1 = len(current_day_cgm)\n",
    "        # daily_power_mex = power_mex[:, current_day_indices_cgm]\n",
    "        # daily_coeffs_mex = coefficients_mex[:, current_day_indices_cgm]\n",
    "        # daily_power_morl = power_morl[:, current_day_indices_cgm]\n",
    "        # daily_coeffs_morl = coefficients_morl[:, current_day_indices_cgm]\n",
    "        \n",
    "        full_matrix1 = np.ndarray(shape=(matrix_shape0, matrix_shape1,3))\n",
    "        full_matrix1[:, :, 0] = coefficients_mex[:, current_day_indices_cgm]\n",
    "        full_matrix1[:, :, 1] = power_mex[:, current_day_indices_cgm]\n",
    "        full_matrix1[:, :, 2] = power_morl[:, current_day_indices_cgm]\n",
    "\n",
    "        \n",
    "        # Store data in the dictionary\n",
    "        data_dict[participant_id][current_date] = {\n",
    "            'len_original_cgm': len(current_day_cgm_original),\n",
    "            'full_matrix_cgm_power': full_matrix1,\n",
    "            \n",
    "            'next_day_cgm_stats_original':next_day_cgm_stats_original,\n",
    "            'next_day_cgm_labels_original':next_day_cgm_labels_original,\n",
    "            'CGM_stats_daily_original': current_day_cgm_stats_original,\n",
    "            \n",
    "            'Basal_stats_daily': current_day_basal_stats,\n",
    "            'Carbs_stats_daily': current_day_carbs_stats,\n",
    "            'HR_stats_daily': current_day_hr_stats,\n",
    "            \n",
    "            'A1c': a1c,\n",
    "            'Weight': weight,\n",
    "            'Height': height,\n",
    "            'Basal_stats_participant': total_basal_stats,\n",
    "            'Carb_stats_participant': total_carbs_stats,\n",
    "            'HR_stats_participant': total_hr_stats\n",
    "        }\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "404788c4-6f23-4b72-9e85-686253827e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are infinite values in the data.\n",
      "There are infinite values in the data.\n",
      "NaNs in the Data ...\n",
      "X_matrices_power: 0  NaNs with shape: (1682, 288, 288, 3)\n",
      "X_features_daily_cgm: 0  NaNs with shape: (1682, 132)\n",
      "X_features_individual_cgm: 0  NaNs with shape: (1682, 154)\n",
      "X_features_individual_scaled: 159006  NaNs with shape: (1682, 171)\n",
      "X_features_daily_scaled: 216935  NaNs with shape: (1682, 327)\n",
      "y_label hypo_early_night: 0  NaNs with shape: (1682, 1)\n",
      "y_label hypo_night: 0  NaNs with shape: (1682, 1)\n",
      "y_label hypo_long_night: 0  NaNs with shape: (1682, 1)\n",
      "y_label hypo_night_morning: 0  NaNs with shape: (1682, 1)\n",
      "y_label hyper_day: 0  NaNs with shape: (1682, 1)\n",
      "y_label hypo_late_night: 0  NaNs with shape: (1682, 1)\n",
      "y_label hyper_night: 0  NaNs with shape: (1682, 1)\n",
      "y_label hyper_early_night: 0  NaNs with shape: (1682, 1)\n",
      "y_label hypo_morning: 0  NaNs with shape: (1682, 1)\n",
      "y_label hypo_day: 0  NaNs with shape: (1682, 1)\n"
     ]
    }
   ],
   "source": [
    "with open('/home/ma98/data_generated/Pediatric/FullFeatures/full_pediatric_cgm_mex_morl_smooth.dill', 'rb') as f:\n",
    "    filtered_data_dict = dill.load(f)\n",
    "\n",
    "# Prepare the groups based on participant IDs\n",
    "participant_ids = []\n",
    "X_features_individual_cgm, X_features_daily_cgm = [], []\n",
    "X_features_individual, X_features_daily = [], []\n",
    "X_matrices_power = [] #, X_matrices_coeffs\n",
    "y_labels = { \n",
    "    \"hypo_early_night\": [], \"hypo_night\": [], \"hypo_long_night\": [],\n",
    "    \"hypo_night_morning\": [], \"hyper_day\": [], \"hypo_late_night\": [],\n",
    "    \"hyper_night\": [], \"hyper_early_night\": [], \"hypo_morning\": [],\n",
    "    \"hypo_day\": []\n",
    "}\n",
    "\n",
    "for participant_id, days_data in filtered_data_dict.items():\n",
    "    participant_ids.extend([participant_id] * len(days_data))\n",
    "    for day_data in days_data:\n",
    "        X_matrices_power.append(day_data['full_matrix_cgm_power'])\n",
    "        # X_matrices_coeffs.append(day_data['full_matrix_cgm_coeffs'])\n",
    "        X_features_daily_cgm.append(day_data['CGM_stats_daily_original'])\n",
    "        X_features_individual_cgm.append(np.concatenate([\n",
    "            [[day_data['A1c']]],\n",
    "            day_data['CGM_stats_participant']], axis=1))\n",
    "        X_features_daily.append(np.concatenate([\n",
    "            day_data['CGM_stats_daily_original'],\n",
    "            day_data['Basal_stats_daily'],\n",
    "            day_data['Carbs_stats_daily'],\n",
    "            day_data['HR_stats_daily']], axis=1))\n",
    "        X_features_individual.append(np.concatenate([\n",
    "            [[day_data['A1c']]],\n",
    "            [[day_data['Weight']]],\n",
    "            [[day_data['Height']]],\n",
    "            day_data['Basal_stats_participant'],\n",
    "            day_data['HR_stats_participant']], axis=1))\n",
    "    \n",
    "        # Store labels in the dictionary\n",
    "        for label in y_labels.keys():\n",
    "            y_labels[label].append(day_data['next_day_cgm_labels_original'][label])\n",
    "    \n",
    "participant_ids = np.array(participant_ids)\n",
    "X_matrices_power = np.array(X_matrices_power)\n",
    "# X_matrices_coeffs = np.array(X_matrices_coeffs)\n",
    "X_features_daily_cgm = np.array(X_features_daily_cgm)\n",
    "X_features_individual_cgm = np.array(X_features_individual_cgm)\n",
    "X_features_daily = np.array(X_features_daily)\n",
    "X_features_individual = np.array(X_features_individual)\n",
    "\n",
    "# Convert labels to NumPy arrays\n",
    "for key in y_labels:\n",
    "    y_labels[key] = np.array(y_labels[key], dtype=np.float32)\n",
    "\n",
    "# Normalize the individual features\n",
    "def standard_scale(data):\n",
    "    scaler = StandardScaler()\n",
    "    if np.isinf(data).any():\n",
    "        print(\"There are infinite values in the data.\")\n",
    "        \n",
    "        # Replace infinite values with NaN (or a large number)\n",
    "        data[np.isinf(data)] = np.nan\n",
    "    return scaler.fit_transform(data.reshape(-1, data.shape[-1]))\n",
    "    \n",
    "# Apply vectorized scaling\n",
    "X_features_individual_cgm_scaled = standard_scale(X_features_individual_cgm)\n",
    "X_features_daily_cgm_scaled = standard_scale(X_features_daily_cgm)\n",
    "X_features_individual_scaled = standard_scale(X_features_individual)\n",
    "X_features_daily_scaled = standard_scale(X_features_daily)\n",
    "\n",
    "def check_nans(arr, name):\n",
    "    print(f\"{name}: {np.isnan(arr).sum()}  NaNs with shape: {arr.shape}\")\n",
    "\n",
    "print('NaNs in the Data ...')\n",
    "check_nans(X_matrices_power, \"X_matrices_power\")\n",
    "# check_nans(X_matrices_coeffs, \"X_matrices_coeffs\")\n",
    "check_nans(X_features_daily_cgm_scaled, \"X_features_daily_cgm\")\n",
    "check_nans(X_features_individual_cgm_scaled, \"X_features_individual_cgm\")\n",
    "check_nans(X_features_individual_scaled, \"X_features_individual_scaled\")\n",
    "check_nans(X_features_daily_scaled, \"X_features_daily_scaled\")\n",
    "for key in y_labels:\n",
    "    check_nans(y_labels[key], f\"y_label {key}\")\n",
    "\n",
    "#del filtered_data_dict, X_features_daily_cgm, X_features_individual_cgm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "803bae51-a7a3-4fe1-b150-cdebab0b4e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231 230\n",
      "NaNs in the Data ...\n",
      "X_matrices_power: 0  NaNs with shape: (1704, 288, 288, 3)\n",
      "X_features_daily_cgm: 0  NaNs with shape: (1704, 132)\n",
      "X_features_individual_cgm: 0  NaNs with shape: (1704, 154)\n",
      "y_label hypo_early_night: 0  NaNs with shape: (1704, 1)\n",
      "y_label hypo_night: 0  NaNs with shape: (1704, 1)\n",
      "y_label hypo_long_night: 0  NaNs with shape: (1704, 1)\n",
      "y_label hypo_night_morning: 0  NaNs with shape: (1704, 1)\n",
      "y_label hyper_day: 0  NaNs with shape: (1704, 1)\n",
      "y_label hypo_late_night: 0  NaNs with shape: (1704, 1)\n",
      "y_label hyper_night: 0  NaNs with shape: (1704, 1)\n",
      "y_label hyper_early_night: 0  NaNs with shape: (1704, 1)\n",
      "y_label hypo_morning: 0  NaNs with shape: (1704, 1)\n",
      "y_label hypo_day: 0  NaNs with shape: (1704, 1)\n"
     ]
    }
   ],
   "source": [
    "filtered_data_dict = {}\n",
    "for participant_id, days_data in data_dict.items():\n",
    "    filtered_days_data = []\n",
    "    for current_date, day_data in days_data.items():\n",
    "\n",
    "        if (day_data['len_original_cgm'] > 228 and np.sum(day_data['CGM_stats_daily_original'].isna()).sum()<1):\n",
    "            filtered_days_data.append(day_data)\n",
    "    \n",
    "    if filtered_days_data:\n",
    "        filtered_data_dict[participant_id] = filtered_days_data\n",
    "\n",
    "with open('/home/ma98/data_generated/pediatric_cgm_mex_morl_smooth.dill', 'wb') as f:\n",
    "    dill.dump(filtered_data_dict, f)\n",
    "    \n",
    "print(len(data_dict),len(filtered_data_dict))\n",
    "\n",
    "# Prepare the groups based on participant IDs\n",
    "participant_ids = []\n",
    "X_features_individual_cgm, X_features_daily_cgm = [], []\n",
    "X_matrices_power = [] #, X_matrices_coeffs\n",
    "y_labels = { \n",
    "    \"hypo_early_night\": [], \"hypo_night\": [], \"hypo_long_night\": [],\n",
    "    \"hypo_night_morning\": [], \"hyper_day\": [], \"hypo_late_night\": [],\n",
    "    \"hyper_night\": [], \"hyper_early_night\": [], \"hypo_morning\": [],\n",
    "    \"hypo_day\": []\n",
    "}\n",
    "\n",
    "for participant_id, days_data in filtered_data_dict.items():\n",
    "    participant_ids.extend([participant_id] * len(days_data))\n",
    "    for day_data in days_data:\n",
    "        X_matrices_power.append(day_data['full_matrix_cgm_power'])\n",
    "        # X_matrices_coeffs.append(day_data['full_matrix_cgm_coeffs'])\n",
    "        X_features_daily_cgm.append(day_data['CGM_stats_daily_original'])\n",
    "        X_features_individual_cgm.append(np.concatenate([\n",
    "            [[day_data['A1c']]],\n",
    "            day_data['CGM_stats_participant']], axis=1))\n",
    "    \n",
    "        # Store labels in the dictionary\n",
    "        for label in y_labels.keys():\n",
    "            y_labels[label].append(day_data['next_day_cgm_labels_original'][label])\n",
    "    \n",
    "participant_ids = np.array(participant_ids)\n",
    "X_matrices_power = np.array(X_matrices_power)\n",
    "# X_matrices_coeffs = np.array(X_matrices_coeffs)\n",
    "X_features_daily_cgm = np.array(X_features_daily_cgm)\n",
    "X_features_individual_cgm = np.array(X_features_individual_cgm)\n",
    "\n",
    "# Convert labels to NumPy arrays\n",
    "for key in y_labels:\n",
    "    y_labels[key] = np.array(y_labels[key], dtype=np.float32)\n",
    "\n",
    "# Normalize the individual features\n",
    "def standard_scale(data):\n",
    "    scaler = StandardScaler()\n",
    "    if np.isinf(data).any():\n",
    "        print(\"There are infinite values in the data.\")\n",
    "        \n",
    "        # Replace infinite values with NaN (or a large number)\n",
    "        data[np.isinf(data)] = np.nan\n",
    "    return scaler.fit_transform(data.reshape(-1, data.shape[-1]))\n",
    "    \n",
    "# Apply vectorized scaling\n",
    "X_features_individual_cgm_scaled = standard_scale(X_features_individual_cgm)\n",
    "X_features_daily_cgm_scaled = standard_scale(X_features_daily_cgm)\n",
    "\n",
    "def check_nans(arr, name):\n",
    "    print(f\"{name}: {np.isnan(arr).sum()}  NaNs with shape: {arr.shape}\")\n",
    "\n",
    "print('NaNs in the Data ...')\n",
    "check_nans(X_matrices_power, \"X_matrices_power\")\n",
    "# check_nans(X_matrices_coeffs, \"X_matrices_coeffs\")\n",
    "check_nans(X_features_daily_cgm_scaled, \"X_features_daily_cgm\")\n",
    "check_nans(X_features_individual_cgm_scaled, \"X_features_individual_cgm\")\n",
    "for key in y_labels:\n",
    "    check_nans(y_labels[key], f\"y_label {key}\")\n",
    "\n",
    "#del filtered_data_dict, X_features_daily_cgm, X_features_individual_cgm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1ed9e32-7f6a-4ee3-83dc-58282899f4d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def manual_standard_scaling(matrix):\n",
    "    # mean = np.nanmean(matrix)\n",
    "    return matrix / np.nanstd(matrix)\n",
    "    \n",
    "X_matrices_power_scaled = np.zeros_like(X_matrices_power)\n",
    "for i in range(X_matrices_power.shape[0]):\n",
    "    X_matrices_power_scaled[i, :, :, 0] = manual_standard_scaling(X_matrices_power[i, :, :, 0])\n",
    "    X_matrices_power_scaled[i, :, :, 1] = manual_standard_scaling(X_matrices_power[i, :, :, 1])\n",
    "    X_matrices_power_scaled[i, :, :, 2] = manual_standard_scaling(X_matrices_power[i, :, :, 2])\n",
    "\n",
    "del X_matrices_power\n",
    "import gc\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3584d52c-eb3a-48de-92cd-8545ae0b177c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "(1682, 171)\n",
      "float64\n",
      "X_features_individual_scaled: 159006  NaNs with shape: (1682, 171)\n",
      "float64\n",
      "(1682, 171)\n",
      "float64\n",
      "X_features_individual: 159006  NaNs with shape: (1682, 171)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Save dataset as a memory-mapped file (Only do this once)\n",
    "def save_memmap_data(X, filename_prefix):\n",
    "    if not os.path.exists(filename_prefix):\n",
    "        shape = X.shape\n",
    "        dtype = X.dtype\n",
    "        print(dtype)\n",
    "        memmap_data = np.memmap(f\"{filename_prefix}.npy\", dtype=dtype, mode='w+', shape=shape)\n",
    "        memmap_data[:] = X[:]  # Copy data into the memory-mapped array\n",
    "        del memmap_data  # Explicitly close the memory-mapped file\n",
    "\n",
    "save_memmap_data(X_features_daily_cgm_scaled, \"/home/__/data_generated/scaled_pediatric_features_individual\")\n",
    "\n",
    "memmap_file = \"/home/__/data_generated/scaled_pediatric_features_individual.npy\"\n",
    "shape =(1682, 171)\n",
    "dtype = X_features_individual_scaled.dtype\n",
    "matrices_memmap = np.memmap(memmap_file, mode='r',dtype=dtype, shape=shape)\n",
    "print(matrices_memmap.shape)\n",
    "print(matrices_memmap.dtype)\n",
    "check_nans(matrices_memmap, \"X_features_individual_scaled\")\n",
    "\n",
    "\n",
    "\n",
    "save_memmap_data(X_features_individual, \"/home/__/data_generated/pediatric_features_individual\")\n",
    "\n",
    "memmap_file = \"/home/__/data_generated/scaled_pediatric_features_individual.npy\"\n",
    "shape = (1682, 171)\n",
    "dtype = X_features_individual_cgm_scaled.dtype\n",
    "matrices_memmap = np.memmap(memmap_file, mode='r',dtype=dtype, shape=shape)\n",
    "print(matrices_memmap.shape)\n",
    "print(matrices_memmap.dtype)\n",
    "check_nans(matrices_memmap, \"X_features_individual\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc4f0abe-1851-4c9a-89f1-aa4624095ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1704, 288, 288, 3)\n",
      "float64\n",
      "(1704,)\n",
      "float16\n",
      "(1704,)\n",
      "float16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Save dataset as a memory-mapped file (Only do this once)\n",
    "def save_memmap_data(X, filename_prefix):\n",
    "    if not os.path.exists(filename_prefix):\n",
    "        shape = X.shape\n",
    "        dtype = X.dtype\n",
    "        memmap_data = np.memmap(f\"{filename_prefix}.npy\", dtype=dtype, mode='w+', shape=shape)\n",
    "        memmap_data[:] = X[:]  # Copy data into the memory-mapped array\n",
    "        del memmap_data  # Explicitly close the memory-mapped file\n",
    "\n",
    "save_memmap_data(X_matrices_power_scaled, \"/home/__/data_generated/pediatric_cgm_matrices\")\n",
    "\n",
    "memmap_file = \"/home/__/data_generated/pediatric_cgm_matrices.npy\"\n",
    "shape = (1704, 288, 288, 3)\n",
    "matrices_memmap = np.memmap(memmap_file, dtype=np.float64, mode='r', shape=shape)\n",
    "print(matrices_memmap.shape)\n",
    "print(matrices_memmap.dtype)\n",
    "\n",
    "def save_memmap_data(X, filename_prefix):\n",
    "    if not os.path.exists(filename_prefix):\n",
    "        shape = X.shape\n",
    "        # dtype = X.dtype\n",
    "        memmap_data = np.memmap(f\"{filename_prefix}.npy\", dtype=np.float16, mode='w+', shape=shape)\n",
    "        memmap_data[:] = X[:]  # Copy data into the memory-mapped array\n",
    "        del memmap_data  # Explicitly close the memory-mapped file\n",
    "save_memmap_data(participant_ids, \"/home/__/data_generated/pediatric_cgm_participant_ids\")\n",
    "\n",
    "participant_ids_memmap = np.memmap(\"/home/__/data_generated/pediatric_cgm_participant_ids.npy\", dtype=np.float16, mode='r')\n",
    "print(participant_ids_memmap.shape)\n",
    "print(participant_ids_memmap.dtype)\n",
    "\n",
    "\n",
    "# Path to store memory-mapped dataset\n",
    "memmap_file = \"/home/__/data_generated/pediatric_cgm_labels.npy\"\n",
    "dtype = np.float16\n",
    "shape = (1704, 1)\n",
    "# if not os.path.exists(memmap_file):\n",
    "labels_memmap = np.memmap(memmap_file, dtype=dtype, mode='w+', shape=shape)\n",
    "labels_memmap[:] = y_labels['hypo_night'][:]  # Copy data\n",
    "del labels_memmap  # Flush changes to disk\n",
    "\n",
    "# Step 2: Load memory-mapped dataset instead of keeping it in RAM\n",
    "labels_memmap = np.memmap(memmap_file, dtype=np.float16, mode='r')\n",
    "print(labels_memmap.shape)\n",
    "print(labels_memmap.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa58d163-ed12-4208-8361-ae5aaac48640",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
