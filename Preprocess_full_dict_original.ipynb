{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83135571-a827-44cb-9fee-23fa6e9f63ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-25 12:16:06.891035: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-25 12:16:07.012296: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-25 12:16:07.012326: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-25 12:16:07.012334: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-25 12:16:07.035990: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from scipy import stats\n",
    "from scipy.signal import find_peaks, detrend\n",
    "import pywt\n",
    "\n",
    "from numpy import arange, array, linspace, loadtxt, log2, logspace, mean, polyfit\n",
    "from numpy import zeros, pi, sin, cos, arctan2, sqrt, real, imag, conj, tile\n",
    "from numpy import round, interp, diff, unique, where\n",
    "from pandas import DataFrame, date_range\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib import pyplot\n",
    "from scipy.stats import pearsonr, mannwhitneyu, kruskal, norm\n",
    "from sklearn.linear_model import LinearRegression, Ridge, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GroupKFold\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score,f1_score, recall_score, precision_score,roc_auc_score, roc_curve, auc,accuracy_score,classification_report,confusion_matrix\n",
    "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier, GradientBoostingClassifier,AdaBoostClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.utils import shuffle\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import GlobalAveragePooling2D,GlobalMaxPooling2D,Concatenate,Input,Dropout, Conv2D, MaxPooling2D, Flatten,Multiply,Attention,Dense,concatenate,Masking,BatchNormalization, Reshape\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.applications import DenseNet121\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, History\n",
    "history = History()\n",
    "\n",
    "import pickle\n",
    "import joblib\n",
    "import dill\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3,4,5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a78b1cc-9d43-4dd3-9364-3a89c41d8889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "493\n"
     ]
    }
   ],
   "source": [
    "DEXI_folder = '/mnt/data1/ma98/Compressed_DEXI_Data/'\n",
    "\n",
    "T1DEXI_CGM_Dataset = pd.read_parquet(os.path.join(DEXI_folder, 'T1DEXI_Dataset.csv')) \n",
    "T1DEXI_CGM_Dataset['LBDTC'] = pd.to_datetime(T1DEXI_CGM_Dataset['LBDTC'])\n",
    "\n",
    "T1DEXI_Insulin_Dataset = pd.read_parquet(os.path.join(DEXI_folder, 'T1DEXI_Insulin_Dataset.csv'))  \n",
    "T1DEXI_Insulin_Dataset['FADTC'] = pd.to_datetime(T1DEXI_Insulin_Dataset['FADTC'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "T1DEXI_Reqcue_Carbs_Dataset = pd.read_parquet(os.path.join(DEXI_folder, 'T1DEXI_Reqcue_Carbs_Dataset.csv'))  \n",
    "T1DEXI_Reqcue_Carbs_Dataset['MLDTC'] = pd.to_datetime(T1DEXI_Reqcue_Carbs_Dataset['MLDTC'])\n",
    "\n",
    "FAMLPM_Dataset = pd.read_parquet(os.path.join(DEXI_folder, 'FAMLPM_Dataset.csv')) \n",
    "FAMLPM_Dataset['FADTC'] = pd.to_datetime(FAMLPM_Dataset['FADTC'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "T1DEXI_sleep_Dataset = pd.read_parquet(os.path.join(DEXI_folder, 'T1DEXI_sleep_Dataset.csv')) \n",
    "\n",
    "T1DEXI_VS_Dataset = pd.read_csv(os.path.join(DEXI_folder, 'T1DEXI_VS_Dataset.csv.gz'))  \n",
    "T1DEXI_VS_Dataset['VSDTC'] = pd.to_datetime(T1DEXI_VS_Dataset['VSDTC'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "#T1DEXI_basal = T1DEXI_Insulin_Dataset[T1DEXI_Insulin_Dataset['INSDVSRC'] != \"Injections\"]\n",
    "T1DEXI_basal = T1DEXI_Insulin_Dataset.copy()\n",
    "shared_participants = sorted(set(T1DEXI_CGM_Dataset['USUBJID']) & set(T1DEXI_basal['USUBJID'])& set(T1DEXI_Reqcue_Carbs_Dataset['USUBJID'])& \n",
    "                             set(FAMLPM_Dataset['USUBJID']) & set(T1DEXI_VS_Dataset['USUBJID'])) \n",
    "\n",
    "print(len(shared_participants))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "377a66ae-8674-47dd-8b4e-9ff06abfa88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values: 0\n",
      "Average: 146.14627450265544\n",
      "Standard Deviation: 56.38279796326335\n",
      "Percentage of data below 70: 3.08%\n"
     ]
    }
   ],
   "source": [
    "CGM_data = T1DEXI_CGM_Dataset[T1DEXI_CGM_Dataset['LBTEST'] == \"Glucose\"].dropna(subset=['LBSTRESN']).reset_index()['LBSTRESN']\n",
    "missing_values = CGM_data.isna().sum()\n",
    "print(f\"Number of missing values: {missing_values}\")\n",
    "\n",
    "average = CGM_data.mean()\n",
    "std_dev = CGM_data.std()\n",
    "print(f\"Average: {average}\")\n",
    "print(f\"Standard Deviation: {std_dev}\")\n",
    "\n",
    "threshold = 70\n",
    "below_threshold_count = (CGM_data < threshold).sum()\n",
    "non_missing_count = len(CGM_data) - missing_values\n",
    "percentage_below_threshold = (below_threshold_count / non_missing_count) * 100\n",
    "print(f\"Percentage of data below {threshold}: {percentage_below_threshold:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d39a12-32ca-49e3-b47b-8be101f92649",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = defaultdict(lambda: defaultdict(dict))\n",
    "\n",
    "for participant_id in shared_participants: \n",
    "    participant_cgm_data = T1DEXI_CGM_Dataset[T1DEXI_CGM_Dataset['USUBJID'] == participant_id].reset_index()\n",
    "\n",
    "    a1c_data = participant_cgm_data[participant_cgm_data['LBTEST'] == \"Hemoglobin A1c\"]\n",
    "    a1c = a1c_data['LBSTRESN'].iloc[0]\n",
    "\n",
    "    participant_cgm_data = participant_cgm_data[participant_cgm_data['LBTEST'] == \"Glucose\"].dropna(subset=['LBSTRESN']).reset_index()\n",
    "\n",
    "    aligned_cgm = resample_avg(participant_cgm_data['LBSTRESN'], participant_cgm_data['LBDTC'], interval='5T')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf1997af-d274-447e-bb0c-06e3b463da34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cgm_prediction_stats(current_day_cgm):\n",
    "    general_stats = calculate_stats(current_day_cgm.values[:,0])\n",
    "    general_stats = extract_prediction_stats(general_stats, 'cgm')\n",
    "\n",
    "    masks = {\n",
    "        'night': (current_day_cgm.index.hour >= 0) & (current_day_cgm.index.hour < 6),\n",
    "        'early_night': (current_day_cgm.index.hour >= 0) & (current_day_cgm.index.hour < 3),\n",
    "        'late_night': (current_day_cgm.index.hour >= 3) & (current_day_cgm.index.hour < 6),\n",
    "        'long_night': (current_day_cgm.index.hour >= 0) & (current_day_cgm.index.hour < 8),\n",
    "        'night_morning': (current_day_cgm.index.hour >= 0) & (current_day_cgm.index.hour < 12),\n",
    "        'morning': (current_day_cgm.index.hour >= 6) & (current_day_cgm.index.hour < 12),\n",
    "        'afternoon': (current_day_cgm.index.hour >= 12) & (current_day_cgm.index.hour < 18),\n",
    "        'evening': (current_day_cgm.index.hour >= 18) & (current_day_cgm.index.hour < 24)\n",
    "    }\n",
    "    time_range_stats = {}\n",
    "    for period, mask in masks.items():\n",
    "        period_stats = calculate_stats(current_day_cgm, mask)\n",
    "        period_stats = extract_prediction_stats(period_stats, f'cgm_{period}')\n",
    "        time_range_stats.update(period_stats)\n",
    "    \n",
    "    time_in_range_stats = {\n",
    "        'time_in_range_70_140_day': calculate_time_in_range(current_day_cgm, 70, 140),\n",
    "        'time_in_range_70_180_day': calculate_time_in_range(current_day_cgm, 70, 180),\n",
    "        'time_below_range_70_day': calculate_time_in_range(current_day_cgm, 0, 70),\n",
    "        'time_above_range_140_day': calculate_time_in_range(current_day_cgm, 140, np.inf),\n",
    "        'time_above_range_180_day': calculate_time_in_range(current_day_cgm, 180, np.inf)\n",
    "    }\n",
    "    for period, mask in masks.items():\n",
    "        time_in_range_stats[f'time_in_range_70_140_{period}'] = calculate_time_in_range(current_day_cgm, 70, 140, mask)\n",
    "        time_in_range_stats[f'time_in_range_70_180_{period}'] = calculate_time_in_range(current_day_cgm, 70, 180, mask)\n",
    "        time_in_range_stats[f'time_below_range_70_{period}'] = calculate_time_in_range(current_day_cgm, 0, 70, mask)\n",
    "        time_in_range_stats[f'time_above_range_140_{period}'] = calculate_time_in_range(current_day_cgm, 140, np.inf, mask)\n",
    "        time_in_range_stats[f'time_above_range_180_{period}'] = calculate_time_in_range(current_day_cgm, 180, np.inf, mask)\n",
    "    \n",
    "    cgm_prediction_stats = pd.DataFrame({**general_stats, **time_range_stats, **time_in_range_stats})\n",
    "    return cgm_prediction_stats\n",
    "\n",
    "def get_daily_cgm_stats(current_day_cgm):\n",
    "    general_stats = calculate_stats(current_day_cgm.values[:,0])\n",
    "    general_stats = extract_stats(general_stats, 'daily_cgm')\n",
    "    mean_crossings = calculate_crossings(current_day_cgm.values[:,0])\n",
    "    general_stats['daily_cgm_mean_crossings'] = mean_crossings\n",
    "\n",
    "    # Add new features\n",
    "    last_cgm_value = current_day_cgm.values[-1, 0]\n",
    "    general_stats['last_cgm_value'] = last_cgm_value\n",
    "    general_stats['last_cgm_sq'] = np.sqrt(last_cgm_value)\n",
    "    \n",
    "    diff_10_min = last_cgm_value - current_day_cgm.values[-3, 0]\n",
    "    general_stats['diff_10_min'] = diff_10_min\n",
    "\n",
    "    diff_20_min = last_cgm_value - current_day_cgm.values[-5, 0]\n",
    "    general_stats['diff_20_min'] = diff_20_min\n",
    "\n",
    "    diff_30_min = last_cgm_value - current_day_cgm.values[-7, 0]\n",
    "    general_stats['diff_30_min'] = diff_30_min\n",
    "\n",
    "    diff_50_min = last_cgm_value - current_day_cgm.values[-11, 0]\n",
    "    general_stats['diff_50_min'] = diff_50_min\n",
    "    \n",
    "    # Rate of change (slope) in CGM in the last hours\n",
    "    slope_last_half_hour = (last_cgm_value - current_day_cgm.values[-7, 0]) / last_cgm_value\n",
    "    general_stats['slope_last_half_hour'] = slope_last_half_hour\n",
    "    slope_last_hour = (last_cgm_value - current_day_cgm.values[-13, 0]) / last_cgm_value\n",
    "    general_stats['slope_last_hour'] = slope_last_hour\n",
    "    slope_last_2_hour = (last_cgm_value - current_day_cgm.values[-25, 0]) / last_cgm_value\n",
    "    general_stats['slope_last_2_hour'] = slope_last_2_hour\n",
    "    \n",
    "    # SD of CGM in the last hours\n",
    "    std_last_2_hours = np.std(current_day_cgm.values[-25:, 0])\n",
    "    general_stats['std_last_2_hours'] = std_last_2_hours\n",
    "    std_last_hour = np.std(current_day_cgm.values[-13:, 0])\n",
    "    general_stats['std_last_hour'] = std_last_hour\n",
    "    \n",
    "    # Sum of all increments in adjacent CGM observations in the last two hours\n",
    "    increments_sum_last_2_hours = np.sum(np.diff(current_day_cgm.values[-25:, 0])[np.diff(current_day_cgm.values[-25:, 0]) > 0])\n",
    "    general_stats['increments_sum_last_2_hours'] = increments_sum_last_2_hours\n",
    "\n",
    "    # Sum of all decrements in adjacent CGM observations in the last two hours\n",
    "    decrements_sum_last_2_hours = np.sum(np.diff(current_day_cgm.values[-25:, 0])[np.diff(current_day_cgm.values[-25:, 0]) < 0])\n",
    "    general_stats['decrements_sum_last_2_hours'] = decrements_sum_last_2_hours\n",
    "\n",
    "    # Maximum increase in adjacent CGM observations in the last two hours\n",
    "    max_increase_last_2_hours = np.max(np.diff(current_day_cgm.values[-25:, 0]))\n",
    "    general_stats['max_increase_last_2_hours'] = max_increase_last_2_hours\n",
    "\n",
    "    # Maximum decrease in adjacent CGM observations in the last two hours\n",
    "    max_decrease_last_2_hours = np.min(np.diff(current_day_cgm.values[-25:, 0]))\n",
    "    general_stats['max_decrease_last_2_hours'] = max_decrease_last_2_hours\n",
    "\n",
    "    masks = {\n",
    "        'night': (current_day_cgm.index.hour >= 0) & (current_day_cgm.index.hour < 6),\n",
    "        'morning': (current_day_cgm.index.hour >= 6) & (current_day_cgm.index.hour < 12),\n",
    "        'afternoon': (current_day_cgm.index.hour >= 12) & (current_day_cgm.index.hour < 18),\n",
    "        'evening': (current_day_cgm.index.hour >= 18) & (current_day_cgm.index.hour < 24),\n",
    "        'late_evening': (current_day_cgm.index.hour >= 21) & (current_day_cgm.index.hour < 24)\n",
    "    }\n",
    "    time_range_stats = {}\n",
    "    for period, mask in masks.items():\n",
    "        period_stats = calculate_stats(current_day_cgm, mask)\n",
    "        period_stats = extract_stats(period_stats, f'daily_cgm_{period}')\n",
    "        time_range_stats.update(period_stats)\n",
    "    \n",
    "    time_in_range_stats = {\n",
    "        'daily_time_in_range_70_140_day': calculate_time_in_range(current_day_cgm, 70, 140),\n",
    "        'daily_time_in_range_70_180_day': calculate_time_in_range(current_day_cgm, 70, 180),\n",
    "        'daily_time_below_range_70_day': calculate_time_in_range(current_day_cgm, 0, 70),\n",
    "        'daily_time_above_range_140_day': calculate_time_in_range(current_day_cgm, 140, np.inf),\n",
    "        'daily_time_above_range_180_day': calculate_time_in_range(current_day_cgm, 180, np.inf)\n",
    "    }\n",
    "    for period, mask in masks.items():\n",
    "        time_in_range_stats[f'daily_time_in_range_70_140_{period}'] = calculate_time_in_range(current_day_cgm, 70, 140, mask)\n",
    "        time_in_range_stats[f'daily_time_in_range_70_180_{period}'] = calculate_time_in_range(current_day_cgm, 70, 180, mask)\n",
    "        time_in_range_stats[f'daily_time_below_range_70_{period}'] = calculate_time_in_range(current_day_cgm, 0, 70, mask)\n",
    "        time_in_range_stats[f'daily_time_above_range_140_{period}'] = calculate_time_in_range(current_day_cgm, 140, np.inf, mask)\n",
    "        time_in_range_stats[f'daily_time_above_range_180_{period}'] = calculate_time_in_range(current_day_cgm, 180, np.inf, mask)\n",
    "    \n",
    "    daily_cgm_stats = pd.DataFrame({**general_stats, **time_range_stats, **time_in_range_stats})\n",
    "    return daily_cgm_stats\n",
    "\n",
    "def get_total_cgm_stats(current_day_cgm):\n",
    "    general_stats = calculate_stats(current_day_cgm.values[:,0])\n",
    "    general_stats = extract_stats(general_stats, 'total_cgm')\n",
    "    mean_crossings = calculate_crossings(current_day_cgm.values[:,0])\n",
    "    general_stats['cgm_mean_crossings'] = mean_crossings\n",
    "\n",
    "    masks = {\n",
    "        'night': (current_day_cgm.index.hour >= 0) & (current_day_cgm.index.hour < 6),\n",
    "        'early_night': (current_day_cgm.index.hour >= 0) & (current_day_cgm.index.hour < 3),\n",
    "        'late_night': (current_day_cgm.index.hour >= 3) & (current_day_cgm.index.hour < 6),\n",
    "        'morning': (current_day_cgm.index.hour >= 6) & (current_day_cgm.index.hour < 12),\n",
    "        'afternoon': (current_day_cgm.index.hour >= 12) & (current_day_cgm.index.hour < 18),\n",
    "        'evening': (current_day_cgm.index.hour >= 18) & (current_day_cgm.index.hour < 24)\n",
    "    }\n",
    "    time_range_stats = {}\n",
    "    for period, mask in masks.items():\n",
    "        period_stats = calculate_stats(current_day_cgm, mask)\n",
    "        period_stats = extract_stats(period_stats, f'total_cgm_{period}')\n",
    "        time_range_stats.update(period_stats)\n",
    "    \n",
    "    time_in_range_stats = {\n",
    "        'total_time_in_range_70_140_day': calculate_time_in_range(current_day_cgm, 70, 140),\n",
    "        'total_time_in_range_70_170_day': calculate_time_in_range(current_day_cgm, 70, 180),\n",
    "        'total_time_below_range_80_day': calculate_time_in_range(current_day_cgm, 0, 70),\n",
    "        'total_time_above_range_140_day': calculate_time_in_range(current_day_cgm, 140, np.inf),\n",
    "        'total_time_above_range_180_day': calculate_time_in_range(current_day_cgm, 180, np.inf)\n",
    "    }\n",
    "    for period, mask in masks.items():\n",
    "        time_in_range_stats[f'total_time_in_range_70_140_{period}'] = calculate_time_in_range(current_day_cgm, 70, 140, mask)\n",
    "        time_in_range_stats[f'total_time_in_range_70_180_{period}'] = calculate_time_in_range(current_day_cgm, 70, 180, mask)\n",
    "        time_in_range_stats[f'total_time_below_range_70_{period}'] = calculate_time_in_range(current_day_cgm, 0, 70, mask)\n",
    "        time_in_range_stats[f'total_time_above_range_140_{period}'] = calculate_time_in_range(current_day_cgm, 140, np.inf, mask)\n",
    "        time_in_range_stats[f'total_time_above_range_180_{period}'] = calculate_time_in_range(current_day_cgm, 180, np.inf, mask)\n",
    "    \n",
    "    total_cgm_stats = pd.DataFrame({**general_stats, **time_range_stats, **time_in_range_stats})\n",
    "    return total_cgm_stats\n",
    "\n",
    "def daily_cgm_labels(current_day_cgm):\n",
    "    masks = {\n",
    "        'night': (current_day_cgm.index.hour >= 0) & (current_day_cgm.index.hour < 6),\n",
    "        'early_night': (current_day_cgm.index.hour >= 0) & (current_day_cgm.index.hour < 3),\n",
    "        'late_night': (current_day_cgm.index.hour >= 3) & (current_day_cgm.index.hour < 6),\n",
    "        'long_night': (current_day_cgm.index.hour >= 0) & (current_day_cgm.index.hour < 8),\n",
    "        'night_morning': (current_day_cgm.index.hour >= 0) & (current_day_cgm.index.hour < 12),\n",
    "        'morning': (current_day_cgm.index.hour >= 6) & (current_day_cgm.index.hour < 12),\n",
    "        'afternoon': (current_day_cgm.index.hour >= 12) & (current_day_cgm.index.hour < 18),\n",
    "        'evening': (current_day_cgm.index.hour >= 18) & (current_day_cgm.index.hour < 24)\n",
    "    }\n",
    "    labels = {\n",
    "        'hyper_day': return_label(current_day_cgm.values[:,0], 180),\n",
    "        'hypo_day': return_label(current_day_cgm.values[:,0], 70)\n",
    "    }\n",
    "    for period, mask in masks.items():\n",
    "        labels[f'hyper_{period}'] = return_label(current_day_cgm.values[:,0], 180, mask)\n",
    "        labels[f'hypo_{period}'] = return_label(current_day_cgm.values[:,0], 70, mask)\n",
    "    \n",
    "    daily_cgm_labels = pd.DataFrame({**labels}, index=[0])\n",
    "    return daily_cgm_labels\n",
    "\n",
    "# --- Calculate Time in Range for Different Ranges and Periods ---\n",
    "def calculate_time_in_range(glucose_data, range_lower, range_upper, period_mask=None):\n",
    "\n",
    "    if period_mask is not None:\n",
    "        glucose_data = glucose_data[period_mask]\n",
    "    in_range = (glucose_data <= range_upper) & (glucose_data >= range_lower)\n",
    "    return 100 * in_range.sum() / len(glucose_data) if len(glucose_data) > 0 else 0\n",
    "\n",
    "def return_label(glucose_data, range, period_mask=None):\n",
    "    if period_mask is not None:\n",
    "        glucose_data = glucose_data[period_mask]\n",
    "    if range==180:\n",
    "        return np.sum(glucose_data>range)>2\n",
    "    if range==70:\n",
    "        return np.sum(glucose_data<range)>2\n",
    "        \n",
    "def calculate_sleep_statistics(daily_sleep_data):\n",
    "    \n",
    "    participant_total_sleep = participant_sleep_data[participant_sleep_data['NVTEST'] == \"Total Sleep Time\"]\n",
    "    participant_deep_sleep = participant_sleep_data[participant_sleep_data['NVTEST'] == \"Deep NREM Duration\"]\n",
    "    participant_light_sleep = participant_sleep_data[participant_sleep_data['NVTEST'] == \"Light NREM Duration\"]\n",
    "    participant_NREM_sleep = participant_sleep_data[participant_sleep_data['NVTEST'] == \"NREM Duration\"]\n",
    "    participant_REM_sleep = participant_sleep_data[participant_sleep_data['NVTEST'] == \"REM Duration\"]\n",
    "    participant_efficiency = participant_sleep_data[participant_sleep_data['NVTEST'] == \"Sleep Efficiency\"]\n",
    "    participant_awakenings = participant_sleep_data[participant_sleep_data['NVTEST'] == \"Number of Awakenings\"]\n",
    "    participant_latency = participant_sleep_data[participant_sleep_data['NVTEST'] == \"Sleep Onset Latency\"]\n",
    "\n",
    "    avg_total_sleep = (participant_total_sleep['NVORRES']).mean()\n",
    "    std_total_sleep = (participant_total_sleep['NVORRES']).std()\n",
    "    avg_deep_sleep = (participant_deep_sleep['NVORRES']).mean()\n",
    "    std_deep_sleep = (participant_deep_sleep['NVORRES']).std()\n",
    "    avg_light_sleep = (participant_light_sleep['NVORRES']).mean()\n",
    "    std_light_sleep = (participant_light_sleep['NVORRES']).std()\n",
    "    avg_NREM_sleep = (participant_NREM_sleep['NVORRES']).mean()\n",
    "    std_NREM_sleep = (participant_NREM_sleep['NVORRES']).std()\n",
    "    avg_REM_sleep = (participant_REM_sleep['NVORRES']).mean()\n",
    "    std_REM_sleep = (participant_REM_sleep['NVORRES']).std()\n",
    "    avg_awakenings = (participant_awakenings['NVORRES']*100000.).mean()\n",
    "    std_awakenings = (participant_awakenings['NVORRES']*100000. ).std()\n",
    "    avg_latency = (participant_latency['NVORRES']).mean()\n",
    "    std_latency = (participant_latency['NVORRES']).std()\n",
    "    avg_efficiency = (participant_efficiency['NVORRES']*100000.).mean()\n",
    "    std_efficiency = (participant_efficiency['NVORRES']*100000.).std()\n",
    "    \n",
    "    # Calculate avg deviation from midnight\n",
    "    bedtime_deviations_midnight = []\n",
    "    wakeup_deviations_midnight = []\n",
    "    for dt in participant_total_sleep['NVDTC']:\n",
    "        sleep_time = 60*dt.hour + (dt.minute)\n",
    "        \n",
    "        if sleep_time >720:\n",
    "            sleep_time -= 1440\n",
    "        bedtime_deviations_midnight.append(sleep_time)\n",
    "    \n",
    "    for dt in participant_total_sleep['NVENDTC']:\n",
    "        wakeup_time = 60*dt.hour + (dt.minute)\n",
    "        wakeup_deviations_midnight.append(wakeup_time)\n",
    "  \n",
    "    average_bedtime_midnight = np.mean(bedtime_deviations_midnight)\n",
    "    average_wakeup_midnight = np.mean(wakeup_deviations_midnight)\n",
    "    std_bedtime_midnight = np.std(bedtime_deviations_midnight)\n",
    "    std_wakeup_midnight =  np.std(wakeup_deviations_midnight)\n",
    "   #     # Convert average deviation back to time format\n",
    "    average_bedtime = pd.to_datetime('00:00:00') + pd.to_timedelta(average_bedtime_midnight, unit='m')\n",
    "    average_wakeup = pd.to_datetime('00:00:00') + pd.to_timedelta(average_wakeup_midnight, unit='m')\n",
    "       \n",
    "    # Calculate bedtime deviations from average & variance & Consistency Score (inverse of variance)\n",
    "    participant_sleep_data['bedtime_from_avg'] = ((60*participant_sleep_data['NVDTC'].dt.hour + participant_sleep_data['NVDTC'].dt.minute) - average_bedtime_midnight)\n",
    "    participant_sleep_data.loc[participant_sleep_data['bedtime_from_avg'] > 720, 'bedtime_from_avg'] -= 1440\n",
    "\n",
    "    participant_sleep_data['wakeup_from_avg'] = ((60*participant_sleep_data['NVENDTC'].dt.hour + participant_sleep_data['NVENDTC'].dt.minute) - average_wakeup_midnight)\n",
    "    bedtime_std = np.round(participant_sleep_data['bedtime_from_avg'].std(),3)\n",
    "    wakeup_std = np.round(participant_sleep_data['wakeup_from_avg'].std(),3)\n",
    "    bedtime_var = np.round(participant_sleep_data['bedtime_from_avg'].var(),3)\n",
    "    wakeup_var = np.round(participant_sleep_data['wakeup_from_avg'].var(),3)\n",
    "    sleep_stats = pd.DataFrame({\n",
    "        'avg_bedtime_midnight': [average_bedtime_midnight.round(3)],\n",
    "        'avg_bedtime': [average_bedtime],\n",
    "        'bedtime_consistency_score': [(100. / bedtime_var).round(3)],\n",
    "        'bedtime_std': [bedtime_std],\n",
    "        'bedtime_var': [bedtime_var],\n",
    "        'avg_wakeup_midnight': [average_wakeup_midnight.round(3)],\n",
    "        'avg_wakeup': [average_wakeup],\n",
    "        'wakeup_consistency_score': [(100. / wakeup_var).round(3)],\n",
    "        'wakeup_std': [wakeup_std],\n",
    "        'wakeup_var': [wakeup_var],\n",
    "        'avg_deep_sleep': [round(avg_deep_sleep, 3)],\n",
    "        'std_deep_sleep': [round(std_deep_sleep, 3)],\n",
    "        'avg_total_sleep': [round(avg_total_sleep, 3)],\n",
    "        'std_total_sleep': [round(std_total_sleep, 3)],\n",
    "        'avg_light_sleep': [round(avg_light_sleep, 3)],\n",
    "        'std_light_sleep': [round(std_light_sleep, 3)],\n",
    "        'avg_NREM_sleep': [round(avg_NREM_sleep, 3)],\n",
    "        'std_NREM_sleep': [round(std_NREM_sleep, 3)],\n",
    "        'avg_REM_sleep': [round(avg_REM_sleep, 3)],\n",
    "        'std_REM_sleep': [round(std_REM_sleep, 3)],\n",
    "        'avg_awakenings': [round(avg_awakenings, 3)],\n",
    "        'std_awakenings': [round(std_awakenings, 3)],\n",
    "        'avg_latency': [round(avg_latency, 3)],\n",
    "        'std_latency': [round(std_latency, 3)],\n",
    "        'avg_efficiency': [round(avg_efficiency, 3)],\n",
    "        'std_efficiency': [round(std_efficiency, 3)]\n",
    "    })\n",
    "    return participant_sleep_data, sleep_stats\n",
    "   \n",
    "def calculate_stats(data, period_mask=None):\n",
    "    if period_mask is not None:\n",
    "        data = data[period_mask]\n",
    "        data = data.values[:,0]\n",
    "    data = data[~np.isnan(data)]\n",
    "    if len(data) == 0:\n",
    "        return pd.DataFrame({\n",
    "            'std': [np.nan],\n",
    "            'min': [np.nan],\n",
    "            'n5': [np.nan],\n",
    "            'n25': [np.nan],\n",
    "            'median': [np.nan],\n",
    "            'avg': [np.nan],\n",
    "            'n75': [np.nan],\n",
    "            'n95': [np.nan],\n",
    "            'max': [np.nan],\n",
    "            'var': [np.nan],\n",
    "            'std_score': [np.nan],\n",
    "            'var_score': [np.nan],\n",
    "            'rms': [np.nan],\n",
    "            'consistency_score': [np.nan],\n",
    "            'entropy': [np.nan]\n",
    "        })\n",
    "    max_val = round(np.nanmax(data), 3)\n",
    "    min_val = round(np.nanmin(data), 3)\n",
    "    avg_val = round(np.nanmean(data), 3)\n",
    "    median = round(np.nanpercentile(data, 50),3)\n",
    "    rms = round(np.nanmean(np.sqrt(data**2)),3)\n",
    "    \n",
    "    if len(data) > 1:\n",
    "        n5 = round(np.nanpercentile(data, 5), 3)\n",
    "        n25 = round(np.nanpercentile(data, 25), 3)\n",
    "        n75 = round(np.nanpercentile(data, 75), 3)\n",
    "        n95 = round(np.nanpercentile(data, 95), 3)\n",
    "        std_val = round(np.nanstd(data), 3)\n",
    "        var_val = round(np.nanvar(data), 3)\n",
    "        std_score_val = round(100 * np.nanstd(data) / np.nanmean(data), 3)\n",
    "        var_score_val = round(np.nanvar(data) / np.nanmean(data), 3)\n",
    "        consistency_score_val = round(100. / np.nanvar(data), 3) if np.nanvar(data) != 0 else np.nan\n",
    "        entropy = calculate_entropy(data)\n",
    "    else:\n",
    "        n5 = np.nan\n",
    "        n25 = np.nan\n",
    "        n75 = np.nan\n",
    "        n95 = np.nan\n",
    "        std_val = np.nan\n",
    "        var_val = np.nan\n",
    "        std_score_val = np.nan\n",
    "        var_score_val = np.nan\n",
    "        consistency_score_val = np.nan\n",
    "        entropy = np.nan\n",
    "    data_stats = pd.DataFrame({\n",
    "        'std': [std_val],\n",
    "        'min': [min_val],\n",
    "        'n5': [n5],\n",
    "        'n25': [n25],\n",
    "        'median': [median],\n",
    "        'avg': [avg_val],\n",
    "        'n75': [n75],\n",
    "        'n95': [n95],\n",
    "        'max': [max_val],\n",
    "        'var': [var_val],\n",
    "        'std_score': [std_score_val],\n",
    "        'var_score': [var_score_val],\n",
    "        'rms': [rms],\n",
    "        'consistency_score': [consistency_score_val],\n",
    "        'entropy': [entropy]\n",
    "    })\n",
    "    return data_stats\n",
    "\n",
    "# Helper function to extract values from the DataFrame\n",
    "def extract_stats(stats_df, prefix):\n",
    "    return {\n",
    "        f'{prefix}_std': float(stats_df['std'].values[0]),\n",
    "        f'{prefix}_min': float(stats_df['min'].values[0]),\n",
    "        f'{prefix}_n5': float(stats_df['n5'].values[0]),\n",
    "        f'{prefix}_n25': float(stats_df['n25'].values[0]),\n",
    "        f'{prefix}_median': float(stats_df['median'].values[0]),\n",
    "        f'{prefix}_avg': float(stats_df['avg'].values[0]),\n",
    "        f'{prefix}_n75': float(stats_df['n75'].values[0]),\n",
    "        f'{prefix}_n95': float(stats_df['n95'].values[0]),\n",
    "        f'{prefix}_max': float(stats_df['max'].values[0]),\n",
    "        f'{prefix}_var': float(stats_df['var'].values[0]),\n",
    "        f'{prefix}_std_score': float(stats_df['std_score'].values[0]),\n",
    "        f'{prefix}_var_score': float(stats_df['var_score'].values[0]),\n",
    "        f'{prefix}_rms': float(stats_df['rms'].values[0]),\n",
    "        f'{prefix}_consistency_score': float(stats_df['consistency_score'].values[0]),\n",
    "        f'{prefix}_entropy': float(stats_df['entropy'].values[0]),\n",
    "    }\n",
    "\n",
    "def extract_prediction_stats(stats_df, prefix):\n",
    "    return {\n",
    "        f'{prefix}_median': float(stats_df['median'].values[0])\n",
    "    }\n",
    "def extract_reduced_stats(stats_df, prefix):\n",
    "    return {\n",
    "        f'{prefix}_median': float(stats_df['median'].values[0]),\n",
    "        f'{prefix}_entropy': float(stats_df['entropy'].values[0])\n",
    "    }\n",
    "    \n",
    "def daily_sleep_stats(daily_sleep_data):\n",
    "    sleep_stats = pd.DataFrame({\n",
    "        'deep_sleep': (daily_sleep_data['NVORRES'][daily_sleep_data['NVTEST'] == \"Deep NREM Duration\"]).values.round(2),\n",
    "        'total_sleep': (daily_sleep_data['NVORRES'][daily_sleep_data['NVTEST'] == \"Total Sleep Time\"]).values.round(2),\n",
    "        'light_sleep': (daily_sleep_data['NVORRES'][daily_sleep_data['NVTEST'] == \"Light NREM Duration\"]).values.round(2),\n",
    "        'NREM_sleep': (daily_sleep_data['NVORRES'][daily_sleep_data['NVTEST'] == \"NREM Duration\"]).values.round(2),\n",
    "        'REM_sleep': (daily_sleep_data['NVORRES'][daily_sleep_data['NVTEST'] == \"REM Duration\"]).values.round(2),\n",
    "        'awakenings': (daily_sleep_data['NVORRES'][daily_sleep_data['NVTEST'] == \"Number of Awakenings\"]*100000).values.round(2),\n",
    "        'latency': (daily_sleep_data['NVORRES'][daily_sleep_data['NVTEST'] == \"Sleep Onset Latency\"]).values.round(2),\n",
    "        'efficiency': (daily_sleep_data['NVORRES'][daily_sleep_data['NVTEST'] == \"Sleep Efficiency\"]*100000).values.round(2),\n",
    "        'bedtime': daily_sleep_data['bedtime_from_avg'][daily_sleep_data['NVTEST'] == \"Total Sleep Time\"].values.round(2),\n",
    "        'wakeup': daily_sleep_data['wakeup_from_avg'][daily_sleep_data['NVTEST'] == \"Total Sleep Time\"].values.round(2)\n",
    "    })\n",
    "    return sleep_stats\n",
    "def calculate_sum(carbs_data, period_mask=None):\n",
    "    if period_mask is not None:\n",
    "        carbs_data = carbs_data[period_mask]\n",
    "    return np.sum(carbs_data['corrected_value']) if not carbs_data['corrected_value'].empty else 0\n",
    "\n",
    "def corrected_daily_carbs_stats(daily_carb_data):\n",
    "    overall_stats = calculate_stats(daily_carb_data['corrected_value'].values)\n",
    "    overall_stats = extract_prediction_stats(overall_stats, 'carbs')\n",
    "    \n",
    "    if len(daily_carb_data) > 0:\n",
    "        # Calculate carb on board at midnight considering only intake after 7 PM\n",
    "        start_time = pd.Timestamp(daily_carb_data['FADTC'].dt.date.iloc[0]) + pd.Timedelta(hours=19)  # 7 PM\n",
    "        midnight = pd.Timestamp(daily_carb_data['FADTC'].dt.date.iloc[0]) + pd.Timedelta(days=1)  # 12 AM next day\n",
    "        carb_on_board = 0.0\n",
    "        for _, row in daily_carb_data[daily_carb_data['FADTC'] >= start_time].iterrows():\n",
    "            carb_amount = row['corrected_value']\n",
    "            intake_time = row['FADTC']\n",
    "            absorption_start_time = intake_time + pd.Timedelta(minutes=15)\n",
    "\n",
    "            if midnight > absorption_start_time:\n",
    "                absorbed_duration = min((midnight - absorption_start_time).total_seconds() / 60, carb_amount / 0.5)\n",
    "                absorbed_carbs = absorbed_duration * 0.5\n",
    "                remaining_carbs = max(carb_amount - absorbed_carbs, 0)\n",
    "                carb_on_board += remaining_carbs\n",
    "    else:\n",
    "        carb_on_board = np.nan\n",
    "    overall_stats['carb_on_board_midnight'] = carb_on_board\n",
    "    \n",
    "    masks = {\n",
    "        'night': (daily_carb_data['FADTC'].dt.hour >= 0) & (daily_carb_data['FADTC'].dt.hour < 6),\n",
    "        'morning': (daily_carb_data['FADTC'].dt.hour >= 6) & (daily_carb_data['FADTC'].dt.hour < 12),\n",
    "        'afternoon': (daily_carb_data['FADTC'].dt.hour >= 12) & (daily_carb_data['FADTC'].dt.hour < 18),\n",
    "        'evening': (daily_carb_data['FADTC'].dt.hour >= 18) & (daily_carb_data['FADTC'].dt.hour < 24),\n",
    "        'late_evening': (daily_carb_data['FADTC'].dt.hour >= 21) & (daily_carb_data['FADTC'].dt.hour < 24)\n",
    "    } \n",
    "    time_range_stats = {}\n",
    "    if len(daily_carb_data) > 0:\n",
    "        time_range_stats['carbs_day'] = calculate_sum(daily_carb_data)\n",
    "    else:\n",
    "        time_range_stats['carbs_day'] = np.nan\n",
    "        \n",
    "    for period, mask in masks.items():\n",
    "        if len(daily_carb_data) > 0:\n",
    "            time_range_stats[f'carbs_{period}'] = calculate_sum(daily_carb_data, mask)\n",
    "        else:\n",
    "            time_range_stats[f'carbs_{period}'] = np.nan\n",
    "\n",
    "    carbs_stats = pd.DataFrame({**overall_stats, **time_range_stats}, index=[0])\n",
    "    return carbs_stats\n",
    "\n",
    "def total_stats(data, times, label_data='bolus'):\n",
    "    daily_data = pd.DataFrame({'datetime': times, 'value': data})\n",
    "    daily_data.set_index('datetime', inplace=True)\n",
    "    \n",
    "    general_stats = calculate_stats(daily_data.values[:,0])\n",
    "    general_stats = extract_stats(general_stats, label_data)\n",
    "    masks = {\n",
    "        'night': (daily_data.index.hour >= 0) & (daily_data.index.hour < 6),\n",
    "        'morning': (daily_data.index.hour >= 6) & (daily_data.index.hour < 12),\n",
    "        'afternoon': (daily_data.index.hour >= 12) & (daily_data.index.hour < 18),\n",
    "        'evening': (daily_data.index.hour >= 18) & (daily_data.index.hour < 24)\n",
    "    }\n",
    "    time_range_stats = {}\n",
    "    for period, mask in masks.items():\n",
    "        period_stats = calculate_stats(daily_data, mask)\n",
    "        period_stats = extract_stats(period_stats, f'{label_data}_{period}')\n",
    "        time_range_stats.update(period_stats)\n",
    "    \n",
    "    total_stats = pd.DataFrame({**general_stats, **time_range_stats}, index=[0])\n",
    "    return total_stats\n",
    "\n",
    "def daily_stats(data, times, label_data='basal'):\n",
    "    daily_data = pd.DataFrame({'datetime': times, 'value': data})\n",
    "    daily_data.set_index('datetime', inplace=True)\n",
    "    \n",
    "    general_stats = calculate_stats(daily_data.values[:,0])\n",
    "    general_stats = extract_stats(general_stats, label_data)\n",
    "    mean_crossings = calculate_crossings(daily_data.values[:,0])\n",
    "    general_stats[f'{label_data}_mean_crossings'] = mean_crossings\n",
    "    masks = {\n",
    "        'night': (daily_data.index.hour >= 0) & (daily_data.index.hour < 6),\n",
    "        'morning': (daily_data.index.hour >= 6) & (daily_data.index.hour < 12),\n",
    "        'afternoon': (daily_data.index.hour >= 12) & (daily_data.index.hour < 18),\n",
    "        'evening': (daily_data.index.hour >= 18) & (daily_data.index.hour < 24),\n",
    "        'late_evening': (daily_data.index.hour >= 22) & (daily_data.index.hour < 24)\n",
    "    }\n",
    "    time_range_stats = {}\n",
    "    for period, mask in masks.items():\n",
    "        period_stats = calculate_stats(daily_data, mask)\n",
    "        period_stats = extract_stats(period_stats, f'{label_data}_{period}')\n",
    "        time_range_stats.update(period_stats)\n",
    "    \n",
    "    daily_stats = pd.DataFrame({**general_stats, **time_range_stats}, index=[0])\n",
    "    return daily_stats\n",
    "def daily_stats_bolus(data, times, label_data='bolus'):\n",
    "    daily_data = pd.DataFrame({'datetime': times, 'value': data})\n",
    "    daily_data.set_index('datetime', inplace=True)\n",
    "    \n",
    "    general_stats = calculate_stats(daily_data.values[:,0])\n",
    "    general_stats = extract_stats(general_stats, label_data)\n",
    "    mean_crossings = calculate_crossings(daily_data.values[:,0])\n",
    "    general_stats[f'{label_data}_mean_crossings'] = mean_crossings\n",
    "    masks = {\n",
    "        'night': (daily_data.index.hour >= 0) & (daily_data.index.hour < 6),\n",
    "        'afternoon': (daily_data.index.hour >= 12) & (daily_data.index.hour < 18),\n",
    "        'evening': (daily_data.index.hour >= 18) & (daily_data.index.hour < 24)\n",
    "    }\n",
    "    time_range_stats = {}\n",
    "    for period, mask in masks.items():\n",
    "        period_stats = calculate_stats(daily_data, mask)\n",
    "        period_stats = extract_stats(period_stats, f'{label_data}_{period}')\n",
    "        time_range_stats.update(period_stats)\n",
    "    \n",
    "    daily_stats = pd.DataFrame({**general_stats, **time_range_stats}, index=[0])\n",
    "    return daily_stats\n",
    "def calculate_entropy(list_values):\n",
    "    counter_values = Counter(list_values).most_common()\n",
    "    probabilities = [elem[1]/len(list_values) for elem in counter_values]\n",
    "    entropy=stats.entropy(probabilities)\n",
    "    return entropy.round(3)\n",
    "\n",
    "def calculate_crossings(list_values):\n",
    "    mean_crossing_indices = np.nonzero(np.diff(list_values > np.nanmean(list_values)))[0]\n",
    "    no_mean_crossings = len(mean_crossing_indices)\n",
    "    return no_mean_crossings\n",
    "\n",
    "def resample_sum(data, times, interval='5T'):\n",
    "    df_data = pd.DataFrame({'datetime': times, 'value': data})\n",
    "    df_data.set_index('datetime', inplace=True)\n",
    "    data_resampled = df_data.copy()\n",
    "    data_resampled.index = data_resampled.index.floor(interval)\n",
    "    data_resampled = data_resampled.groupby(data_resampled.index).sum()\n",
    "    data_resampled = data_resampled.resample(interval).asfreq()\n",
    "    return data_resampled\n",
    "def resample_avg(data, times, interval='5T'):\n",
    "    df_data = pd.DataFrame({'datetime': times, 'value': data})\n",
    "    df_data.set_index('datetime', inplace=True)\n",
    "    data_resampled = df_data.copy()\n",
    "    data_resampled.index = data_resampled.index.floor(interval)\n",
    "    data_resampled = data_resampled.groupby(data_resampled.index).mean()\n",
    "    data_resampled = data_resampled.resample(interval).asfreq()\n",
    "    return data_resampled\n",
    "\n",
    "def smooth_signal(df, window_size=17, treshold=10, order=None):\n",
    "    data = df['value']\n",
    "\n",
    "    if order is not None:\n",
    "        # Temporarily fill NaNs with interpolation to avoid issues with savgol_filter\n",
    "        data_filled = data.interpolate(method='linear')\n",
    "        # Apply savgol_filter only to non-NaN values\n",
    "        smoothed_filled = savgol_filter(data_filled.fillna(0), window_length=window_size-8, polyorder=order)\n",
    "        # Reintroduce NaNs where they were originally\n",
    "        smoothed_values = pd.Series(smoothed_filled, index=data.index)\n",
    "        smoothed_values[np.isnan(data)] = np.nan\n",
    "    else:\n",
    "        smoothed_values = data\n",
    "        \n",
    "    half_window = window_size // 2\n",
    "    # Generate a Gaussian (normal) distribution for weights\n",
    "    # The mean is at the center of the window, and the standard deviation determines the spread\n",
    "    weights = norm.pdf(np.arange(-half_window, half_window + 1), 0, 2)\n",
    "    # Normalize weights so that they sum to 1\n",
    "    weights /= weights.sum()\n",
    "    data_smooth = pd.Series(smoothed_values, index=df.index)\n",
    "    rolling_avg = data_smooth.rolling(window=window_size, center=True).apply(lambda x: np.dot(x, weights), raw=True)\n",
    "\n",
    "    #rolling_avg = data.rolling(window=window_size, center=True).mean()\n",
    "    fluctuations = data - rolling_avg\n",
    "    smoothed_values = rolling_avg + fluctuations.where(fluctuations.abs() > treshold, 0)\n",
    "    \n",
    "    weights = norm.pdf(np.arange(-3, 3 + 1), 0, 2)\n",
    "    weights /= weights.sum()\n",
    "    rolling_avg = smoothed_values.rolling(window=7, center=True).apply(lambda x: np.dot(x, weights), raw=True)\n",
    "    fluctuations = smoothed_values - rolling_avg\n",
    "    smoothed_values = rolling_avg + fluctuations.where(fluctuations.abs() > treshold, 0)\n",
    "\n",
    "    data_filled = smoothed_values.interpolate(method='linear')\n",
    "    # Apply savgol_filter only to non-NaN values\n",
    "    smoothed_filled = savgol_filter(data_filled.fillna(0), window_length=window_size-10, polyorder=order+1)\n",
    "    # Reintroduce NaNs where they were originally\n",
    "    smoothed_values = pd.Series(smoothed_filled, index=data.index)\n",
    "    smoothed_values[np.isnan(data)] = np.nan\n",
    "        \n",
    "    smoothed_df = pd.DataFrame({'datetime': df.index, 'value': smoothed_values})\n",
    "    smoothed_df.set_index('datetime', inplace=True)\n",
    "    return smoothed_df\n",
    "\n",
    "def plot_cwt(data, power_mex,period_mex,power_morl, period_morl, waveletname = 'mexh', label_data = 'CGM '):\n",
    "    times = data.index\n",
    "    levels = [0.0625, 0.125, 0.25, 0.5, 1, 2, 4, 8,16,32,64]\n",
    "    contourlevels = np.log2(levels)\n",
    "    title_mex = \"MexicanHat Power Spectrum of \"+ str(label_data)\n",
    "    title_morl = \"Morlet Power Spectrum of \"+ str(label_data)\n",
    "    ylabel = 'Period (Hours)'\n",
    "    xlabel = 'Time'\n",
    "    cmap = plt.cm.jet\n",
    "\n",
    "    ax[0].contourf(times, np.log2(period_mex), np.log2(power_mex/1024), contourlevels, extend='both',cmap=cmap)\n",
    "    ax[0].set_title(title_mex, fontsize=20)\n",
    "    ax[0].set_ylabel(ylabel, fontsize=16)\n",
    "    ax[0].set_xlabel(xlabel, fontsize=16)\n",
    "    \n",
    "    yticks = 2**np.arange(np.ceil(np.log2(period_mex.min())), np.ceil(np.log2(period_mex.max())))\n",
    "    ax[0].set_yticks(np.log2(yticks))\n",
    "    ax[0].set_yticklabels(yticks)\n",
    "    ax[0].invert_yaxis()\n",
    "    ax[0].set_ylim(np.log2([2*period_mex.min(), period_mex.max()]))\n",
    "    \n",
    "    ax[1].contourf(times, np.log2(period_morl), np.log2(power_morl/1024), contourlevels, extend='both',cmap=cmap)\n",
    "    ax[1].set_title(title_morl, fontsize=20)\n",
    "    ax[1].set_ylabel(ylabel, fontsize=16)\n",
    "    ax[1].set_xlabel(xlabel, fontsize=16)\n",
    "    \n",
    "    yticks = 2**np.arange(np.ceil(np.log2(period_morl.min())), np.ceil(np.log2(period_morl.max())))\n",
    "    ax[1].set_yticks(np.log2(yticks))\n",
    "    ax[1].set_yticklabels(yticks)\n",
    "    ax[1].invert_yaxis()\n",
    "    ax[1].set_ylim(np.log2([2*period_morl.min(), period_morl.max()]))\n",
    "    \n",
    "    if len(data)<300:\n",
    "        ax[0].xaxis.set_major_locator(mdates.HourLocator(interval=2))\n",
    "        ax[0].xaxis.set_major_formatter(mdates.DateFormatter('%m/%d--%H:%M'))\n",
    "        ax[0].xaxis.set_minor_locator(mdates.HourLocator(byhour=[6, 12, 18]))\n",
    "        ax[0].xaxis.grid(True, which='minor') \n",
    "\n",
    "        ax[1].xaxis.set_major_locator(mdates.HourLocator(interval=2))\n",
    "        ax[1].xaxis.set_major_formatter(mdates.DateFormatter('%m/%d--%H:%M'))\n",
    "        ax[1].xaxis.set_minor_locator(mdates.HourLocator(byhour=[6, 12, 18]))\n",
    "        ax[1].xaxis.grid(True, which='minor') \n",
    "    else:\n",
    "        ax[0].xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
    "        ax[0].xaxis.set_major_formatter(mdates.DateFormatter('%m/%d'))\n",
    "        ax[1].xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
    "        ax[1].xaxis.set_major_formatter(mdates.DateFormatter('%m/%d'))\n",
    "        \n",
    "    return ax[0],ax[1]\n",
    "\n",
    "def compute_cwt(data, scales, dt=1./12, normalize = True, waveletname = 'mexh', label_data = 'CGM ' ):\n",
    "    times = data.index\n",
    "    N = len(data)\n",
    "    if waveletname in ['cmor', 'shan']:\n",
    "        waveletname += '1.5-1'\n",
    "      \n",
    "    if normalize:\n",
    "        data_detrended = (data - data.mean())/data.std()\n",
    "    else:\n",
    "        data_detrended = data #- data.mean()\n",
    "        var = np.var(data_detrended).iloc[0]\n",
    "    \n",
    "    [coefficients_mex, frequencies_mex] = pywt.cwt(data_detrended.values.squeeze(), scales, waveletname, dt)\n",
    "    [coefficients_morl, frequencies_morl] = pywt.cwt(data_detrended.values.squeeze(), 4*scales, 'morl', dt)\n",
    "    period_mex = 1. / frequencies_mex\n",
    "    period_morl = 1. / frequencies_morl\n",
    "    \n",
    "    fft = np.fft.fft(data_detrended)\n",
    "    fftfreqs = np.fft.fftfreq(N, dt)\n",
    "    fft_power = np.abs(fft) ** 2\n",
    "    scaleMatrix = np.ones([1, N]) * scales[:, None]\n",
    "\n",
    "    if normalize:\n",
    "        power_mex =  (abs(coefficients_mex)) ** 2 / scaleMatrix\n",
    "    else:\n",
    "        power_mex  = (abs(coefficients_mex)) ** 2 / (scaleMatrix) \n",
    "        \n",
    "    if normalize:\n",
    "        power_morl =  (abs(coefficients_morl)) ** 2 / scaleMatrix\n",
    "    else:\n",
    "        power_morl  = (abs(coefficients_morl)) ** 2 / (scaleMatrix) \n",
    "    \n",
    "    signed_log_power_mex = np.sign(coefficients_mex) * np.log2(power_mex)\n",
    "    signed_log_power_morl = np.sign(coefficients_morl) * np.log2(power_morl)\n",
    "    # Replace -inf with the minimum finite value\n",
    "    signed_log_power_mex = np.where(np.isfinite(signed_log_power_mex), signed_log_power_mex, np.min(signed_log_power_mex[np.isfinite(signed_log_power_mex)])/10)\n",
    "    signed_log_power_morl = np.where(np.isfinite(signed_log_power_morl), signed_log_power_morl, np.min(signed_log_power_morl[np.isfinite(signed_log_power_morl)])/10)\n",
    "\n",
    "    # ax[0], ax[1] = plot_cwt(data, power_mex,period_mex,power_morl, period_morl, waveletname = waveletname, label_data = label_data )\n",
    "    glbl_power = power_mex.mean(axis=1)\n",
    "    #ax[0],ax[1], \n",
    "    return coefficients_mex,power_mex,signed_log_power_mex,period_mex,coefficients_morl,power_morl,signed_log_power_morl,period_morl, fft, fftfreqs, fft_power, glbl_power\n",
    "\n",
    "def plot_loss_accuracy(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def remove_nan_rows(X_daily, X_individual, y_label):\n",
    "    # Find rows with NaNs in either feature vector\n",
    "    nan_mask_daily = np.any(np.isnan(X_daily), axis=(1, 2))\n",
    "    nan_mask_individual = np.any(np.isnan(X_individual), axis=(1, 2))\n",
    "    \n",
    "    nan_mask_combined = nan_mask_daily | nan_mask_individual    \n",
    "    # Keep only rows without NaNs\n",
    "    X_daily_clean = X_daily[~nan_mask_combined]\n",
    "    X_individual_clean = X_individual[~nan_mask_combined]\n",
    "    y_label_clean = y_label[~nan_mask_combined]\n",
    "    return X_daily_clean, X_individual_clean, y_label_clean\n",
    "\n",
    "def apply_smote_resampling(X_matrices, X_features_daily, X_features_individual, y, matrix_shape0=288):\n",
    "    smote = SMOTE(random_state=42)\n",
    "\n",
    "    # Combine all inputs into a single 2D array for SMOTE\n",
    "    X_combined = np.hstack([\n",
    "        X_matrices.reshape(-1, matrix_shape0 * 288*3),\n",
    "        X_features_daily,\n",
    "        X_features_individual\n",
    "    ])\n",
    "\n",
    "    # Apply SMOTE to the combined array and the target\n",
    "    X_resampled_combined, y_resampled = smote.fit_resample(X_combined, y)\n",
    "\n",
    "    # Extract the resampled components\n",
    "    X_matrices_resampled = X_resampled_combined[:, :matrix_shape0 * 288*3].reshape(-1, matrix_shape0, 288, 3)\n",
    "    start = matrix_shape0 * 288*3\n",
    "    X_features_daily_resampled = X_resampled_combined[:, start:start + X_features_daily.shape[1]]\n",
    "    start += X_features_daily.shape[1]\n",
    "    X_features_individual_resampled = X_resampled_combined[:, start:]\n",
    "    \n",
    "    return (X_matrices_resampled, \n",
    "            X_features_daily_resampled, \n",
    "            X_features_individual_resampled, \n",
    "            y_resampled)\n",
    "def apply_smote_resampling_mask(X_matrices, X_features_daily, X_features_individual, mask_daily, mask_individual, y, matrix_shape0=288):\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_combined = np.hstack([\n",
    "        X_matrices.reshape(-1, matrix_shape0 * 288*3),\n",
    "        X_features_daily,\n",
    "        X_features_individual,\n",
    "        mask_daily,\n",
    "        mask_individual\n",
    "    ])\n",
    "    X_resampled_combined, y_resampled = smote.fit_resample(X_combined, y)\n",
    "\n",
    "    X_matrices_resampled = X_resampled_combined[:, :matrix_shape0 * 288*3].reshape(-1, matrix_shape0, 288, 3)\n",
    "    start = matrix_shape0 * 288*3\n",
    "    X_features_daily_resampled = X_resampled_combined[:, start:start + X_features_daily.shape[1]]\n",
    "    start += X_features_daily.shape[1]\n",
    "    X_features_individual_resampled = X_resampled_combined[:, start:start + X_features_individual.shape[1]]\n",
    "    start += X_features_individual.shape[1]\n",
    "    mask_daily_resampled = X_resampled_combined[:, start:start + mask_daily.shape[1]]\n",
    "    start += mask_daily.shape[1]\n",
    "    mask_individual_resampled = X_resampled_combined[:, start:]\n",
    "\n",
    "    return (X_matrices_resampled, \n",
    "            X_features_daily_resampled, \n",
    "            X_features_individual_resampled, \n",
    "            mask_daily_resampled, \n",
    "            mask_individual_resampled, \n",
    "            y_resampled)\n",
    "\n",
    "def apply_smote_resampling_mask_2d(X_matrices, X_features_daily, X_features_individual, mask_daily, mask_individual, y, matrix_shape0=288):\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_combined = np.hstack([\n",
    "        X_matrices.reshape(-1, matrix_shape0 * 288*2),\n",
    "        X_features_daily,\n",
    "        X_features_individual,\n",
    "        mask_daily,\n",
    "        mask_individual\n",
    "    ])\n",
    "    X_resampled_combined, y_resampled = smote.fit_resample(X_combined, y)\n",
    "\n",
    "    X_matrices_resampled = X_resampled_combined[:, :matrix_shape0 * 288*2].reshape(-1, matrix_shape0, 288, 2)\n",
    "    start = matrix_shape0 * 288*2\n",
    "    X_features_daily_resampled = X_resampled_combined[:, start:start + X_features_daily.shape[1]]\n",
    "    start += X_features_daily.shape[1]\n",
    "    X_features_individual_resampled = X_resampled_combined[:, start:start + X_features_individual.shape[1]]\n",
    "    start += X_features_individual.shape[1]\n",
    "    mask_daily_resampled = X_resampled_combined[:, start:start + mask_daily.shape[1]]\n",
    "    start += mask_daily.shape[1]\n",
    "    mask_individual_resampled = X_resampled_combined[:, start:]\n",
    "\n",
    "    return (X_matrices_resampled, \n",
    "            X_features_daily_resampled, \n",
    "            X_features_individual_resampled, \n",
    "            mask_daily_resampled, \n",
    "            mask_individual_resampled, \n",
    "            y_resampled)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e14dfa0-e3cc-4f75-a914-1181e37a8687",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "participant id:  1\n",
      "\n",
      "participant id:  4\n",
      "\n",
      "participant id:  11\n",
      "\n",
      "participant id:  14\n",
      "\n",
      "participant id:  18\n",
      "\n",
      "participant id:  24\n",
      "\n",
      "participant id:  25\n",
      "\n",
      "participant id:  29\n",
      "\n",
      "participant id:  32\n",
      "\n",
      "participant id:  34\n",
      "\n",
      "participant id:  37\n",
      "\n",
      "participant id:  43\n",
      "\n",
      "participant id:  48\n",
      "\n",
      "participant id:  50\n",
      "\n",
      "participant id:  54\n",
      "\n",
      "participant id:  58\n",
      "\n",
      "participant id:  60\n",
      "\n",
      "participant id:  63\n",
      "\n",
      "participant id:  66\n",
      "\n",
      "participant id:  67\n",
      "\n",
      "participant id:  69\n",
      "\n",
      "participant id:  71\n",
      "\n",
      "participant id:  77\n",
      "\n",
      "participant id:  78\n",
      "\n",
      "participant id:  80\n",
      "\n",
      "participant id:  84\n",
      "\n",
      "participant id:  85\n",
      "\n",
      "participant id:  95\n",
      "\n",
      "participant id:  97\n",
      "\n",
      "participant id:  103\n",
      "\n",
      "participant id:  104\n",
      "\n",
      "participant id:  107\n",
      "\n",
      "participant id:  110\n",
      "\n",
      "participant id:  114\n",
      "\n",
      "participant id:  115\n",
      "\n",
      "participant id:  127\n",
      "\n",
      "participant id:  137\n",
      "\n",
      "participant id:  143\n",
      "\n",
      "participant id:  144\n",
      "\n",
      "participant id:  145\n",
      "\n",
      "participant id:  152\n",
      "\n",
      "participant id:  159\n",
      "\n",
      "participant id:  163\n",
      "\n",
      "participant id:  166\n",
      "\n",
      "participant id:  167\n",
      "\n",
      "participant id:  173\n",
      "\n",
      "participant id:  174\n",
      "\n",
      "participant id:  183\n",
      "\n",
      "participant id:  187\n",
      "\n",
      "participant id:  191\n",
      "\n",
      "participant id:  194\n",
      "\n",
      "participant id:  202\n",
      "\n",
      "participant id:  214\n",
      "\n",
      "participant id:  217\n",
      "\n",
      "participant id:  219\n",
      "\n",
      "participant id:  221\n",
      "\n",
      "participant id:  223\n",
      "\n",
      "participant id:  226\n",
      "\n",
      "participant id:  227\n",
      "\n",
      "participant id:  235\n",
      "\n",
      "participant id:  248\n",
      "\n",
      "participant id:  252\n",
      "\n",
      "participant id:  253\n",
      "\n",
      "participant id:  254\n",
      "\n",
      "participant id:  255\n",
      "\n",
      "participant id:  256\n",
      "\n",
      "participant id:  261\n",
      "\n",
      "participant id:  263\n",
      "\n",
      "participant id:  267\n",
      "\n",
      "participant id:  270\n",
      "\n",
      "participant id:  284\n",
      "\n",
      "participant id:  288\n",
      "\n",
      "participant id:  290\n",
      "\n",
      "participant id:  295\n",
      "\n",
      "participant id:  301\n",
      "\n",
      "participant id:  304\n",
      "\n",
      "participant id:  306\n",
      "\n",
      "participant id:  308\n",
      "\n",
      "participant id:  312\n",
      "\n",
      "participant id:  313\n",
      "\n",
      "participant id:  316\n",
      "\n",
      "participant id:  317\n",
      "\n",
      "participant id:  320\n",
      "\n",
      "participant id:  334\n",
      "\n",
      "participant id:  336\n",
      "\n",
      "participant id:  341\n",
      "\n",
      "participant id:  349\n",
      "\n",
      "participant id:  352\n",
      "\n",
      "participant id:  354\n",
      "\n",
      "participant id:  355\n",
      "\n",
      "participant id:  364\n",
      "\n",
      "participant id:  367\n",
      "\n",
      "participant id:  372\n",
      "\n",
      "participant id:  374\n",
      "\n",
      "participant id:  378\n",
      "\n",
      "participant id:  380\n",
      "\n",
      "participant id:  384\n",
      "\n",
      "participant id:  385\n",
      "\n",
      "participant id:  386\n",
      "\n",
      "participant id:  389\n",
      "\n",
      "participant id:  390\n",
      "\n",
      "participant id:  393\n",
      "\n",
      "participant id:  398\n",
      "\n",
      "participant id:  402\n",
      "\n",
      "participant id:  404\n",
      "\n",
      "participant id:  406\n",
      "\n",
      "participant id:  410\n",
      "\n",
      "participant id:  414\n",
      "\n",
      "participant id:  419\n",
      "\n",
      "participant id:  421\n",
      "\n",
      "participant id:  422\n",
      "\n",
      "participant id:  423\n",
      "\n",
      "participant id:  425\n",
      "\n",
      "participant id:  428\n",
      "\n",
      "participant id:  431\n",
      "\n",
      "participant id:  433\n",
      "\n",
      "participant id:  434\n",
      "\n",
      "participant id:  435\n",
      "\n",
      "participant id:  436\n",
      "\n",
      "participant id:  438\n",
      "\n",
      "participant id:  446\n",
      "\n",
      "participant id:  453\n",
      "\n",
      "participant id:  455\n",
      "\n",
      "participant id:  463\n",
      "\n",
      "participant id:  467\n",
      "\n",
      "participant id:  468\n",
      "\n",
      "participant id:  469\n",
      "\n",
      "participant id:  471\n",
      "\n",
      "participant id:  475\n",
      "\n",
      "participant id:  477\n",
      "\n",
      "participant id:  479\n",
      "\n",
      "participant id:  482\n",
      "\n",
      "participant id:  483\n",
      "\n",
      "participant id:  484\n",
      "\n",
      "participant id:  486\n",
      "\n",
      "participant id:  491\n",
      "\n",
      "participant id:  498\n",
      "\n",
      "participant id:  499\n",
      "\n",
      "participant id:  504\n",
      "\n",
      "participant id:  506\n",
      "\n",
      "participant id:  507\n",
      "\n",
      "participant id:  509\n",
      "\n",
      "participant id:  511\n",
      "\n",
      "participant id:  515\n",
      "\n",
      "participant id:  521\n",
      "\n",
      "participant id:  528\n",
      "\n",
      "participant id:  531\n",
      "\n",
      "participant id:  532\n",
      "\n",
      "participant id:  539\n",
      "\n",
      "participant id:  545\n",
      "\n",
      "participant id:  548\n",
      "\n",
      "participant id:  549\n",
      "\n",
      "participant id:  550\n",
      "\n",
      "participant id:  556\n",
      "\n",
      "participant id:  558\n",
      "\n",
      "participant id:  564\n",
      "\n",
      "participant id:  567\n",
      "\n",
      "participant id:  573\n",
      "\n",
      "participant id:  576\n",
      "\n",
      "participant id:  577\n",
      "\n",
      "participant id:  583\n",
      "\n",
      "participant id:  591\n",
      "\n",
      "participant id:  598\n",
      "\n",
      "participant id:  602\n",
      "\n",
      "participant id:  603\n",
      "\n",
      "participant id:  606\n",
      "\n",
      "participant id:  608\n",
      "\n",
      "participant id:  610\n",
      "\n",
      "participant id:  612\n",
      "\n",
      "participant id:  614\n",
      "\n",
      "participant id:  616\n",
      "\n",
      "participant id:  620\n",
      "\n",
      "participant id:  629\n",
      "\n",
      "participant id:  646\n",
      "\n",
      "participant id:  648\n",
      "\n",
      "participant id:  652\n",
      "\n",
      "participant id:  653\n",
      "\n",
      "participant id:  654\n",
      "\n",
      "participant id:  655\n",
      "\n",
      "participant id:  662\n",
      "\n",
      "participant id:  666\n",
      "\n",
      "participant id:  667\n",
      "\n",
      "participant id:  670\n",
      "\n",
      "participant id:  671\n",
      "\n",
      "participant id:  674\n",
      "\n",
      "participant id:  676\n",
      "\n",
      "participant id:  677\n",
      "\n",
      "participant id:  678\n",
      "\n",
      "participant id:  683\n",
      "\n",
      "participant id:  692\n",
      "\n",
      "participant id:  695\n",
      "\n",
      "participant id:  702\n",
      "\n",
      "participant id:  704\n",
      "\n",
      "participant id:  707\n",
      "\n",
      "participant id:  715\n",
      "\n",
      "participant id:  716\n",
      "\n",
      "participant id:  723\n",
      "\n",
      "participant id:  729\n",
      "\n",
      "participant id:  733\n",
      "\n",
      "participant id:  734\n",
      "\n",
      "participant id:  735\n",
      "\n",
      "participant id:  740\n",
      "\n",
      "participant id:  741\n",
      "\n",
      "participant id:  742\n",
      "\n",
      "participant id:  748\n",
      "\n",
      "participant id:  749\n",
      "\n",
      "participant id:  756\n",
      "\n",
      "participant id:  760\n",
      "\n",
      "participant id:  761\n",
      "\n",
      "participant id:  762\n",
      "\n",
      "participant id:  766\n",
      "\n",
      "participant id:  767\n",
      "\n",
      "participant id:  769\n",
      "\n",
      "participant id:  771\n",
      "\n",
      "participant id:  773\n",
      "\n",
      "participant id:  775\n",
      "\n",
      "participant id:  781\n",
      "\n",
      "participant id:  791\n",
      "\n",
      "participant id:  796\n",
      "\n",
      "participant id:  811\n",
      "\n",
      "participant id:  812\n",
      "\n",
      "participant id:  816\n",
      "\n",
      "participant id:  824\n",
      "\n",
      "participant id:  829\n",
      "\n",
      "participant id:  830\n",
      "\n",
      "participant id:  831\n",
      "\n",
      "participant id:  834\n",
      "\n",
      "participant id:  838\n",
      "\n",
      "participant id:  839\n",
      "\n",
      "participant id:  843\n",
      "\n",
      "participant id:  852\n",
      "\n",
      "participant id:  854\n",
      "\n",
      "participant id:  856\n",
      "\n",
      "participant id:  862\n",
      "\n",
      "participant id:  870\n",
      "\n",
      "participant id:  873\n",
      "\n",
      "participant id:  886\n",
      "\n",
      "participant id:  888\n",
      "\n",
      "participant id:  889\n",
      "\n",
      "participant id:  890\n",
      "\n",
      "participant id:  893\n",
      "\n",
      "participant id:  894\n",
      "\n",
      "participant id:  896\n",
      "\n",
      "participant id:  897\n",
      "\n",
      "participant id:  900\n",
      "\n",
      "participant id:  905\n",
      "\n",
      "participant id:  907\n",
      "\n",
      "participant id:  911\n",
      "\n",
      "participant id:  920\n",
      "\n",
      "participant id:  930\n",
      "\n",
      "participant id:  932\n",
      "\n",
      "participant id:  933\n",
      "\n",
      "participant id:  943\n",
      "\n",
      "participant id:  945\n",
      "\n",
      "participant id:  946\n",
      "\n",
      "participant id:  948\n",
      "\n",
      "participant id:  953\n",
      "\n",
      "participant id:  956\n",
      "\n",
      "participant id:  958\n",
      "\n",
      "participant id:  963\n",
      "\n",
      "participant id:  965\n",
      "\n",
      "participant id:  966\n",
      "\n",
      "participant id:  970\n",
      "\n",
      "participant id:  971\n",
      "\n",
      "participant id:  974\n",
      "\n",
      "participant id:  976\n",
      "\n",
      "participant id:  979\n",
      "\n",
      "participant id:  981\n",
      "\n",
      "participant id:  985\n",
      "\n",
      "participant id:  987\n",
      "\n",
      "participant id:  988\n",
      "\n",
      "participant id:  1000\n",
      "\n",
      "participant id:  1004\n",
      "\n",
      "participant id:  1010\n",
      "\n",
      "participant id:  1012\n",
      "\n",
      "participant id:  1013\n",
      "\n",
      "participant id:  1014\n",
      "\n",
      "participant id:  1015\n",
      "\n",
      "participant id:  1016\n",
      "\n",
      "participant id:  1020\n",
      "\n",
      "participant id:  1021\n",
      "\n",
      "participant id:  1022\n",
      "\n",
      "participant id:  1024\n",
      "\n",
      "participant id:  1033\n",
      "\n",
      "participant id:  1039\n",
      "\n",
      "participant id:  1043\n",
      "\n",
      "participant id:  1046\n",
      "\n",
      "participant id:  1050\n",
      "\n",
      "participant id:  1051\n",
      "\n",
      "participant id:  1067\n",
      "\n",
      "participant id:  1072\n",
      "\n",
      "participant id:  1076\n",
      "\n",
      "participant id:  1077\n",
      "\n",
      "participant id:  1082\n",
      "\n",
      "participant id:  1090\n",
      "\n",
      "participant id:  1100\n",
      "\n",
      "participant id:  1103\n",
      "\n",
      "participant id:  1107\n",
      "\n",
      "participant id:  1110\n",
      "\n",
      "participant id:  1112\n",
      "\n",
      "participant id:  1115\n",
      "\n",
      "participant id:  1119\n",
      "\n",
      "participant id:  1120\n",
      "\n",
      "participant id:  1121\n",
      "\n",
      "participant id:  1123\n",
      "\n",
      "participant id:  1127\n",
      "\n",
      "participant id:  1138\n",
      "\n",
      "participant id:  1139\n",
      "\n",
      "participant id:  1141\n",
      "\n",
      "participant id:  1142\n",
      "\n",
      "participant id:  1143\n",
      "\n",
      "participant id:  1146\n",
      "\n",
      "participant id:  1149\n",
      "\n",
      "participant id:  1152\n",
      "\n",
      "participant id:  1155\n",
      "\n",
      "participant id:  1156\n",
      "\n",
      "participant id:  1157\n",
      "\n",
      "participant id:  1158\n",
      "\n",
      "participant id:  1161\n",
      "\n",
      "participant id:  1163\n",
      "\n",
      "participant id:  1170\n",
      "\n",
      "participant id:  1171\n",
      "\n",
      "participant id:  1185\n",
      "\n",
      "participant id:  1193\n",
      "\n",
      "participant id:  1194\n",
      "\n",
      "participant id:  1195\n",
      "\n",
      "participant id:  1199\n",
      "\n",
      "participant id:  1201\n",
      "\n",
      "participant id:  1202\n",
      "\n",
      "participant id:  1203\n",
      "\n",
      "participant id:  1205\n",
      "\n",
      "participant id:  1206\n",
      "\n",
      "participant id:  1210\n",
      "\n",
      "participant id:  1211\n",
      "\n",
      "participant id:  1219\n",
      "\n",
      "participant id:  1221\n",
      "\n",
      "participant id:  1224\n",
      "\n",
      "participant id:  1230\n",
      "\n",
      "participant id:  1239\n",
      "\n",
      "participant id:  1243\n",
      "\n",
      "participant id:  1247\n",
      "\n",
      "participant id:  1248\n",
      "\n",
      "participant id:  1249\n",
      "\n",
      "participant id:  1250\n",
      "\n",
      "participant id:  1253\n",
      "\n",
      "participant id:  1255\n",
      "\n",
      "participant id:  1257\n",
      "\n",
      "participant id:  1261\n",
      "\n",
      "participant id:  1265\n",
      "\n",
      "participant id:  1266\n",
      "\n",
      "participant id:  1271\n",
      "\n",
      "participant id:  1273\n",
      "\n",
      "participant id:  1281\n",
      "\n",
      "participant id:  1283\n",
      "\n",
      "participant id:  1286\n",
      "\n",
      "participant id:  1287\n",
      "\n",
      "participant id:  1291\n",
      "\n",
      "participant id:  1292\n",
      "\n",
      "participant id:  1297\n",
      "\n",
      "participant id:  1302\n",
      "\n",
      "participant id:  1303\n",
      "\n",
      "participant id:  1304\n",
      "\n",
      "participant id:  1305\n",
      "\n",
      "participant id:  1307\n",
      "\n",
      "participant id:  1311\n",
      "\n",
      "participant id:  1312\n",
      "\n",
      "participant id:  1322\n",
      "\n",
      "participant id:  1323\n",
      "\n",
      "participant id:  1325\n",
      "\n",
      "participant id:  1327\n",
      "\n",
      "participant id:  1328\n",
      "\n",
      "participant id:  1329\n",
      "\n",
      "participant id:  1330\n",
      "\n",
      "participant id:  1335\n",
      "\n",
      "participant id:  1336\n",
      "\n",
      "participant id:  1338\n",
      "\n",
      "participant id:  1339\n",
      "\n",
      "participant id:  1343\n",
      "\n",
      "participant id:  1344\n",
      "\n",
      "participant id:  1345\n",
      "\n",
      "participant id:  1348\n",
      "\n",
      "participant id:  1351\n",
      "\n",
      "participant id:  1354\n",
      "\n",
      "participant id:  1357\n",
      "\n",
      "participant id:  1359\n",
      "\n",
      "participant id:  1361\n",
      "\n",
      "participant id:  1362\n",
      "\n",
      "participant id:  1363\n",
      "\n",
      "participant id:  1365\n",
      "\n",
      "participant id:  1369\n",
      "\n",
      "participant id:  1377\n",
      "\n",
      "participant id:  1378\n",
      "\n",
      "participant id:  1381\n",
      "\n",
      "participant id:  1385\n",
      "\n",
      "participant id:  1386\n",
      "\n",
      "participant id:  1387\n",
      "\n",
      "participant id:  1396\n",
      "\n",
      "participant id:  1400\n",
      "\n",
      "participant id:  1406\n",
      "\n",
      "participant id:  1408\n",
      "\n",
      "participant id:  1410\n",
      "\n",
      "participant id:  1412\n",
      "\n",
      "participant id:  1416\n",
      "\n",
      "participant id:  1418\n",
      "\n",
      "participant id:  1422\n",
      "\n",
      "participant id:  1425\n",
      "\n",
      "participant id:  1426\n",
      "\n",
      "participant id:  1427\n",
      "\n",
      "participant id:  1433\n",
      "\n",
      "participant id:  1434\n",
      "\n",
      "participant id:  1435\n",
      "\n",
      "participant id:  1438\n",
      "\n",
      "participant id:  1439\n",
      "\n",
      "participant id:  1441\n",
      "\n",
      "participant id:  1444\n",
      "\n",
      "participant id:  1451\n",
      "\n",
      "participant id:  1453\n",
      "\n",
      "participant id:  1456\n",
      "\n",
      "participant id:  1457\n",
      "\n",
      "participant id:  1459\n",
      "\n",
      "participant id:  1462\n",
      "\n",
      "participant id:  1464\n",
      "\n",
      "participant id:  1476\n",
      "\n",
      "participant id:  1484\n",
      "\n",
      "participant id:  1490\n",
      "\n",
      "participant id:  1493\n",
      "\n",
      "participant id:  1494\n",
      "\n",
      "participant id:  1495\n",
      "\n",
      "participant id:  1497\n",
      "\n",
      "participant id:  1500\n",
      "\n",
      "participant id:  1501\n",
      "\n",
      "participant id:  1503\n",
      "\n",
      "participant id:  1507\n",
      "\n",
      "participant id:  1509\n",
      "\n",
      "participant id:  1517\n",
      "\n",
      "participant id:  1520\n",
      "\n",
      "participant id:  1528\n",
      "\n",
      "participant id:  1535\n",
      "\n",
      "participant id:  1536\n",
      "\n",
      "participant id:  1540\n",
      "\n",
      "participant id:  1542\n",
      "\n",
      "participant id:  1543\n",
      "\n",
      "participant id:  1550\n",
      "\n",
      "participant id:  1552\n",
      "\n",
      "participant id:  1554\n",
      "\n",
      "participant id:  1558\n",
      "\n",
      "participant id:  1559\n",
      "\n",
      "participant id:  1566\n",
      "\n",
      "participant id:  1567\n",
      "\n",
      "participant id:  1568\n",
      "\n",
      "participant id:  1580\n",
      "\n",
      "participant id:  1586\n",
      "\n",
      "participant id:  1596\n",
      "\n",
      "participant id:  1602\n",
      "\n",
      "participant id:  1603\n",
      "\n",
      "participant id:  1604\n",
      "\n",
      "participant id:  1611\n",
      "\n",
      "participant id:  1615\n",
      "\n",
      "participant id:  1616\n",
      "\n",
      "participant id:  1617\n",
      "\n",
      "participant id:  1621\n",
      "\n",
      "participant id:  1622\n",
      "\n",
      "participant id:  1625\n",
      "\n",
      "participant id:  1632\n",
      "\n",
      "participant id:  1634\n",
      "\n",
      "participant id:  1635\n",
      "\n",
      "participant id:  1636\n",
      "\n",
      "participant id:  1638\n",
      "\n",
      "participant id:  1644\n",
      "\n",
      "participant id:  1647\n",
      "\n",
      "participant id:  1649\n",
      "\n",
      "participant id:  1650\n",
      "\n",
      "participant id:  1656\n",
      "\n",
      "participant id:  1658\n",
      "\n",
      "participant id:  1660\n",
      "\n",
      "participant id:  1666\n",
      "\n",
      "participant id:  1668\n",
      "\n",
      "participant id:  1669\n",
      "\n",
      "participant id:  1673\n",
      "\n",
      "participant id:  1683\n",
      "\n",
      "participant id:  1689\n",
      "\n",
      "participant id:  1695\n",
      "\n",
      "participant id:  1696\n",
      "\n",
      "participant id:  1698\n",
      "\n",
      "participant id:  1704\n",
      "\n",
      "participant id:  1711\n",
      "\n",
      "participant id:  1712\n",
      "\n",
      "participant id:  1714\n",
      "\n",
      "participant id:  1717\n",
      "\n",
      "participant id:  1718\n",
      "\n",
      "participant id:  1719\n",
      "\n",
      "participant id:  1722\n",
      "\n",
      "participant id:  1726\n"
     ]
    }
   ],
   "source": [
    "data_dict = defaultdict(lambda: defaultdict(dict))\n",
    "\n",
    "for participant_id in shared_participants: \n",
    "    participant_insulin_data = T1DEXI_basal[T1DEXI_basal['USUBJID'] == participant_id].reset_index()     \n",
    "    participant_cgm_data = T1DEXI_CGM_Dataset[T1DEXI_CGM_Dataset['USUBJID'] == participant_id].reset_index()\n",
    "    # participant_sleep_data = T1DEXI_sleep_Dataset[T1DEXI_sleep_Dataset['USUBJID'] == participant_id].reset_index()\n",
    "    participant_hr_data = T1DEXI_VS_Dataset[T1DEXI_VS_Dataset['USUBJID'] == participant_id].reset_index()\n",
    "\n",
    "    participant_Reqcue_Carbs = T1DEXI_Reqcue_Carbs_Dataset[T1DEXI_Reqcue_Carbs_Dataset['USUBJID'] == participant_id][T1DEXI_Reqcue_Carbs_Dataset['MLCAT']=='RESCUE CARBS']\n",
    "    participant_carbs_self = FAMLPM_Dataset[FAMLPM_Dataset['USUBJID']==participant_id][FAMLPM_Dataset['FATESTCD']=='DCARBT'][FAMLPM_Dataset['FACAT']=='CONSUMED'][FAMLPM_Dataset['FAMETHOD']=='SELF-REPORT'].reset_index()\n",
    "    participant_carbs_RFPM = FAMLPM_Dataset[FAMLPM_Dataset['USUBJID']==participant_id][FAMLPM_Dataset['FATESTCD']=='DCARBT'][FAMLPM_Dataset['FACAT']=='CONSUMED'][FAMLPM_Dataset['FAMETHOD']=='RFPM'].reset_index()\n",
    "\n",
    "    merged_carbs = pd.merge(participant_carbs_self[['FAORRES','FADTC']], participant_carbs_RFPM[['FAORRES','FADTC']], on='FADTC', how='left', suffixes=('_self', '_RFPM'))\n",
    "    merged_carbs['corrected_value'] = (merged_carbs['FAORRES_self'] + merged_carbs['FAORRES_RFPM']) / 2\n",
    "    participant_carbs_self['corrected_value'] = merged_carbs['corrected_value'].fillna(participant_carbs_self['FAORRES'])        \n",
    "    participant_carbs_self = participant_carbs_self.dropna(subset=['corrected_value']).reset_index()\n",
    "    participant_rescue_carbs_renamed = participant_Reqcue_Carbs.rename(columns={\n",
    "        'MLDTC': 'FADTC',\n",
    "        'MLDOSE': 'corrected_value',\n",
    "        'MLCAT': 'FAOBJ'\n",
    "    }).dropna(subset=['corrected_value']).reset_index()\n",
    "    participant_carbs = pd.concat([participant_carbs_self[['FADTC', 'corrected_value', 'FAOBJ']], participant_rescue_carbs_renamed[['FADTC', 'corrected_value', 'FAOBJ']]], ignore_index=True).sort_values(by='FADTC')\n",
    "    if participant_hr_data.empty:\n",
    "        continue\n",
    "    a1c_data = participant_cgm_data[participant_cgm_data['LBTEST'] == \"Hemoglobin A1c\"]\n",
    "    a1c = a1c_data['LBSTRESN'].iloc[0]\n",
    "    height_data = participant_hr_data[participant_hr_data['VSTESTCD'] == \"HEIGHT\"]\n",
    "    height = height_data['VSSTRESN'].iloc[0]\n",
    "    weight_data = participant_hr_data[participant_hr_data['VSTESTCD'] == \"WEIGHT\"]\n",
    "    weight = weight_data['VSSTRESN'].iloc[0]\n",
    "    print(\"\\nparticipant id: \", participant_id)\n",
    "\n",
    "    # --- Separate data types ---\n",
    "    participant_basal_data = participant_insulin_data[participant_insulin_data['FACAT'] == \"BASAL\"][participant_insulin_data['FATESTCD'] == \"INSBASAL\"].dropna(subset=['FASTRESN']).reset_index() #BASFLRT  #INSBASAL\n",
    "    participant_bolus_data = participant_insulin_data[participant_insulin_data['FACAT'] == \"BOLUS\"][participant_insulin_data['FATESTCD'] == \"INSBOLUS\"].dropna(subset=['FASTRESN']).reset_index() \n",
    "    participant_cgm_data = participant_cgm_data[participant_cgm_data['LBTEST'] == \"Glucose\"].dropna(subset=['LBSTRESN']).reset_index()\n",
    "    participant_hr_data = participant_hr_data[participant_hr_data['VSTESTCD']=='HRM'].dropna(subset=['VSSTRESN']).reset_index()\n",
    "\n",
    "    # --- Align data to 5-minute intervals ---    \n",
    "    aligned_cgm = resample_avg(participant_cgm_data['LBSTRESN'], participant_cgm_data['LBDTC'], interval='5T')\n",
    "    # --- Interpolate max 3 hour gaps ---    \n",
    "    method = 'linear'  \n",
    "    interpolated_cgm = aligned_cgm.interpolate(method=method, order=2,limit_direction='both')\n",
    "    \n",
    "    smoothed_cgm = smooth_signal(interpolated_cgm, window_size=17, treshold=10, order=2)\n",
    "    interpolated_cgm = smoothed_cgm.interpolate(method=method, order=2,limit_direction='both')\n",
    "  \n",
    "    # Total Participant Stats\n",
    "    total_cgm_stats = get_total_cgm_stats(aligned_cgm)\n",
    "    total_bolus_stats = total_stats(participant_bolus_data['FASTRESN'], participant_bolus_data['FADTC'], 'bolus')\n",
    "    total_basal_stats = total_stats(participant_basal_data['FASTRESN'], participant_basal_data['FADTC'], 'basal')\n",
    "    total_hr_stats = total_stats(participant_hr_data['VSSTRESN'], participant_hr_data['VSDTC'], 'hr')\n",
    "\n",
    "# --- Compute & Plot overall CWT ---    \n",
    "    dt_cgm = 1./12 \n",
    "    s0 = 1./2  #starting period is 15 minutes\n",
    "    dj = 1/8  # Resolusion\n",
    "    J = int(12 / dj)\n",
    "    #scales = s0 * 2 ** (dj * np.arange(0, J + 1))\n",
    "    scales = s0 * np.arange(1,576,2)\n",
    "\n",
    "    # fig, ax = plt.subplots(2, 1, figsize=(20, 10), sharex=True)  \n",
    "    #ax[0], ax[1],\n",
    "    coefficients_mex,power_mex,signed_log_power_mex,period_mex,coefficients_morl,power_morl,signed_log_power_morl,period_morl, fft, fftfreqs, fft_power_cgm, glbl_power_cgm = compute_cwt(interpolated_cgm, scales, dt=dt_cgm , normalize = False, waveletname = 'mexh', label_data = \"CGM ID#\" +str(participant_id) )\n",
    " \n",
    "    # --- Find overlapping dates ---\n",
    "    unique_dates = sorted(set(interpolated_cgm.index.date)) #& set(participant_carbs['FADTC'].dt.date) & set(participant_bolus_data['FADTC'].dt.date) & set(participant_sleep_data['NVENDTC'].dt.date) & set(participant_basal_data['FADTC'].dt.date) & set(participant_hr_data['VSDTC'].dt.date)\n",
    "    unique_dates = unique_dates[1:-1] # Skip the first & last days\n",
    "    for current_date in unique_dates:\n",
    "        # --- Get current day data (for feature extraction) ---\n",
    "        current_day_mask_cgm = interpolated_cgm.index.date == current_date\n",
    "        current_day_indices_cgm = np.where(current_day_mask_cgm)[0]\n",
    "        \n",
    "        current_day_basal = participant_basal_data[participant_basal_data['FADTC'].dt.date == current_date]\n",
    "        current_day_bolus = participant_bolus_data[participant_bolus_data['FADTC'].dt.date == current_date]\n",
    "        current_day_mask_cgm = current_day_mask_cgm.astype(bool)\n",
    "        current_day_cgm = interpolated_cgm[current_day_mask_cgm]\n",
    "        current_day_cgm_original = aligned_cgm[aligned_cgm.index.date == current_date]\n",
    "        current_day_hr = participant_hr_data[participant_hr_data['VSDTC'].dt.date == current_date]\n",
    "        current_day_carbs = participant_carbs[participant_carbs['FADTC'].dt.date == current_date]\n",
    "        if current_day_cgm_original.notna().sum()[0]<252 or current_day_cgm_original[-24:].notna().sum()[0]<18:\n",
    "            continue \n",
    "\n",
    "        current_day_carbs_stats = corrected_daily_carbs_stats(current_day_carbs)   \n",
    "        current_day_cgm_stats = get_daily_cgm_stats(current_day_cgm)\n",
    "        current_day_cgm_stats_original = get_daily_cgm_stats(current_day_cgm_original)  \n",
    "        \n",
    "        current_day_bolus_stats = daily_stats_bolus(current_day_bolus['FASTRESN'], current_day_bolus['FADTC'], 'bolus_daily')\n",
    "        current_day_basal_stats = daily_stats(current_day_basal['FASTRESN'], current_day_basal['FADTC'], 'basal_daily')\n",
    "#         current_day_basal_stats = daily_stats(current_day_basal.values.squeeze(), current_day_basal.index, 'basal_daily')\n",
    "        current_day_hr_stats = daily_stats(current_day_hr['VSSTRESN'], current_day_hr['VSDTC'], 'hr_daily')\n",
    "        \n",
    "        next_date = current_date + pd.Timedelta(days=1) # Get one hour before \n",
    "        next_day_cgm = interpolated_cgm[interpolated_cgm.index.date == next_date]\n",
    "        next_day_cgm_original = aligned_cgm[aligned_cgm.index.date == next_date]\n",
    "        next_night = current_day_cgm.index[-1] + pd.Timedelta(hours=8) #More than 2hour missing in the next 6 hours\n",
    "        if np.sum(next_day_cgm_original.index <= next_night)<72:\n",
    "            continue \n",
    "        # next_day_8pm = next_day_cgm_original[next_day_cgm_original.index <= next_night]\n",
    "        # if next_day_8pm.isna().sum()[0]>24:\n",
    "        #     continue\n",
    "            \n",
    "        next_day_cgm_stats = cgm_prediction_stats(next_day_cgm)\n",
    "        next_day_cgm_labels = daily_cgm_labels(next_day_cgm)\n",
    "        next_day_cgm_stats_original = cgm_prediction_stats(next_day_cgm_original)\n",
    "        next_day_cgm_labels_original = daily_cgm_labels(next_day_cgm_original)\n",
    "        \n",
    "        # --- for Plotting 48 hrs, the current and previous day ---\n",
    "        start_time = current_day_cgm.index[0] \n",
    "        end_time = current_day_cgm.index[-1] + pd.Timedelta(days=1)\n",
    "\n",
    "        original_basal = participant_basal_data[(participant_basal_data['FADTC']>=start_time)&(participant_basal_data['FADTC']<=end_time)]\n",
    "        original_bolus = participant_bolus_data[(participant_bolus_data['FADTC']>=start_time)&(participant_bolus_data['FADTC']<=end_time)]\n",
    "        original_cgm = participant_cgm_data[(participant_cgm_data['LBDTC']>=start_time)&(participant_cgm_data['LBDTC']<=end_time)] \n",
    "        original_hr = participant_hr_data[(participant_hr_data['VSDTC']>=start_time)&(participant_hr_data['VSDTC']<=end_time)] \n",
    "        data_carbs = participant_carbs[(participant_carbs['FADTC']>= start_time) & (participant_carbs['FADTC']<=end_time)]\n",
    "#         data_sleep = participant_sleep_data_updated[(participant_sleep_data_updated['NVDTC']>= start_time) & (participant_sleep_data_updated['NVENDTC']<=end_time)]        \n",
    "        matrix_shape0 = len(scales)\n",
    "        matrix_shape1 = len(current_day_cgm)\n",
    "        daily_power_mex = power_mex[:, current_day_indices_cgm]\n",
    "        daily_signed_log_power_mex = signed_log_power_mex[:, current_day_indices_cgm]\n",
    "        daily_coeffs_mex = coefficients_mex[:, current_day_indices_cgm]\n",
    "        daily_signed_log_power_morl = signed_log_power_morl[:, current_day_indices_cgm]\n",
    "        daily_power_morl = power_morl[:, current_day_indices_cgm]\n",
    "        daily_coeffs_morl = coefficients_morl[:, current_day_indices_cgm]\n",
    "        \n",
    "        full_matrix1 = np.ndarray(shape=(matrix_shape0, matrix_shape1,3))\n",
    "        full_matrix1[:, :, 0] = daily_coeffs_mex\n",
    "        full_matrix1[:, :, 1] = daily_power_mex\n",
    "        full_matrix1[:, :, 2] = daily_power_morl\n",
    "        full_matrix2 = np.ndarray(shape=(matrix_shape0, matrix_shape1,3))\n",
    "        full_matrix2[:, :, 0] = daily_coeffs_morl\n",
    "        full_matrix2[:, :, 1] = daily_signed_log_power_morl\n",
    "        full_matrix2[:, :, 2] = daily_signed_log_power_mex\n",
    "        \n",
    "            # Store data in the dictionary\n",
    "        data_dict[participant_id][current_date] = {\n",
    "            'len_original_cgm': len(current_day_cgm_original),\n",
    "            'full_matrix_cgm_power': full_matrix1,\n",
    "            'full_matrix_cgm_coeffs': full_matrix2,\n",
    "            \n",
    "            'next_day_cgm_stats':next_day_cgm_stats,\n",
    "            'next_day_cgm_labels':next_day_cgm_labels,\n",
    "            'CGM_stats_daily': current_day_cgm_stats,\n",
    "            'next_day_cgm_stats_original':next_day_cgm_stats_original,\n",
    "            'next_day_cgm_labels_original':next_day_cgm_labels_original,\n",
    "            'CGM_stats_daily_original': current_day_cgm_stats_original,\n",
    "            \n",
    "            'Basal_stats_daily': current_day_basal_stats,\n",
    "            'Carbs_stats_daily': current_day_carbs_stats,\n",
    "#             'Sleep_stats_daily': current_day_sleep_stats[:1],\n",
    "            'HR_stats_daily': current_day_hr_stats,\n",
    "            \n",
    "            'A1c': a1c,\n",
    "            'Weight': weight,\n",
    "            'Height': height,\n",
    "            'CGM_stats_participant': total_cgm_stats,\n",
    "            'Basal_stats_participant': total_basal_stats,\n",
    "#             'Sleep_stats_participant': sleep_stats,\n",
    "            'HR_stats_participant': total_hr_stats\n",
    "        }\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc41a777-4d11-422f-9873-89250a5ee178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491 491\n"
     ]
    }
   ],
   "source": [
    "filtered_data_dict = {}\n",
    "for participant_id, days_data in data_dict.items():\n",
    "    filtered_days_data = []\n",
    "    for current_date, day_data in days_data.items():\n",
    "\n",
    "        if (day_data['len_original_cgm'] > 252 and np.sum(day_data['CGM_stats_daily_original'].isna()).sum()<1):\n",
    "            filtered_days_data.append(day_data)\n",
    "    \n",
    "    if filtered_days_data:\n",
    "        filtered_data_dict[participant_id] = filtered_days_data\n",
    "\n",
    "print(len(data_dict),len(filtered_data_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c2738e4-2631-4a9a-b6ee-697f819c11f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('/mnt/data1/ma98/data_generated/dict_3d_cgm_mex_morl_smooth_70_15min.dill', 'wb') as f:\n",
    "    dill.dump(data_dict, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4720e29-3f2e-441d-a19e-47f7eaac608c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491\n"
     ]
    }
   ],
   "source": [
    "with open('/mnt/data1/ma98/data_generated/dict_3d_cgm_mex_morl_smooth_70_15min.dill', 'rb') as f:\n",
    "    loaded_dict = dill.load(f)\n",
    "    \n",
    "data_dict = loaded_dict.copy()\n",
    "print(len(data_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20a5d135-1862-4381-a0fd-0b63fa5be97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/ma98/temp/dict_3d_cgm_mex_morl_smooth_70_15min.dill', 'wb') as f:\n",
    "    dill.dump(data_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c4f2f4-2b05-4c7f-b69f-4eed98fe525d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
