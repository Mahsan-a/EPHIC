{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276114e3-b357-4a3f-8c83-f95a444ef341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from scipy import stats\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.signal import find_peaks, detrend\n",
    "import pywt\n",
    "from numpy import arange, array, linspace, loadtxt, log2, logspace, mean, polyfit\n",
    "from numpy import zeros, pi, sin, cos, arctan2, sqrt, real, imag, conj, tile\n",
    "from numpy import round, interp, diff, unique, where\n",
    "from pandas import DataFrame, date_range\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib import pyplot\n",
    "from scipy.stats import pearsonr, mannwhitneyu, kruskal, norm, wasserstein_distance\n",
    "from sklearn.linear_model import LinearRegression, Ridge, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GroupKFold, GroupShuffleSplit\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score,f1_score, recall_score, precision_score,roc_auc_score, roc_curve, auc,accuracy_score,classification_report,confusion_matrix\n",
    "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier, GradientBoostingClassifier,AdaBoostClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.utils import shuffle\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.utils import Sequence\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Model, load_model\n",
    "from keras import layers\n",
    "# from tensorflow.keras.models import Model, load_model\n",
    "from keras.layers import GlobalAveragePooling2D,GlobalMaxPooling2D,Concatenate,Input,Dropout, Conv2D, MaxPooling2D, Flatten,Multiply,Attention,Dense,concatenate,Masking,BatchNormalization, Reshape\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.applications import DenseNet121\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, History\n",
    "history = History()\n",
    "\n",
    "import pickle\n",
    "import joblib\n",
    "import dill\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from scipy.signal import savgol_filter\n",
    "import gc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "import sys\n",
    "sys.stderr = open(os.devnull, 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595a451b-287d-4dd5-a8ea-37dc068fcf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab328cfa-c35b-44fd-bbf6-c7d26f77834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure GPUs to grow memory as needed\n",
    "def configure_gpu():\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "def plot_training_history(folds_val_loss, folds_val_auc, folds_val_acc, folder):\n",
    "    # Determine max_epochs as the minimum length of the folds minus 10\n",
    "    max_epochs = min(len(fold) for fold in folds_val_auc) - 10\n",
    "\n",
    "    # Initialize arrays to store average and standard deviation for each epoch\n",
    "    avg_loss = np.zeros(max_epochs)\n",
    "    std_loss = np.zeros(max_epochs)\n",
    "    avg_auc = np.zeros(max_epochs)\n",
    "    std_auc = np.zeros(max_epochs)\n",
    "    avg_acc = np.zeros(max_epochs)\n",
    "    std_acc = np.zeros(max_epochs)\n",
    "\n",
    "    # Compute average and standard deviation for each epoch\n",
    "    for epoch in range(max_epochs):\n",
    "        losses = []\n",
    "        aucs = []\n",
    "        accuracies = []\n",
    "        for fold_loss, fold_auc, fold_acc in zip(folds_val_loss, folds_val_auc, folds_val_acc):\n",
    "            if epoch < len(fold_loss):\n",
    "                losses.append(fold_loss[epoch])\n",
    "            if epoch < len(fold_auc):\n",
    "                aucs.append(fold_auc[epoch])\n",
    "            if epoch < len(fold_acc):\n",
    "                accuracies.append(fold_acc[epoch])\n",
    "        if losses:\n",
    "            avg_loss[epoch] = np.mean(losses)\n",
    "            std_loss[epoch] = np.std(losses)\n",
    "        if aucs:\n",
    "            avg_auc[epoch] = np.mean(aucs)\n",
    "            std_auc[epoch] = np.std(aucs)\n",
    "        if accuracies:\n",
    "            avg_acc[epoch] = np.mean(accuracies)\n",
    "            std_acc[epoch] = np.std(accuracies)\n",
    "\n",
    "    # Adjusted x-axis range: 1 to 3*max_epochs + 1 with step size of 3\n",
    "    epochs_range = range(1, 3 * max_epochs + 1, 3)\n",
    "\n",
    "    # Plot average loss, AUC, and accuracy\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Loss plot\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(epochs_range, avg_loss, color='blue', label='Validation Loss')\n",
    "    plt.fill_between(epochs_range, \n",
    "                     avg_loss - std_loss,\n",
    "                     avg_loss + std_loss,\n",
    "                     color='blue', alpha=0.2)\n",
    "    plt.xlabel('Epochs', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.title('Validation Loss per Epoch', fontsize=14)\n",
    "    plt.legend(fontsize=10)\n",
    "\n",
    "    # AUC plot\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(epochs_range, avg_auc, color='blue', label='Validation AUC')\n",
    "    plt.fill_between(epochs_range, \n",
    "                     avg_auc - std_auc,\n",
    "                     avg_auc + std_auc,\n",
    "                     color='blue', alpha=0.2)\n",
    "    plt.xlabel('Epochs', fontsize=12)\n",
    "    plt.ylabel('AUC', fontsize=12)\n",
    "    plt.title('Validation AUC per Epoch', fontsize=14)\n",
    "    plt.legend(fontsize=10)\n",
    "\n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(epochs_range, avg_acc, color='blue', label='Validation Accuracy')\n",
    "    plt.fill_between(epochs_range, \n",
    "                     avg_acc - std_acc,\n",
    "                     avg_acc + std_acc,\n",
    "                     color='blue', alpha=0.2)\n",
    "    plt.xlabel('Epochs', fontsize=12)\n",
    "    plt.ylabel('Accuracy', fontsize=12)\n",
    "    plt.title('Validation Accuracy per Epoch', fontsize=14)\n",
    "    plt.legend(fontsize=10)\n",
    "\n",
    "    # Tight layout and save\n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(folder, 'loss_auc_accuracy_plot.png')\n",
    "    plt.savefig(plot_path)\n",
    "    print(f\"Plot saved at: {plot_path}\")\n",
    "    plt.show()\n",
    "\n",
    "def apply_smote_resampling_cgm(X_matrices, y, matrix_shape0=288):\n",
    "    smote = SMOTE(random_state=42)\n",
    "\n",
    "    # Combine all inputs into a single 2D array for SMOTE\n",
    "    X_combined = np.hstack([\n",
    "        X_matrices.reshape(-1, matrix_shape0 * 288*3)\n",
    "    ])\n",
    "\n",
    "    # Apply SMOTE to the combined array and the target\n",
    "    X_resampled_combined, y_resampled = smote.fit_resample(X_combined, y)\n",
    "\n",
    "    # Extract the resampled components\n",
    "    X_matrices_resampled = X_resampled_combined[:, :matrix_shape0 * 288*3].reshape(-1, matrix_shape0, 288, 3)\n",
    "\n",
    "    return (X_matrices_resampled, \n",
    "            y_resampled)\n",
    "def apply_smote_resampling(X_matrices, X_features_daily, X_features_individual, y, matrix_shape0=288):\n",
    "    smote = SMOTE(random_state=42)\n",
    "\n",
    "    # Combine all inputs into a single 2D array for SMOTE\n",
    "    X_combined = np.hstack([\n",
    "        X_matrices.reshape(-1, matrix_shape0 * 288*3),\n",
    "        X_features_daily,\n",
    "        X_features_individual\n",
    "    ])\n",
    "\n",
    "    # Apply SMOTE to the combined array and the target\n",
    "    X_resampled_combined, y_resampled = smote.fit_resample(X_combined, y)\n",
    "\n",
    "    # Extract the resampled components\n",
    "    X_matrices_resampled = X_resampled_combined[:, :matrix_shape0 * 288*3].reshape(-1, matrix_shape0, 288, 3)\n",
    "    start = matrix_shape0 * 288*3\n",
    "    X_features_daily_resampled = X_resampled_combined[:, start:start + X_features_daily.shape[1]]\n",
    "    start += X_features_daily.shape[1]\n",
    "    X_features_individual_resampled = X_resampled_combined[:, start:]\n",
    "    \n",
    "    return X_matrices_resampled, X_features_daily_resampled, X_features_individual_resampled, y_resampled\n",
    "\n",
    "def initialize_results():\n",
    "    return {\n",
    "        \"fold_model\": [], \"train_acc\": [], \"val_acc\": [], \"accuracy_scores\": [], \"recall_scores\": [],\"precision_scores\": [], \"f1_scores\": [], \"roc_auc_scores\": [], \"specificity_scores\": [],\n",
    "        \"mae\": [], \"mse\": [],\"fold_train_loss\": [],\"fold_train_auc\": [],\"fold_train_acc\": [],\"fold_val_loss\": [],\"fold_val_auc\": [],\"fold_val_acc\": [],\n",
    "        \"conf_matrix\": [], \"class_report\": [],\"train_loss\": [],\"train_acc\": [],\"val_loss\": [],\"val_acc\": [],\n",
    "    }\n",
    "\n",
    "def prepare_datasets(file_path):\n",
    "    \"\"\"\n",
    "    Loads and processes dataset from the given file path.\n",
    "    Filters data, extracts relevant features and labels, applies normalization, \n",
    "    and prepares input matrices for analysis.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the dataset file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: matrices, daily_features, individual_features, labels, participant_ids\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load data\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        data_dict = dill.load(f)\n",
    "    \n",
    "    # Filter data\n",
    "    if len(data_dict)>350:\n",
    "        filtered_data_dict = {}\n",
    "        for participant_id, days_data in data_dict.items():\n",
    "            filtered_days_data = [day_data for current_date, day_data in days_data.items()\n",
    "                                  if (day_data['len_original_cgm'] > 252 and \n",
    "                                      np.sum(day_data['CGM_stats_daily_original'].isna()).sum() < 1)]\n",
    "            if filtered_days_data:\n",
    "                filtered_data_dict[participant_id] = filtered_days_data\n",
    "    else:\n",
    "        filtered_data_dict = data_dict\n",
    "    \n",
    "    del data_dict\n",
    "    # Initialize data structures\n",
    "    participant_ids = []\n",
    "    X_features_individual, X_features_daily = [], []\n",
    "    X_matrices_power = []\n",
    "    labels = { \n",
    "        \"hypo_early_night\": [], \"hypo_night\": [], \"hypo_long_night\": [],\n",
    "        \"hypo_night_morning\": [], \"hyper_day\": [], \"hypo_late_night\": [],\n",
    "        \"hyper_night\": [], \"hyper_early_night\": [], \"hypo_morning\": [],\n",
    "        \"hypo_day\": []\n",
    "    }\n",
    "    \n",
    "    # Process data\n",
    "    for participant_id, days_data in filtered_data_dict.items():\n",
    "        participant_ids.extend([participant_id] * len(days_data))\n",
    "        for day_data in days_data:\n",
    "            X_matrices_power.append(day_data['full_matrix_cgm_power'])\n",
    "            X_features_daily.append(np.concatenate([\n",
    "                day_data['CGM_stats_daily_original'],\n",
    "                day_data['Basal_stats_daily'],\n",
    "                day_data['Carbs_stats_daily'],\n",
    "                day_data['HR_stats_daily']], axis=1))\n",
    "            X_features_individual.append(np.concatenate([\n",
    "                [[day_data['A1c']]],\n",
    "                [[day_data['Weight']]],\n",
    "                [[day_data['Height']]],\n",
    "                day_data['CGM_stats_participant'],\n",
    "                day_data['Basal_stats_participant'],\n",
    "                day_data['HR_stats_participant']], axis=1))\n",
    "            \n",
    "            for label in labels.keys():\n",
    "                labels[label].append(day_data['next_day_cgm_labels_original'][label])\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    participant_ids = np.array(participant_ids)\n",
    "    X_matrices_power = np.array(X_matrices_power)\n",
    "    X_features_daily = np.array(X_features_daily)\n",
    "    X_features_individual = np.array(X_features_individual)\n",
    "    for key in labels:\n",
    "        labels[key] = np.array(labels[key], dtype=np.float32)\n",
    "    \n",
    "    # Standardization function\n",
    "    def standard_scale(data):\n",
    "        scaler = StandardScaler()\n",
    "        data[np.isinf(data)] = np.nan  # Replace infinities with NaN\n",
    "        return scaler.fit_transform(data.reshape(-1, data.shape[-1]))\n",
    "    \n",
    "    # Apply standardization\n",
    "    individual_features = standard_scale(X_features_individual)\n",
    "    daily_features = standard_scale(X_features_daily)\n",
    "    \n",
    "    # # Function to check for NaNs\n",
    "    # def check_nans(arr, name):\n",
    "    #     print(f\"{name}: {np.isnan(arr).sum()} NaNs found, shape: {arr.shape}\")\n",
    "    \n",
    "    # print('Checking for NaNs in the data...')\n",
    "    # check_nans(X_matrices_power, \"X_matrices_power\")\n",
    "    # check_nans(individual_features, \"X_features_individual_scaled\")\n",
    "    # check_nans(daily_features, \"X_features_daily_scaled\")\n",
    "    # for key in labels:\n",
    "    #     check_nans(labels[key], f\"y_label {key}\")\n",
    "    \n",
    "    del filtered_data_dict, X_features_daily, X_features_individual    \n",
    "    # Manual standard scaling for matrices\n",
    "    def manual_standard_scaling(matrix):\n",
    "        return matrix / np.nanstd(matrix)\n",
    "    \n",
    "    matrices = np.zeros_like(X_matrices_power)\n",
    "    for i in range(X_matrices_power.shape[0]):\n",
    "        for channel in range(3):  # Assuming 3 channels in last dimension\n",
    "            matrices[i, :, :, channel] = manual_standard_scaling(X_matrices_power[i, :, :, channel])\n",
    "    \n",
    "    del X_matrices_power\n",
    "    gc.collect()\n",
    "    \n",
    "    return matrices, daily_features, individual_features, labels, participant_ids\n",
    " \n",
    "# Attention mechanism\n",
    "def attention_block(x):\n",
    "    attention = Conv2D(32, kernel_size=(1, 1), activation='relu')(x)\n",
    "    attention = BatchNormalization()(attention)\n",
    "    attention = GlobalMaxPooling2D()(attention)\n",
    "    attention = Dense(x.shape[-1], activation='sigmoid')(attention)\n",
    "    attention = Reshape((1, 1, x.shape[-1]))(attention)\n",
    "    attention = Multiply()([x, attention])\n",
    "    return attention\n",
    "\n",
    "def create_model_1d():\n",
    "    # Define the CNN Model\n",
    "    input_matrix = Input(shape=(matrix_shape0, 288, 3), name='input_matrix')\n",
    "    \n",
    "    conv1 = Conv2D(32, kernel_size=(5, 5), strides=(2, 2), activation='relu', kernel_regularizer=l2(0.001))(input_matrix)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv1 = attention_block(conv1)  # Add attention here\n",
    "\n",
    "    conv2 = Conv2D(64, kernel_size=(3, 3), dilation_rate=(2, 2), activation='relu', kernel_regularizer=l2(0.001))(conv1)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    # Convolution Block 3 with Spatial Pyramid Pooling\n",
    "    conv3 = Conv2D(128, kernel_size=(3, 3), strides=(2, 2), activation='relu', kernel_regularizer=l2(0.001))(conv2)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    # Flatten and concatenate with feature vectors\n",
    "    flat = Flatten()(conv3)\n",
    "    dense_flat = Dense(1024, activation='relu', kernel_regularizer=l2(0.001))(flat)\n",
    "    attention_flat = Dense(1024, activation='softmax')(dense_flat)\n",
    "    attention_flat = Multiply()([dense_flat, attention_flat])\n",
    "\n",
    "    densed_fully = Dense(256, activation='relu')(attention_flat)\n",
    "    densed_fully = Dropout(0.5)(densed_fully)\n",
    "    \n",
    "    fc_additional = Dense(128, activation='relu')(densed_fully)\n",
    "    output = Dense(1, activation='sigmoid', dtype='float32')(fc_additional)\n",
    "    model = Model(inputs=input_matrix, outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=5e-5), loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "    return model\n",
    "def create_model_cgm():\n",
    "    # Define the CNN Model\n",
    "    input_matrix = Input(shape=(matrix_shape0, 288, 3), name='input_matrix')\n",
    "    input_features_daily = Input(shape=(dailyshape,), name='input_features_daily')\n",
    "    input_features_individual = Input(shape=(individualshape,), name='input_features_individual')\n",
    "\n",
    "    conv1 = Conv2D(32, kernel_size=(5, 5), strides=(2, 2), activation='relu', kernel_regularizer=l2(0.001))(input_matrix)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv1 = attention_block(conv1)  # Add attention here\n",
    "\n",
    "    conv2 = Conv2D(64, kernel_size=(3, 3), dilation_rate=(2, 2), activation='relu', kernel_regularizer=l2(0.001))(conv1)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    # Convolution Block 3 with Spatial Pyramid Pooling\n",
    "    conv3 = Conv2D(128, kernel_size=(3, 3), strides=(2, 2), activation='relu', kernel_regularizer=l2(0.001))(conv2)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    # Flatten and concatenate with feature vectors\n",
    "    flat = Flatten()(conv3)\n",
    "    dense_flat = Dense(1024, activation='relu', kernel_regularizer=l2(0.001))(flat)\n",
    "    attention_flat = Dense(1024, activation='softmax')(dense_flat)\n",
    "    attention_flat = Multiply()([dense_flat, attention_flat])\n",
    "\n",
    "    densed_fully = Dense(512, activation='relu')(attention_flat)\n",
    "    densed_fully = Dropout(0.5)(densed_fully)\n",
    "    dense_daily = Dense(128, activation='relu')(input_features_daily)\n",
    "    dense_individual = Dense(128, activation='relu')(input_features_individual)\n",
    "    \n",
    "    # Combine feature inputs\n",
    "    concat_features = concatenate([dense_daily, dense_individual])\n",
    "    # Cross-Attention mechanism to extract relevant parts of both feature vectors\n",
    "    attention_weights = Dense(256, activation='softmax')(concat_features)\n",
    "    attention_features = Multiply()([concat_features, attention_weights])\n",
    "    # Combine all inputs\n",
    "    concat_all = concatenate([densed_fully, attention_features])\n",
    "    \n",
    "    fc_combined = Dense(512, activation='relu', kernel_regularizer=l2(0.001))(concat_all)\n",
    "    fc_combined = Dropout(0.6)(fc_combined)\n",
    "\n",
    "    attention_fc_combined = Dense(256, activation='relu')(fc_combined)\n",
    "    attention_fc_combined = Dropout(0.7)(attention_fc_combined)\n",
    "    \n",
    "    fc_additional = Dense(128, activation='relu')(attention_fc_combined)\n",
    "    output = Dense(1, activation='sigmoid')(fc_additional)\n",
    "    # Define the model\n",
    "    model = Model(inputs=[input_matrix, input_features_daily, input_features_individual], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=2e-4), loss='binary_crossentropy', metrics=['accuracy','AUC'])\n",
    "    return model\n",
    "def create_model_mask():\n",
    "    # Define the CNN Model\n",
    "    input_matrix = Input(shape=(matrix_shape0, 288, 3), name='input_matrix')\n",
    "    input_features_daily = Input(shape=(dailyshape,), name='input_features_daily')\n",
    "    input_features_individual = Input(shape=(individualshape,), name='input_features_individual')\n",
    "    input_mask_daily = Input(shape=(dailyshape,), name='input_mask_daily')\n",
    "    input_mask_individual = Input(shape=(individualshape,), name='input_mask_individual')\n",
    "    \n",
    "    conv1 = Conv2D(32, kernel_size=(5, 5), strides=(2, 2), activation='relu', kernel_regularizer=l2(0.001))(input_matrix)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv1 = attention_block(conv1)  # Add attention here\n",
    "\n",
    "    conv2 = Conv2D(64, kernel_size=(3, 3), dilation_rate=(2, 2), activation='relu', kernel_regularizer=l2(0.001))(conv1)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    # Convolution Block 3 with Spatial Pyramid Pooling\n",
    "    conv3 = Conv2D(128, kernel_size=(3, 3), strides=(2, 2), activation='relu', kernel_regularizer=l2(0.001))(conv2)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    # Flatten and concatenate with feature vectors\n",
    "    flat = Flatten()(conv3)\n",
    "    dense_flat = Dense(1024, activation='relu', kernel_regularizer=l2(0.001))(flat)\n",
    "    # attention_flat = Dense(1024, activation='softmax')(dense_flat)\n",
    "    # attention_flat = Multiply()([dense_flat, attention_flat])\n",
    "\n",
    "    densed_fully = Dense(512, activation='relu')(dense_flat)\n",
    "    densed_fully = Dropout(0.5)(densed_fully)\n",
    "\n",
    "    # Masked dense layers for the daily features\n",
    "    masked_daily = Multiply()([input_features_daily, input_mask_daily])  # Apply mask to daily features\n",
    "    dense_daily = Dense(128, activation='relu')(masked_daily)\n",
    "    \n",
    "    # Masked dense layers for the individual features\n",
    "    masked_individual = Multiply()([input_features_individual, input_mask_individual])\n",
    "    dense_individual = Dense(128, activation='relu')(masked_individual)\n",
    "    \n",
    "    # Combine feature inputs\n",
    "    concat_features = concatenate([dense_daily, dense_individual])\n",
    "    \n",
    "    # Cross-Attention mechanism to extract relevant parts of both feature vectors\n",
    "    attention_weights = Dense(256, activation='softmax')(concat_features)\n",
    "    attention_features = Multiply()([concat_features, attention_weights])\n",
    "    \n",
    "    # Combine all inputs\n",
    "    concat_all = concatenate([densed_fully, attention_features])\n",
    "    \n",
    "    fc_combined = Dense(512, activation='relu', kernel_regularizer=l2(0.001))(concat_all)\n",
    "    fc_combined = Dropout(0.6)(fc_combined)\n",
    "\n",
    "    attention_fc_combined = Dense(256, activation='relu')(fc_combined)\n",
    "    attention_fc_combined = Dropout(0.7)(attention_fc_combined)\n",
    "    \n",
    "    fc_additional = Dense(128, activation='relu')(attention_fc_combined)\n",
    "    output = Dense(1, activation='sigmoid', dtype='float32')(fc_additional)\n",
    "    model = Model(inputs=[input_matrix, input_features_daily, input_features_individual, input_mask_daily, input_mask_individual], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=2e-4), loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "    return model\n",
    "    \n",
    "# Function to create an efficient TF dataset\n",
    "def prepare_tf_dataset(X_matrix, X_features1, X_features2, X_mask1, X_mask2, y, batch_size=16, shuffle=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(((X_matrix, X_features1, X_features2, X_mask1, X_mask2), y))\n",
    "    \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(y), reshuffle_each_iteration=True)\n",
    "    \n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "def prepare_tf_dataset_mask(X_matrix, X_features1, X_features2,X_mask1, X_mask2, y, batch_size=16, shuffle=True):\n",
    "    def data_generator():\n",
    "        for i in range(len(y)):\n",
    "            yield (X_matrix[i], X_features1[i], X_features2[i], X_mask1[i], X_mask2[i]), y[i]\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        data_generator,\n",
    "        output_signature=(\n",
    "            (\n",
    "                tf.TensorSpec(shape=(288, 288, 3), dtype=tf.float64),  # Matrix input\n",
    "                tf.TensorSpec(shape=(dailyshape,), dtype=tf.float16),         # Feature vector 1\n",
    "                tf.TensorSpec(shape=(individualshape,), dtype=tf.float64),          # Feature vector 2\n",
    "                tf.TensorSpec(shape=(dailyshape,), dtype=tf.float16),         # Mask vector 1\n",
    "                tf.TensorSpec(shape=(individualshape,), dtype=tf.float16)          # Mask vector 2            \n",
    "            ),\n",
    "            tf.TensorSpec(shape=(), dtype=tf.float16)  # Target variable\n",
    "        )\n",
    "    )\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(y), reshuffle_each_iteration=True)\n",
    "    \n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    " \n",
    "def get_train_val_test_indices(participant_ids, train_val_idx, test_idx):\n",
    "    model_participants = np.unique(participant_ids)\n",
    "    model_participants = shuffle(model_participants, random_state=42)\n",
    "    # 75% of the data as train set\n",
    "    train_val_participants = model_participants[np.isin(model_participants, participant_ids[train_val_idx])]\n",
    "    test_participants = model_participants[np.isin(model_participants, participant_ids[test_idx])]\n",
    "    train_size = int(len(train_val_participants) * 8 / 9)\n",
    "    train_participants = train_val_participants[:train_size]\n",
    "    val_participants = train_val_participants[train_size:]\n",
    "    print(f\"{len(train_participants)}, {len(val_participants)}, {len(test_participants)}, participants in Train, Validation, and Test sets\" )\n",
    "    \n",
    "    train_idx = np.where(np.isin(participant_ids, train_participants))[0]\n",
    "    val_idx = np.where(np.isin(participant_ids, val_participants))[0]\n",
    "    return train_idx, val_idx\n",
    "\n",
    "def train_and_evaluate_resampled_model_mask(fold, train_idx, val_idx, X_matrix,X_daily_features,X_individual_features, y_labels, num_epochs=40, batch_s=16, num_patience=20, monitored='val_loss'):\n",
    "    model = create_model_3d()\n",
    "    \n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor=monitored, patience=num_patience, restore_best_weights=True)\n",
    "    learning_rate_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor=monitored, factor=0.5, patience=4, min_lr=1e-7)\n",
    "    checkpoint = ModelCheckpoint(filepath=os.path.join(folder_name, 'best_model_fold_{}.keras'.format(fold)), monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "\n",
    "    X_train_resampled,X_daily_train_resampled,X_individual_train_resampled, y_label_train_resampled = apply_smote_resampling(\n",
    "        X_matrix[train_idx],X_daily_features[train_idx],X_individual_features[train_idx],\n",
    "        y_labels[train_idx], matrix_shape0=matrix_shape0\n",
    "    )\n",
    "    X_val_resampled,X_daily_val_resampled,X_individual_val_resampled, y_label_val_resampled = apply_smote_resampling(\n",
    "        X_matrix[val_idx],X_daily_features[val_idx],X_individual_features[val_idx],\n",
    "        y_labels[val_idx], matrix_shape0=matrix_shape0\n",
    "    )\n",
    "    train_dataset = prepare_tf_dataset(X_train_resampled,X_daily_train_resampled,X_individual_train_resampled, y_label_train_resampled, batch_size=batch_s)\n",
    "    val_dataset = prepare_tf_dataset(X_val_resampled,X_daily_val_resampled,X_individual_val_resampled, y_label_val_resampled, batch_size=batch_s, shuffle=False)\n",
    "    del X_train_resampled,X_daily_train_resampled,X_individual_train_resampled, y_label_train_resampled\n",
    "    del X_val_resampled,X_daily_val_resampled,X_individual_val_resampled, y_label_val_resampled\n",
    "    # gc.collect()\n",
    "    history = model.fit(train_dataset, epochs=num_epochs, verbose=1, validation_data=val_dataset, callbacks=[early_stopping, learning_rate_scheduler, checkpoint])\n",
    "    model.load_weights(os.path.join(folder_name, 'best_model_fold_{}.keras'.format(fold)))\n",
    "    del train_dataset, val_dataset, early_stopping, learning_rate_scheduler, checkpoint\n",
    "    gc.collect()\n",
    "    return model, history\n",
    "    \n",
    "def train_and_evaluate_model_mask(fold, train_idx,val_idx, X_matrix, X_daily_features,X_individual_features, mask_daily, mask_individual, y_labels, num_epochs=40, batch_s=16, num_patience=20, monitored='val_loss'):\n",
    "    model = create_model_mask()\n",
    "\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor=monitored, patience=num_patience, restore_best_weights=True)\n",
    "    learning_rate_scheduler = keras.callbacks.ReduceLROnPlateau(monitor=monitored, factor=0.5, patience=8, min_lr=1e-7)\n",
    "    checkpoint = ModelCheckpoint(filepath=os.path.join(folder_name, 'best_model_fold_{}.keras'.format(fold)), monitor='val_loss', save_best_only=True, mode='min')\n",
    "    train_dataset = prepare_tf_dataset(X_matrix[train_idx], X_daily_features[train_idx], X_individual_features[train_idx],mask_daily[train_idx], mask_individual[train_idx], y_labels[train_idx], batch_size=batch_s)\n",
    "    val_dataset = prepare_tf_dataset(X_matrix[val_idx], X_daily_features[val_idx], X_individual_features[val_idx],mask_daily[val_idx], mask_individual[val_idx], y_labels[val_idx], batch_size=batch_s, shuffle=False)\n",
    "    \n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_labels[train_idx]), y=y_labels[train_idx].ravel())\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    print(\"Class weights: \", class_weight_dict)\n",
    "    history = model.fit(train_dataset, epochs=num_epochs, verbose=1, validation_data=val_dataset, callbacks=[early_stopping, learning_rate_scheduler,checkpoint],class_weight=class_weight_dict)\n",
    "    model.load_weights(os.path.join(folder_name, 'best_model_fold_{}.keras'.format(fold)))\n",
    "\n",
    "    del train_dataset, val_dataset\n",
    "    gc.collect()\n",
    "    return model, history\n",
    "\n",
    "def evaluate_performance_mask(model, test_idx, X_matrices, X_daily_features, X_individual_features,mask_daily, mask_individual, y_labels, threshold=0.3):\n",
    "    y_pred_test = model.predict([X_matrices[test_idx], X_daily_features[test_idx], X_individual_features[test_idx],mask_daily[test_idx], mask_individual[test_idx]])\n",
    "    y_pred_test_binary = (y_pred_test >= threshold).astype(int)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy_scores\": accuracy_score(y_labels[test_idx], y_pred_test_binary),\n",
    "        \"recall_scores\": recall_score(y_labels[test_idx], y_pred_test_binary),\n",
    "        \"precision_scores\": precision_score(y_labels[test_idx], y_pred_test_binary),\n",
    "        \"f1_scores\": f1_score(y_labels[test_idx], y_pred_test_binary),\n",
    "        \"specificity_scores\": recall_score(y_labels[test_idx], y_pred_test_binary, pos_label=0),\n",
    "        \"roc_auc_scores\": roc_auc_score(y_labels[test_idx], y_pred_test),\n",
    "        \"conf_matrix\": confusion_matrix(y_labels[test_idx], y_pred_test_binary)}\n",
    "\n",
    "def get_target_results_mask(models, num_splits, X_matrices, X_daily_features, X_individual_features,mask_daily, mask_individual,participant_ids, y_labels):\n",
    "    # Initialize arrays to store predictions from each fold\n",
    "    y_pred_target_agg = np.zeros((len(y_labels),)) \n",
    "    # Aggregate predictions from each fold model\n",
    "    for model in models:\n",
    "        y_pred_target_fold = model.predict([X_matrices, X_daily_features, X_individual_features,mask_daily, mask_individual])\n",
    "        y_pred_target_agg += y_pred_target_fold.flatten()  # Accumulate predictions\n",
    "        y_pred_target_avg = np.mean(y_pred_target_fold, axis=0)\n",
    "    # Average the predictions from all folds\n",
    "    y_pred_target_agg /= num_splits\n",
    "    \n",
    "    y_pred_target_binary_agg = (y_pred_target_agg >= 0.5).astype(int) \n",
    "    conf_matrix = confusion_matrix(y_labels, y_pred_target_binary_agg)\n",
    "    \n",
    "    # Compute recall per participant\n",
    "    df_results = pd.DataFrame({\n",
    "        \"participant_id\": participant_ids.astype(int),\n",
    "        \"y_true\": y_labels.flatten(),\n",
    "        \"y_pred_binary\": y_pred_target_binary_agg\n",
    "    })\n",
    "\n",
    "    recall_per_participant = df_results.groupby(\"participant_id\").apply(lambda g: recall_score(g[\"y_true\"], g[\"y_pred_binary\"]) if sum(g[\"y_true\"]) > 0 else np.nan)\n",
    "\n",
    "    return {\n",
    "        \"accuracy_scores\": accuracy_score(y_labels, y_pred_target_binary_agg),\n",
    "        \"recall_scores\": recall_score(y_labels, y_pred_target_binary_agg),\n",
    "        \"recall_scores_per_participant\": recall_per_participant,\n",
    "        \"precision_scores\": precision_score(y_labels, y_pred_target_binary_agg),\n",
    "        \"f1_scores\": f1_score(y_labels, y_pred_target_binary_agg),\n",
    "        \"specificity_scores\": recall_score(y_labels, y_pred_target_binary_agg, pos_label=0),\n",
    "        \"roc_auc_scores\": roc_auc_score(y_labels, y_pred_target_agg),\n",
    "        \"conf_matrix\": confusion_matrix(y_labels, y_pred_target_binary_agg)\n",
    "    }\n",
    "\n",
    "def cross_validation_holdout_analysis(label_name, num_splits=7, num_epochs=30, batch_s=32, num_patience=10, monitored='val_loss',threshold=0.3):\n",
    "    configure_gpu()\n",
    "    results = initialize_results()\n",
    "    # group_kfold = GroupKFold(n_splits=num_splits)\n",
    "    file_path = '/home/ma98/data_generated/Adult/dict_3d_cgm_mex_morl_smooth_updated.dill'\n",
    "    X_matrices, X_daily_features, X_individual_features, labels, participant_ids = prepare_datasets(file_path)\n",
    "    y_labels = labels[label_name]\n",
    "    model_participants = np.unique(participant_ids)\n",
    "    # model_participants = shuffle(model_participants, random_state=42)\n",
    "    participant_labels = []\n",
    "    participant_sample_indices = []\n",
    "    \n",
    "    for participant in model_participants:\n",
    "        idx = np.where(participant_ids == participant)[0]\n",
    "        participant_sample_indices.append(idx)\n",
    "        label_values = y_labels[idx]\n",
    "        participant_labels.append(int(np.any(label_values == 1)))  # binary: 1 if any day is positive\n",
    "    \n",
    "    participant_labels = np.array(participant_labels)\n",
    "    sgkf = MultilabelStratifiedKFold(num_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    with tf.distribute.MirroredStrategy().scope():\n",
    "        # for fold, (train_val_idx, test_idx) in enumerate(group_kfold.split(X_matrices, y_labels, groups=participant_ids)):\n",
    "        for fold, (train_val_group_idx, test_group_idx) in enumerate(\n",
    "            sgkf.split(model_participants.reshape(-1, 1), \n",
    "                       np.vstack((1 - participant_labels, participant_labels)).T)\n",
    "        ):            \n",
    "            train_val_participants = model_participants[train_val_group_idx]\n",
    "            test_participants = model_participants[test_group_idx]\n",
    "        \n",
    "            # Step 4: Map back to sample indices\n",
    "            train_val_idx = np.concatenate([np.where(participant_ids == p)[0] for p in train_val_participants])\n",
    "            test_idx = np.concatenate([np.where(participant_ids == p)[0] for p in test_participants])\n",
    "\n",
    "            train_idx, val_idx = get_train_val_test_indices(participant_ids, train_val_idx, test_idx)\n",
    "            print(f\"Processing fold {fold + 1}...\")\n",
    "            X_daily_imputed = np.nan_to_num(X_daily_features, nan=0.0)\n",
    "            X_individual_imputed = np.nan_to_num(X_individual_features, nan=0.0)\n",
    "            \n",
    "            # Create masks for missing values\n",
    "            mask_daily = np.isfinite(X_daily_features).astype(np.float16)\n",
    "            mask_individual = np.isfinite(X_individual_features).astype(np.float16)\n",
    "\n",
    "            model, model_history = train_and_evaluate_model_mask(fold, train_idx,  val_idx, X_matrices, X_daily_imputed, X_individual_imputed,mask_daily, mask_individual, y_labels, \n",
    "                                                                 num_epochs, batch_s, num_patience, monitored)\n",
    "    \n",
    "            fold_results = evaluate_performance_mask(model, test_idx, X_matrices, X_daily_imputed, X_individual_imputed,mask_daily, mask_individual, y_labels, threshold)\n",
    "            \n",
    "            for key in results.keys():\n",
    "                if key in fold_results.keys():\n",
    "                    results[key].append(fold_results[key])\n",
    "            results[\"fold_model\"].append(model)\n",
    "            results[\"fold_train_acc\"].append(model_history.history['accuracy'])\n",
    "            results[\"fold_train_auc\"].append(model_history.history['AUC'])\n",
    "            results[\"fold_train_loss\"].append(model_history.history['loss'])\n",
    "            results[\"fold_val_acc\"].append(model_history.history['val_accuracy'])\n",
    "            results[\"fold_val_auc\"].append(model_history.history['val_AUC'])\n",
    "            results[\"fold_val_loss\"].append(model_history.history['val_loss'])\n",
    "            \n",
    "            print(f\"\\nFold {fold + 1} - {fold_results}\")\n",
    "            print(\"\\n\", fold_results[\"roc_auc_scores\"])\n",
    "            # Free GPU memory\n",
    "            del model, model_history,fold_results\n",
    "            gc.collect()\n",
    "    del X_matrices, X_daily_features, X_individual_features, labels, participant_ids\n",
    "    gc.collect()\n",
    "    # Calculate and print average and std across folds\n",
    "    print(f'\\nAverage Test Accuracy: {np.mean(results[\"accuracy_scores\"]):.4f} (+- {np.std(results[\"accuracy_scores\"]):.4f})')\n",
    "    print(f'Average Test Recall: {np.mean(results[\"recall_scores\"]):.4f} (+- {np.std(results[\"recall_scores\"]):.4f})')\n",
    "    print(f'Average Test Specificity: {np.mean(results[\"specificity_scores\"]):.4f} (+- {np.std(results[\"specificity_scores\"]):.4f})')\n",
    "    print(f'Average Test AUC: {np.mean(results[\"roc_auc_scores\"]):.4f} (+- {np.std(results[\"roc_auc_scores\"]):.4f})')\n",
    "\n",
    "    print(f\"Aggregated Models on Pediatric set ... \\n\")\n",
    "    file_path = '/home/ma98/data_generated/Pediatric/FullFeatures/full_pediatric_cgm_mex_morl_smooth.dill'\n",
    "    target_matrices, target_daily_features, target_individual_features, labels,target_participant_ids = prepare_datasets(file_path)\n",
    "    target_labels = labels[label_name]\n",
    "    target_daily_imputed = np.nan_to_num(target_daily_features, nan=0.0)\n",
    "    target_individual_imputed = np.nan_to_num(target_individual_features, nan=0.0)\n",
    "    \n",
    "    # Create masks for missing values\n",
    "    target_mask_daily = np.isfinite(target_daily_features).astype(np.float16)\n",
    "    target_mask_individual = np.isfinite(target_individual_features).astype(np.float16)\n",
    "    target_results = get_target_results_mask(results[\"fold_model\"],num_splits, target_matrices, target_daily_imputed, target_individual_imputed,target_mask_daily,target_mask_individual,target_participant_ids, target_labels)\n",
    "\n",
    "    print(f'Target Accuracy: {target_results[\"accuracy_scores\"]:.4f}')\n",
    "    print(f'Target Recall: {target_results[\"recall_scores\"]:.4f}')\n",
    "    print(f'Target Specificity: {target_results[\"specificity_scores\"]:.4f}')\n",
    "    print(f'Target ROC AUC: {target_results[\"roc_auc_scores\"]:.4f} \\n')\n",
    "    \n",
    "    pediatric_hbA1c_values = target_individual_features[:, 0] \n",
    "    # y_true = target_labels\n",
    "    # y_pred_binary = (target_results[\"recall_scores\"] >= 0.5).astype(int)\n",
    "    # df_results = pd.DataFrame({\n",
    "    #     \"participant_id\": target_participant_ids.astype(int),\n",
    "    #     \"y_true\": y_true,\n",
    "    #     \"y_pred_binary\": y_pred_binary})    \n",
    "    # df_results = df_results[df_results.groupby(\"participant_id\")[\"y_true\"].transform(\"sum\") > 0]\n",
    "    df = pd.DataFrame({'participant_id': target_participant_ids.astype(int), 'HbA1c': pediatric_hbA1c_values})\n",
    "    df = df.groupby('participant_id', as_index=False).mean()\n",
    "    \n",
    "    df_recall = target_results[\"recall_scores_per_participant\"].reset_index()\n",
    "    df_recall.columns = [\"participant_id\", \"avg_recall\"]\n",
    "    df_recall.dropna(inplace=True)\n",
    "    # df_recall = df_results.groupby(\"participant_id\").apply(lambda x: recall_score(x[\"y_true\"], x[\"y_pred_binary\"])).reset_index()\n",
    "    # df_recall.columns = [\"participant_id\", \"avg_recall\"]\n",
    "    \n",
    "    # Merge with HbA1c values\n",
    "    df_plot = pd.merge(df_recall, df, on=\"participant_id\")\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(df_plot[\"HbA1c\"], df_plot[\"avg_recall\"], alpha=0.7, edgecolors=\"black\", color=\"blue\")\n",
    "    plt.xlabel(\"HbA1c\")\n",
    "    plt.ylabel(\"Average Recall Score\")\n",
    "    plt.title('Hypoglycemia Prediction vs HbA1c')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    plot_training_history(results[\"fold_val_loss\"], results[\"fold_val_auc\"], results[\"fold_val_acc\"], folder_name)\n",
    "    return results, target_results\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
